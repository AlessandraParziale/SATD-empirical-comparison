ID,FilePath,ClassName,MethodName,Content,CommentFor,CommentsIn,CommentsAssociated,StartLine,EndLine,eachLabelCommentFor,CommentForLabel,eachLabelCommentsIn,CommentsInLabel,eachLabelCommentsAssociated,CommentsAssociatedLabel,PseudoLabelForCASFromMAT,PseudoLabelForCASFromGGSATD,PseudoLabelForCASFromXGBoost,MethodSimplified,class,method,constructor,line,cbo,cboModified,fanin,fanout,wmc,rfc,loc,returnsQty,variablesQty,parametersQty,methodsInvokedQty,methodsInvokedLocalQty,methodsInvokedIndirectLocalQty,loopQty,comparisonsQty,tryCatchQty,parenthesizedExpsQty,stringLiteralsQty,numbersQty,assignmentsQty,mathOperationsQty,maxNestedBlocksQty,anonymousClassesQty,innerClassesQty,lambdasQty,uniqueWordsQty,modifiers,logStatementsQty,hasJavaDoc
0,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Codec.java,co.elastic.logstash.api.Codec,"void decode(ByteBuffer, Consumer<Map<String, Object>>)","/**
 * Decodes events from the specified {@link ByteBuffer} and passes them to the provided
 * {@link Consumer}.
 *
 * <ul>
 * <li>The client (typically an {@link Input}) must provide a {@link ByteBuffer} that
 * is ready for reading with with {@link ByteBuffer#position} indicating the next
 * position to read and {@link ByteBuffer#limit} indicating the first byte in the
 * buffer that is not safe to read.</li>
 *
 * <li>Implementations of {@link Codec} must ensure that {@link ByteBuffer#position}
 * reflects the last-read position before returning control.</li>
 *
 * <li>The client is then responsible for returning the buffer
 * to write mode via either {@link ByteBuffer#clear} or {@link ByteBuffer#compact} before
 * resuming writes.</li>
 * </ul>
 *
 * @param buffer        Input buffer from which events will be decoded.
 * @param eventConsumer Consumer to which decoded events will be passed.
 */
void decode(ByteBuffer buffer, Consumer<Map<String, Object>> eventConsumer);","/**
 * Decodes events from the specified {@link ByteBuffer} and passes them to the provided
 * {@link Consumer}.
 *
 * <ul>
 * <li>The client (typically an {@link Input}) must provide a {@link ByteBuffer} that
 * is ready for reading with with {@link ByteBuffer#position} indicating the next
 * position to read and {@link ByteBuffer#limit} indicating the first byte in the
 * buffer that is not safe to read.</li>
 *
 * <li>Implementations of {@link Codec} must ensure that {@link ByteBuffer#position}
 * reflects the last-read position before returning control.</li>
 *
 * <li>The client is then responsible for returning the buffer
 * to write mode via either {@link ByteBuffer#clear} or {@link ByteBuffer#compact} before
 * resuming writes.</li>
 * </ul>
 *
 * @param buffer        Input buffer from which events will be decoded.
 * @param eventConsumer Consumer to which decoded events will be passed.
 */
", ,/** * Decodes events from the specified {@link ByteBuffer} and passes them to the provided * {@link Consumer}. * * <ul> * <li>The client (typically an {@link Input}) must provide a {@link ByteBuffer} that * is ready for reading with with {@link ByteBuffer#position} indicating the next * position to read and {@link ByteBuffer#limit} indicating the first byte in the * buffer that is not safe to read.</li> * * <li>Implementations of {@link Codec} must ensure that {@link ByteBuffer#position} * reflects the last-read position before returning control.</li> * * <li>The client is then responsible for returning the buffer * to write mode via either {@link ByteBuffer#clear} or {@link ByteBuffer#compact} before * resuming writes.</li> * </ul> * * @param buffer        Input buffer from which events will be decoded. * @param eventConsumer Consumer to which decoded events will be passed. */,56,56,[0],0,[0],0,[0],0,0,0,0,"decode(ByteBuffer, Consumer<Map<String, Object>>)",co.elastic.logstash.api.Codec,"decode/2[java.nio.ByteBuffer,java.util.function.Consumer<java.util.Map<java.lang.String,java.lang.Object>>]",False,35,0,2,2,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,61,0,0,True
1,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Codec.java,co.elastic.logstash.api.Codec,"void flush(ByteBuffer, Consumer<Map<String, Object>>)","/**
 * Decodes all remaining events from the specified {@link ByteBuffer} along with any internal
 * state that may remain after previous calls to {@link #decode(ByteBuffer, Consumer)}.
 * @param buffer        Input buffer from which events will be decoded.
 * @param eventConsumer Consumer to which decoded events will be passed.
 */
void flush(ByteBuffer buffer, Consumer<Map<String, Object>> eventConsumer);","/**
 * Decodes all remaining events from the specified {@link ByteBuffer} along with any internal
 * state that may remain after previous calls to {@link #decode(ByteBuffer, Consumer)}.
 * @param buffer        Input buffer from which events will be decoded.
 * @param eventConsumer Consumer to which decoded events will be passed.
 */
", ,"/** * Decodes all remaining events from the specified {@link ByteBuffer} along with any internal * state that may remain after previous calls to {@link #decode(ByteBuffer, Consumer)}. * @param buffer        Input buffer from which events will be decoded. * @param eventConsumer Consumer to which decoded events will be passed. */",64,64,[0],0,[0],0,[0],0,0,0,0,"flush(ByteBuffer, Consumer<Map<String, Object>>)",co.elastic.logstash.api.Codec,"flush/2[java.nio.ByteBuffer,java.util.function.Consumer<java.util.Map<java.lang.String,java.lang.Object>>]",False,58,0,2,2,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,29,0,0,True
2,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Codec.java,co.elastic.logstash.api.Codec,"void encode(Event, OutputStream)","/**
 * Encodes an {@link Event} and writes it to the specified {@link OutputStream}.
 * @param event The event to encode.
 * @param output The stream to which the encoded event should be written.
 * @throws java.io.IOException Exceptions coming from the output stream
 */
void encode(Event event, OutputStream output) throws IOException;","/**
 * Encodes an {@link Event} and writes it to the specified {@link OutputStream}.
 * @param event The event to encode.
 * @param output The stream to which the encoded event should be written.
 * @throws java.io.IOException Exceptions coming from the output stream
 */
", ,/** * Encodes an {@link Event} and writes it to the specified {@link OutputStream}. * @param event The event to encode. * @param output The stream to which the encoded event should be written. * @throws java.io.IOException Exceptions coming from the output stream */,72,72,[0],0,[0],0,[0],0,0,0,0,"encode(Event, OutputStream)",co.elastic.logstash.api.Codec,"encode/2[co.elastic.logstash.api.Event,java.io.OutputStream]",False,66,1,2,2,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,0,0,True
3,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Codec.java,co.elastic.logstash.api.Codec,Codec cloneCodec(),"/**
 * Clones this {@link Codec}. All codecs should be capable of cloning themselves
 * so that distinct instances of each codec can be supplied to multi-threaded
 * inputs or outputs in cases where the codec is stateful.
 * @return The cloned {@link Codec}.
 */
Codec cloneCodec();","/**
 * Clones this {@link Codec}. All codecs should be capable of cloning themselves
 * so that distinct instances of each codec can be supplied to multi-threaded
 * inputs or outputs in cases where the codec is stateful.
 * @return The cloned {@link Codec}.
 */
", ,/** * Clones this {@link Codec}. All codecs should be capable of cloning themselves * so that distinct instances of each codec can be supplied to multi-threaded * inputs or outputs in cases where the codec is stateful. * @return The cloned {@link Codec}. */,80,80,[0],0,[0],0,[0],0,0,0,0,cloneCodec(),co.elastic.logstash.api.Codec,cloneCodec/0,False,74,1,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,0,0,True
4,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Configuration.java,co.elastic.logstash.api.Configuration,T get(PluginConfigSpec<T>),"/**
 * Strongly-typed accessor for a configuration setting.
 * @param configSpec The setting specification for which to retrieve the setting value.
 * @param <T>        The type of the setting value to be retrieved.
 * @return           The value of the setting for the specified setting specification.
 */
<T> T get(PluginConfigSpec<T> configSpec);","/**
 * Strongly-typed accessor for a configuration setting.
 * @param configSpec The setting specification for which to retrieve the setting value.
 * @param <T>        The type of the setting value to be retrieved.
 * @return           The value of the setting for the specified setting specification.
 */
", ,/** * Strongly-typed accessor for a configuration setting. * @param configSpec The setting specification for which to retrieve the setting value. * @param <T>        The type of the setting value to be retrieved. * @return           The value of the setting for the specified setting specification. */,36,36,[0],0,[0],0,[0],0,0,0,0,get(PluginConfigSpec<T>),co.elastic.logstash.api.Configuration,get/1[co.elastic.logstash.api.PluginConfigSpec<T>],False,30,2,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,21,0,0,True
5,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Configuration.java,co.elastic.logstash.api.Configuration,Object getRawValue(PluginConfigSpec<?>),"/**
 * Weakly-typed accessor for a configuration setting.
 * @param configSpec The setting specification for which to retrieve the setting value.
 * @return           The weakly-typed value of the setting for the specified setting specification.
 */
Object getRawValue(PluginConfigSpec<?> configSpec);","/**
 * Weakly-typed accessor for a configuration setting.
 * @param configSpec The setting specification for which to retrieve the setting value.
 * @return           The weakly-typed value of the setting for the specified setting specification.
 */
", ,/** * Weakly-typed accessor for a configuration setting. * @param configSpec The setting specification for which to retrieve the setting value. * @return           The weakly-typed value of the setting for the specified setting specification. */,43,43,[0],0,[0],0,[0],0,0,0,0,getRawValue(PluginConfigSpec<?>),co.elastic.logstash.api.Configuration,getRawValue/1[co.elastic.logstash.api.PluginConfigSpec<?>],False,38,1,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,22,0,0,True
6,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Configuration.java,co.elastic.logstash.api.Configuration,boolean contains(PluginConfigSpec<?>),"/**
 * @param configSpec The setting specification for which to search.
 * @return           {@code true} if a value for the specified setting specification exists in
 * this {@link Configuration}.
 */
boolean contains(PluginConfigSpec<?> configSpec);","/**
 * @param configSpec The setting specification for which to search.
 * @return           {@code true} if a value for the specified setting specification exists in
 * this {@link Configuration}.
 */
", ,/** * @param configSpec The setting specification for which to search. * @return           {@code true} if a value for the specified setting specification exists in * this {@link Configuration}. */,50,50,[0],0,[0],0,[0],0,0,0,0,contains(PluginConfigSpec<?>),co.elastic.logstash.api.Configuration,contains/1[co.elastic.logstash.api.PluginConfigSpec<?>],False,45,1,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,0,0,True
7,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Configuration.java,co.elastic.logstash.api.Configuration,Collection<String> allKeys(),"/**
 * @return Collection of the names of all settings in this configuration as reported by
 * {@link PluginConfigSpec#name()}.
 */
Collection<String> allKeys();","/**
 * @return Collection of the names of all settings in this configuration as reported by
 * {@link PluginConfigSpec#name()}.
 */
", ,/** * @return Collection of the names of all settings in this configuration as reported by * {@link PluginConfigSpec#name()}. */,56,56,[0],0,[0],0,[0],0,0,0,0,allKeys(),co.elastic.logstash.api.Configuration,allKeys/0,False,52,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,12,0,0,True
8,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Context.java,co.elastic.logstash.api.Context,DeadLetterQueueWriter getDlqWriter(),"/**
 * Provides a dead letter queue (DLQ) writer, if configured, to output plugins. If no DLQ writer
 * is configured or the plugin is not an output, {@code null} will be returned.
 * @return {@link DeadLetterQueueWriter} instance if available or {@code null} otherwise.
 */
DeadLetterQueueWriter getDlqWriter();","/**
 * Provides a dead letter queue (DLQ) writer, if configured, to output plugins. If no DLQ writer
 * is configured or the plugin is not an output, {@code null} will be returned.
 * @return {@link DeadLetterQueueWriter} instance if available or {@code null} otherwise.
 */
", ,"/** * Provides a dead letter queue (DLQ) writer, if configured, to output plugins. If no DLQ writer * is configured or the plugin is not an output, {@code null} will be returned. * @return {@link DeadLetterQueueWriter} instance if available or {@code null} otherwise. */",35,35,[0],0,[0],0,[0],0,0,0,0,getDlqWriter(),co.elastic.logstash.api.Context,getDlqWriter/0,False,30,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,31,0,0,True
9,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Context.java,co.elastic.logstash.api.Context,NamespacedMetric getMetric(Plugin),"/**
 * Provides a metric namespace scoped to the given {@code plugin} that metrics can be written to and
 * can be nested deeper with further namespaces.
 * @param plugin The plugin the metric should be scoped to
 * @return       A metric scoped to the current plugin
 */
NamespacedMetric getMetric(Plugin plugin);","/**
 * Provides a metric namespace scoped to the given {@code plugin} that metrics can be written to and
 * can be nested deeper with further namespaces.
 * @param plugin The plugin the metric should be scoped to
 * @return       A metric scoped to the current plugin
 */
", ,/** * Provides a metric namespace scoped to the given {@code plugin} that metrics can be written to and * can be nested deeper with further namespaces. * @param plugin The plugin the metric should be scoped to * @return       A metric scoped to the current plugin */,43,43,[0],0,[0],0,[0],0,0,0,0,getMetric(Plugin),co.elastic.logstash.api.Context,getMetric/1[co.elastic.logstash.api.Plugin],False,37,2,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,27,0,0,True
10,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Context.java,co.elastic.logstash.api.Context,Logger getLogger(Plugin),"/**
 * Provides a {@link Logger} instance to plugins.
 * @param plugin The plugin for which the logger should be supplied.
 * @return       The supplied Logger instance.
 */
Logger getLogger(Plugin plugin);","/**
 * Provides a {@link Logger} instance to plugins.
 * @param plugin The plugin for which the logger should be supplied.
 * @return       The supplied Logger instance.
 */
", ,/** * Provides a {@link Logger} instance to plugins. * @param plugin The plugin for which the logger should be supplied. * @return       The supplied Logger instance. */,50,50,[0],0,[0],0,[0],0,0,0,0,getLogger(Plugin),co.elastic.logstash.api.Context,getLogger/1[co.elastic.logstash.api.Plugin],False,45,2,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,0,0,True
11,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Context.java,co.elastic.logstash.api.Context,DeprecationLogger getDeprecationLogger(Plugin),"/**
 * Provides a {@link Logger} instance to plugins.
 * @param plugin The plugin for which the logger should be supplied.
 * @return       The supplied Logger instance.
 */
DeprecationLogger getDeprecationLogger(Plugin plugin);","/**
 * Provides a {@link Logger} instance to plugins.
 * @param plugin The plugin for which the logger should be supplied.
 * @return       The supplied Logger instance.
 */
", ,/** * Provides a {@link Logger} instance to plugins. * @param plugin The plugin for which the logger should be supplied. * @return       The supplied Logger instance. */,57,57,[0],0,[0],0,[0],0,0,0,0,getDeprecationLogger(Plugin),co.elastic.logstash.api.Context,getDeprecationLogger/1[co.elastic.logstash.api.Plugin],False,52,2,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,0,0,True
12,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Context.java,co.elastic.logstash.api.Context,EventFactory getEventFactory(),"/**
 * Provides an {@link EventFactory} to constructs instance of {@link Event}.
 * @return The event factory.
 */
EventFactory getEventFactory();","/**
 * Provides an {@link EventFactory} to constructs instance of {@link Event}.
 * @return The event factory.
 */
", ,/** * Provides an {@link EventFactory} to constructs instance of {@link Event}. * @return The event factory. */,63,63,[0],0,[0],0,[0],0,0,0,0,getEventFactory(),co.elastic.logstash.api.Context,getEventFactory/0,False,59,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,0,0,True
13,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\CounterMetric.java,co.elastic.logstash.api.CounterMetric,void increment(),"/**
 * Increments the counter by 1.
 */
void increment();","/**
 * Increments the counter by 1.
 */
", ,/** * Increments the counter by 1. */,32,32,[0],0,[0],0,[0],0,0,0,0,increment(),co.elastic.logstash.api.CounterMetric,increment/0,False,29,0,3,3,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,0,0,True
14,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\CounterMetric.java,co.elastic.logstash.api.CounterMetric,void increment(long),"/**
 * Increments the counter by {@code delta}.
 *
 * @param delta amount to increment the counter by
 */
void increment(long delta);","/**
 * Increments the counter by {@code delta}.
 *
 * @param delta amount to increment the counter by
 */
", ,/** * Increments the counter by {@code delta}. * * @param delta amount to increment the counter by */,39,39,[0],0,[0],0,[0],0,0,0,0,increment(long),co.elastic.logstash.api.CounterMetric,increment/1[long],False,34,0,3,3,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,0,0,True
15,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\CounterMetric.java,co.elastic.logstash.api.CounterMetric,long getValue(),"/**
 * Gets the current value of the counter.
 *
 * @return the counter value
 */
long getValue();","/**
 * Gets the current value of the counter.
 *
 * @return the counter value
 */
", ,/** * Gets the current value of the counter. * * @return the counter value */,46,46,[0],0,[0],0,[0],0,0,0,0,getValue(),co.elastic.logstash.api.CounterMetric,getValue/0,False,41,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,0,0,True
16,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\CounterMetric.java,co.elastic.logstash.api.CounterMetric,void reset(),"/**
 * Sets the counter back to 0.
 */
void reset();","/**
 * Sets the counter back to 0.
 */
", ,/** * Sets the counter back to 0. */,51,51,[0],0,[0],0,[0],0,0,0,0,reset(),co.elastic.logstash.api.CounterMetric,reset/0,False,48,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,True
17,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\DeprecationLogger.java,co.elastic.logstash.api.DeprecationLogger,"void deprecated(String, Object...)","/**
 * Print to deprecation log the message with placeholder replaced by param values. The placeholder
 * are {} form, like in log4j's syntax.
 *
 * @param message string message with parameter's placeholders.
 * @param params var args with all the replacement parameters.
 */
void deprecated(String message, Object... params);","/**
 * Print to deprecation log the message with placeholder replaced by param values. The placeholder
 * are {} form, like in log4j's syntax.
 *
 * @param message string message with parameter's placeholders.
 * @param params var args with all the replacement parameters.
 */
", ,"/** * Print to deprecation log the message with placeholder replaced by param values. The placeholder * are {} form, like in log4j's syntax. * * @param message string message with parameter's placeholders. * @param params var args with all the replacement parameters. */",35,35,[0],0,[0],0,[0],0,0,0,0,"deprecated(String, Object[])",co.elastic.logstash.api.DeprecationLogger,"deprecated/2[java.lang.String,java.lang.Object[]]",False,28,0,1,1,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,22,0,0,True
18,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\EventFactory.java,co.elastic.logstash.api.EventFactory,Event newEvent(),"/**
 * @return New and empty event.
 */
default Event newEvent() {
    return newEvent(Collections.emptyMap());
}","/**
 * @return New and empty event.
 */
", ,/** * @return New and empty event. */,36,38,[0],0,[0],0,[0],0,0,0,0,newEvent(),co.elastic.logstash.api.EventFactory,newEvent/0,False,36,2,1,0,1,1,2,3,1,0,0,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,5,65536,0,True
19,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\EventFactory.java,co.elastic.logstash.api.EventFactory,"Event newEvent(Map<? extends Serializable, Object>)","/**
 * @param data Map from which the new event should copy its data.
 * @return     New event copied from the supplied map data.
 */
Event newEvent(final Map<? extends Serializable, Object> data);","/**
 * @param data Map from which the new event should copy its data.
 * @return     New event copied from the supplied map data.
 */
", ,/** * @param data Map from which the new event should copy its data. * @return     New event copied from the supplied map data. */,44,44,[0],0,[0],0,[0],0,0,0,0,"newEvent(Map<?Serializable, Object>)",co.elastic.logstash.api.EventFactory,"newEvent/1[java.util.Map<? extends java.io.Serializable,java.lang.Object>]",False,40,1,3,3,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,0,0,True
20,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Filter.java,co.elastic.logstash.api.Filter,"Collection<Event> filter(Collection<Event>, FilterMatchListener)","/**
 * Events from the event pipeline are presented for filtering through this method. If the filter either mutates
 * the events in-place or simply observes them, the incoming collection of events may be returned without
 * modification. If the filter creates new events, those new events must be added to the returned collection.
 * If the filter deletes events, the deleted events must be removed from the returned collection.
 * @param events        Collection of events to be filtered.
 * @param matchListener Filter match listener to be notified for each matching event. See
 * {@link FilterMatchListener} for more details.
 * @return              Collection of filtered events.
 */
Collection<Event> filter(Collection<Event> events, FilterMatchListener matchListener);","/**
 * Events from the event pipeline are presented for filtering through this method. If the filter either mutates
 * the events in-place or simply observes them, the incoming collection of events may be returned without
 * modification. If the filter creates new events, those new events must be added to the returned collection.
 * If the filter deletes events, the deleted events must be removed from the returned collection.
 * @param events        Collection of events to be filtered.
 * @param matchListener Filter match listener to be notified for each matching event. See
 * {@link FilterMatchListener} for more details.
 * @return              Collection of filtered events.
 */
", ,"/** * Events from the event pipeline are presented for filtering through this method. If the filter either mutates * the events in-place or simply observes them, the incoming collection of events may be returned without * modification. If the filter creates new events, those new events must be added to the returned collection. * If the filter deletes events, the deleted events must be removed from the returned collection. * @param events        Collection of events to be filtered. * @param matchListener Filter match listener to be notified for each matching event. See * {@link FilterMatchListener} for more details. * @return              Collection of filtered events. */",54,54,[0],0,[0],0,[0],0,0,0,0,"filter(Collection<Event>, FilterMatchListener)",co.elastic.logstash.api.Filter,"filter/2[java.util.Collection<co.elastic.logstash.api.Event>,co.elastic.logstash.api.FilterMatchListener]",False,44,2,1,1,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,46,0,0,True
21,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Filter.java,co.elastic.logstash.api.Filter,Collection<Event> flush(FilterMatchListener),"/**
 * If this filter maintains state between calls to {@link #filter(Collection, FilterMatchListener)}, this
 * method should return events for all state currently held by the filter. This method will never be called
 * by the Logstash execution engine unless {@link #requiresFlush()} returns {@code true} for this filter.
 * @param matchListener Filter match listener to be notified for each matching event. See
 * {@link FilterMatchListener} for more details.
 * @return              Collection of events for all state currently held by the filter.
 */
default Collection<Event> flush(FilterMatchListener matchListener) {
    return Collections.emptyList();
}","/**
 * If this filter maintains state between calls to {@link #filter(Collection, FilterMatchListener)}, this
 * method should return events for all state currently held by the filter. This method will never be called
 * by the Logstash execution engine unless {@link #requiresFlush()} returns {@code true} for this filter.
 * @param matchListener Filter match listener to be notified for each matching event. See
 * {@link FilterMatchListener} for more details.
 * @return              Collection of events for all state currently held by the filter.
 */
", ,"/** * If this filter maintains state between calls to {@link #filter(Collection, FilterMatchListener)}, this * method should return events for all state currently held by the filter. This method will never be called * by the Logstash execution engine unless {@link #requiresFlush()} returns {@code true} for this filter. * @param matchListener Filter match listener to be notified for each matching event. See * {@link FilterMatchListener} for more details. * @return              Collection of events for all state currently held by the filter. */",64,66,[0],0,[0],0,[0],0,0,0,0,flush(FilterMatchListener),co.elastic.logstash.api.Filter,flush/1[co.elastic.logstash.api.FilterMatchListener],False,64,2,1,1,0,1,1,3,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,39,65536,0,True
22,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Filter.java,co.elastic.logstash.api.Filter,boolean requiresFlush(),"/**
 * @return {@code true} if this filter maintains state between calls to
 * {@link #filter(Collection, FilterMatchListener)} and therefore requires a flush upon pipeline
 * shutdown to return the final events from the filter. The default implementation returns {@code false}
 * as is appropriate for stateless filters.
 */
default boolean requiresFlush() {
    return false;
}","/**
 * @return {@code true} if this filter maintains state between calls to
 * {@link #filter(Collection, FilterMatchListener)} and therefore requires a flush upon pipeline
 * shutdown to return the final events from the filter. The default implementation returns {@code false}
 * as is appropriate for stateless filters.
 */
", ,"/** * @return {@code true} if this filter maintains state between calls to * {@link #filter(Collection, FilterMatchListener)} and therefore requires a flush upon pipeline * shutdown to return the final events from the filter. The default implementation returns {@code false} * as is appropriate for stateless filters. */",74,76,[0],0,[0],0,[0],0,0,0,0,requiresFlush(),co.elastic.logstash.api.Filter,requiresFlush/0,False,74,0,2,2,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,65536,0,True
23,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Filter.java,co.elastic.logstash.api.Filter,boolean requiresPeriodicFlush(),"/**
 * @return {@code true} if this filter maintains state between calls to
 * {@link #filter(Collection, FilterMatchListener)} and requires periodic calls to flush events from the filter.
 * If {@code true}, {@link #requiresFlush()} must also return {@code true} for this filter. The default
 * implementation returns {@code false} as is appropriate for stateless filters.
 */
default boolean requiresPeriodicFlush() {
    return false;
}","/**
 * @return {@code true} if this filter maintains state between calls to
 * {@link #filter(Collection, FilterMatchListener)} and requires periodic calls to flush events from the filter.
 * If {@code true}, {@link #requiresFlush()} must also return {@code true} for this filter. The default
 * implementation returns {@code false} as is appropriate for stateless filters.
 */
", ,"/** * @return {@code true} if this filter maintains state between calls to * {@link #filter(Collection, FilterMatchListener)} and requires periodic calls to flush events from the filter. * If {@code true}, {@link #requiresFlush()} must also return {@code true} for this filter. The default * implementation returns {@code false} as is appropriate for stateless filters. */",84,86,[0],0,[0],0,[0],0,0,0,0,requiresPeriodicFlush(),co.elastic.logstash.api.Filter,requiresPeriodicFlush/0,False,84,0,1,1,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,65536,0,True
24,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\FilterMatchListener.java,co.elastic.logstash.api.FilterMatchListener,void filterMatched(Event),"/**
 * Notify the filter match listener that the specified event ""matches"" the filter criteria.
 * @param e Event that matches the filter criteria.
 */
void filterMatched(Event e);","/**
 * Notify the filter match listener that the specified event ""matches"" the filter criteria.
 * @param e Event that matches the filter criteria.
 */
", ,"/** * Notify the filter match listener that the specified event ""matches"" the filter criteria. * @param e Event that matches the filter criteria. */",37,37,[0],0,[0],0,[0],0,0,0,0,filterMatched(Event),co.elastic.logstash.api.FilterMatchListener,filterMatched/1[co.elastic.logstash.api.Event],False,33,1,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,12,0,0,True
25,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Input.java,co.elastic.logstash.api.Input,"void start(Consumer<Map<String, Object>>)","/**
 * Start the input and begin pushing events to the supplied {@link Consumer} instance. If the input produces
 * an infinite stream of events, this method should loop until a {@link #stop()} request is made. If the
 * input produces a finite stream of events, this method should terminate when the last event in the stream
 * is produced.
 * @param writer Consumer to which events should be pushed
 */
void start(Consumer<Map<String, Object>> writer);","/**
 * Start the input and begin pushing events to the supplied {@link Consumer} instance. If the input produces
 * an infinite stream of events, this method should loop until a {@link #stop()} request is made. If the
 * input produces a finite stream of events, this method should terminate when the last event in the stream
 * is produced.
 * @param writer Consumer to which events should be pushed
 */
", ,"/** * Start the input and begin pushing events to the supplied {@link Consumer} instance. If the input produces * an infinite stream of events, this method should loop until a {@link #stop()} request is made. If the * input produces a finite stream of events, this method should terminate when the last event in the stream * is produced. * @param writer Consumer to which events should be pushed */",48,48,[0],0,[0],0,[0],0,0,0,0,"start(Consumer<Map<String, Object>>)",co.elastic.logstash.api.Input,"start/1[java.util.function.Consumer<java.util.Map<java.lang.String,java.lang.Object>>]",False,41,0,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,35,0,0,True
26,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Input.java,co.elastic.logstash.api.Input,void stop(),"/**
 * Notifies the input to stop producing events. Inputs stop both asynchronously and cooperatively. Use the
 * {@link #awaitStop()} method to block until the input has completed the stop process.
 */
void stop();","/**
 * Notifies the input to stop producing events. Inputs stop both asynchronously and cooperatively. Use the
 * {@link #awaitStop()} method to block until the input has completed the stop process.
 */
", ,/** * Notifies the input to stop producing events. Inputs stop both asynchronously and cooperatively. Use the * {@link #awaitStop()} method to block until the input has completed the stop process. */,54,54,[0],0,[0],0,[0],0,0,0,0,stop(),co.elastic.logstash.api.Input,stop/0,False,50,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,0,0,True
27,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Input.java,co.elastic.logstash.api.Input,void awaitStop(),"/**
 * Blocks until the input has stopped producing events. Note that this method should <b>not</b> signal the
 * input to stop as the {@link #stop()} method does.
 * @throws InterruptedException On Interrupt
 */
void awaitStop() throws InterruptedException;","/**
 * Blocks until the input has stopped producing events. Note that this method should <b>not</b> signal the
 * input to stop as the {@link #stop()} method does.
 * @throws InterruptedException On Interrupt
 */
", ,/** * Blocks until the input has stopped producing events. Note that this method should <b>not</b> signal the * input to stop as the {@link #stop()} method does. * @throws InterruptedException On Interrupt */,61,61,[0],0,[0],0,[0],0,0,0,0,awaitStop(),co.elastic.logstash.api.Input,awaitStop/0,False,56,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,22,0,0,True
28,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Metric.java,co.elastic.logstash.api.Metric,NamespacedMetric namespace(String...),"/**
 * Creates a namespace under the current {@link Metric} and returns it.
 *
 * @param key namespace to traverse into
 * @return the {@code key} namespace under the current Metric
 */
NamespacedMetric namespace(String... key);","/**
 * Creates a namespace under the current {@link Metric} and returns it.
 *
 * @param key namespace to traverse into
 * @return the {@code key} namespace under the current Metric
 */
", ,/** * Creates a namespace under the current {@link Metric} and returns it. * * @param key namespace to traverse into * @return the {@code key} namespace under the current Metric */,33,33,[0],0,[0],0,[0],0,0,0,0,namespace(String[]),co.elastic.logstash.api.Metric,namespace/1[java.lang.String[]],False,27,1,2,2,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,0,0,True
29,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\NamespacedMetric.java,co.elastic.logstash.api.NamespacedMetric,"void gauge(String, Object)","/**
 * Writes an absolute value to the {@code metric}.
 *
 * @param metric metric to write value to
 * @param value value to write
 */
void gauge(String metric, Object value);","/**
 * Writes an absolute value to the {@code metric}.
 *
 * @param metric metric to write value to
 * @param value value to write
 */
", ,/** * Writes an absolute value to the {@code metric}. * * @param metric metric to write value to * @param value value to write */,36,36,[0],0,[0],0,[0],0,0,0,0,"gauge(String, Object)",co.elastic.logstash.api.NamespacedMetric,"gauge/2[java.lang.String,java.lang.Object]",False,30,0,1,1,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,0,0,True
30,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\NamespacedMetric.java,co.elastic.logstash.api.NamespacedMetric,CounterMetric counter(String),"/**
 * Creates a counter with the name {@code metric}.
 *
 * @param metric name of the counter
 * @return an instance tracking a counter metric allowing easy incrementing and resetting
 */
CounterMetric counter(String metric);","/**
 * Creates a counter with the name {@code metric}.
 *
 * @param metric name of the counter
 * @return an instance tracking a counter metric allowing easy incrementing and resetting
 */
", ,/** * Creates a counter with the name {@code metric}. * * @param metric name of the counter * @return an instance tracking a counter metric allowing easy incrementing and resetting */,44,44,[0],0,[0],0,[0],0,0,0,0,counter(String),co.elastic.logstash.api.NamespacedMetric,counter/1[java.lang.String],False,38,1,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,18,0,0,True
31,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\NamespacedMetric.java,co.elastic.logstash.api.NamespacedMetric,void increment(String),"/**
 * Increment the {@code metric} metric by 1.
 *
 * @param metric metric to increment
 */
void increment(String metric);","/**
 * Increment the {@code metric} metric by 1.
 *
 * @param metric metric to increment
 */
", ,/** * Increment the {@code metric} metric by 1. * * @param metric metric to increment */,51,51,[0],0,[0],0,[0],0,0,0,0,increment(String),co.elastic.logstash.api.NamespacedMetric,increment/1[java.lang.String],False,46,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,True
32,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\NamespacedMetric.java,co.elastic.logstash.api.NamespacedMetric,"void increment(String, int)","/**
 * Increment the {@code metric} by {@code delta}.
 *
 * @param metric metric to increment
 * @param delta amount to increment by
 */
void increment(String metric, int delta);","/**
 * Increment the {@code metric} by {@code delta}.
 *
 * @param metric metric to increment
 * @param delta amount to increment by
 */
", ,/** * Increment the {@code metric} by {@code delta}. * * @param metric metric to increment * @param delta amount to increment by */,59,59,[0],0,[0],0,[0],0,0,0,0,"increment(String, int)",co.elastic.logstash.api.NamespacedMetric,"increment/2[java.lang.String,int]",False,53,0,0,0,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,0,0,True
33,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\NamespacedMetric.java,co.elastic.logstash.api.NamespacedMetric,"T time(String, Supplier<T>)","/**
 * Times the {@code callable} and returns its value and increments the
 * {@code metric} with the time taken.
 *
 * @param metric metric to increment
 * @param callable callable to time
 * @param <T> return type of the {@code callable}
 * @return the return value from the {@code callable}
 */
<T> T time(String metric, Supplier<T> callable);","/**
 * Times the {@code callable} and returns its value and increments the
 * {@code metric} with the time taken.
 *
 * @param metric metric to increment
 * @param callable callable to time
 * @param <T> return type of the {@code callable}
 * @return the return value from the {@code callable}
 */
", ,/** * Times the {@code callable} and returns its value and increments the * {@code metric} with the time taken. * * @param metric metric to increment * @param callable callable to time * @param <T> return type of the {@code callable} * @return the return value from the {@code callable} */,70,70,[0],0,[0],0,[0],0,0,0,0,"time(String, Supplier<T>)",co.elastic.logstash.api.NamespacedMetric,"time/2[java.lang.String,java.util.function.Supplier<T>]",False,61,1,0,0,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,17,0,0,True
34,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\NamespacedMetric.java,co.elastic.logstash.api.NamespacedMetric,"void reportTime(String, long)","/**
 * Increments the {@code metric} by {@code duration}.
 *
 * @param metric metric to increment
 * @param duration duration to increment by
 */
void reportTime(String metric, long duration);","/**
 * Increments the {@code metric} by {@code duration}.
 *
 * @param metric metric to increment
 * @param duration duration to increment by
 */
", ,/** * Increments the {@code metric} by {@code duration}. * * @param metric metric to increment * @param duration duration to increment by */,78,78,[0],0,[0],0,[0],0,0,0,0,"reportTime(String, long)",co.elastic.logstash.api.NamespacedMetric,"reportTime/2[java.lang.String,long]",False,72,0,0,0,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,0,0,True
35,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\NamespacedMetric.java,co.elastic.logstash.api.NamespacedMetric,String[] namespaceName(),"/**
 * Retrieves each namespace component that makes up this metric.
 *
 * @return the namespaces this metric is nested within
 */
String[] namespaceName();","/**
 * Retrieves each namespace component that makes up this metric.
 *
 * @return the namespaces this metric is nested within
 */
", ,/** * Retrieves each namespace component that makes up this metric. * * @return the namespaces this metric is nested within */,85,85,[0],0,[0],0,[0],0,0,0,0,namespaceName(),co.elastic.logstash.api.NamespacedMetric,namespaceName/0,False,80,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,0,0,True
36,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\NamespacedMetric.java,co.elastic.logstash.api.NamespacedMetric,Metric root(),"/**
 * Gets Logstash's root metric namespace.
 *
 * @return the root namespace
 */
Metric root();","/**
 * Gets Logstash's root metric namespace.
 *
 * @return the root namespace
 */
", ,/** * Gets Logstash's root metric namespace. * * @return the root namespace */,92,92,[0],0,[0],0,[0],0,0,0,0,root(),co.elastic.logstash.api.NamespacedMetric,root/0,False,87,1,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,True
37,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Output.java,co.elastic.logstash.api.Output,void output(Collection<Event>),"/**
 * Outputs Collection of {@link Event}.
 * @param events Events to be sent through the output.
 */
void output(Collection<Event> events);","/**
 * Outputs Collection of {@link Event}.
 * @param events Events to be sent through the output.
 */
", ,/** * Outputs Collection of {@link Event}. * @param events Events to be sent through the output. */,35,35,[0],0,[0],0,[0],0,0,0,0,output(Collection<Event>),co.elastic.logstash.api.Output,output/1[java.util.Collection<co.elastic.logstash.api.Event>],False,31,1,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,12,0,0,True
38,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Output.java,co.elastic.logstash.api.Output,void stop(),"/**
 * Notifies the output to stop sending events. Outputs with connections to external systems or other resources
 * requiring cleanup should perform those tasks upon a stop notification. Outputs stop both asynchronously and
 * cooperatively. Use the {@link #awaitStop()} method to block until an output has completed the stop process.
 */
void stop();","/**
 * Notifies the output to stop sending events. Outputs with connections to external systems or other resources
 * requiring cleanup should perform those tasks upon a stop notification. Outputs stop both asynchronously and
 * cooperatively. Use the {@link #awaitStop()} method to block until an output has completed the stop process.
 */
", ,/** * Notifies the output to stop sending events. Outputs with connections to external systems or other resources * requiring cleanup should perform those tasks upon a stop notification. Outputs stop both asynchronously and * cooperatively. Use the {@link #awaitStop()} method to block until an output has completed the stop process. */,42,42,[0],0,[0],0,[0],0,0,0,0,stop(),co.elastic.logstash.api.Output,stop/0,False,37,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,0,0,True
39,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Output.java,co.elastic.logstash.api.Output,void awaitStop(),"/**
 * Blocks until the output has stopped sending events. Note that this method should <b>not</b> signal the
 * output to stop as the {@link #stop()} method does.
 * @throws InterruptedException On Interrupt
 */
void awaitStop() throws InterruptedException;","/**
 * Blocks until the output has stopped sending events. Note that this method should <b>not</b> signal the
 * output to stop as the {@link #stop()} method does.
 * @throws InterruptedException On Interrupt
 */
", ,/** * Blocks until the output has stopped sending events. Note that this method should <b>not</b> signal the * output to stop as the {@link #stop()} method does. * @throws InterruptedException On Interrupt */,49,49,[0],0,[0],0,[0],0,0,0,0,awaitStop(),co.elastic.logstash.api.Output,awaitStop/0,False,44,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,22,0,0,True
40,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Password.java,co.elastic.logstash.api.Password,String getValue(),"// Ruby code compatibility, value attribute
public String getValue() {
    return getPassword();
}","// Ruby code compatibility, value attribute
", ,"// Ruby code compatibility, value attribute",48,50,[0],0,[0],0,[0],0,0,0,0,getValue(),co.elastic.logstash.api.Password,getValue/0,False,48,1,1,0,1,1,1,3,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,3,1,0,False
41,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Password.java,co.elastic.logstash.api.Password,String inspect(),"// Ruby code compatibility, inspect method
public String inspect() {
    return toString();
}","// Ruby code compatibility, inspect method
", ,"// Ruby code compatibility, inspect method",53,55,[0],0,[0],0,[0],0,0,0,0,inspect(),co.elastic.logstash.api.Password,inspect/0,False,53,1,1,0,1,1,1,3,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,3,1,0,False
42,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Plugin.java,co.elastic.logstash.api.Plugin,Collection<PluginConfigSpec<?>> configSchema(),"/**
 * Provides all valid settings for this plugin as a collection of {@link PluginConfigSpec}. This will be used
 * to validate against the configuration settings that are supplied to this plugin at runtime.
 * @return Valid settings for this plugin.
 */
Collection<PluginConfigSpec<?>> configSchema();","/**
 * Provides all valid settings for this plugin as a collection of {@link PluginConfigSpec}. This will be used
 * to validate against the configuration settings that are supplied to this plugin at runtime.
 * @return Valid settings for this plugin.
 */
", ,/** * Provides all valid settings for this plugin as a collection of {@link PluginConfigSpec}. This will be used * to validate against the configuration settings that are supplied to this plugin at runtime. * @return Valid settings for this plugin. */,35,35,[0],0,[0],0,[0],0,0,0,0,configSchema(),co.elastic.logstash.api.Plugin,configSchema/0,False,30,1,2,2,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,28,0,0,True
43,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Plugin.java,co.elastic.logstash.api.Plugin,String getName(),"/**
 * @return Name for this plugin. The default implementation uses the name specified in the {@link LogstashPlugin}
 * annotation, if available, and the class name otherwise.
 */
default String getName() {
    LogstashPlugin annotation = getClass().getDeclaredAnnotation(LogstashPlugin.class);
    return (annotation != null && !annotation.name().equals("""")) ? annotation.name() : getClass().getName();
}","/**
 * @return Name for this plugin. The default implementation uses the name specified in the {@link LogstashPlugin}
 * annotation, if available, and the class name otherwise.
 */
", ,"/** * @return Name for this plugin. The default implementation uses the name specified in the {@link LogstashPlugin} * annotation, if available, and the class name otherwise. */",41,46,[0],0,[0],0,[0],0,0,0,0,getName(),co.elastic.logstash.api.Plugin,getName/0,False,41,1,13,12,1,3,5,4,1,1,0,5,0,0,0,1,0,1,1,0,1,0,0,0,0,0,15,65536,0,True
44,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\Plugin.java,co.elastic.logstash.api.Plugin,String getId(),"/**
 * @return ID for the plugin. Input, filter, and output plugins must return the ID value that was supplied
 * to them at construction time. Codec plugins should generally create their own UUID at instantiation time
 * and supply that as their ID.
 */
String getId();","/**
 * @return ID for the plugin. Input, filter, and output plugins must return the ID value that was supplied
 * to them at construction time. Codec plugins should generally create their own UUID at instantiation time
 * and supply that as their ID.
 */
", ,"/** * @return ID for the plugin. Input, filter, and output plugins must return the ID value that was supplied * to them at construction time. Codec plugins should generally create their own UUID at instantiation time * and supply that as their ID. */",53,53,[0],0,[0],0,[0],0,0,0,0,getId(),co.elastic.logstash.api.Plugin,getId/0,False,48,0,9,9,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,27,0,0,True
45,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\PluginHelper.java,co.elastic.logstash.api.PluginHelper,Collection<PluginConfigSpec<?>> commonInputSettings(),"/**
 * @return Settings that are common to all input plugins.
 */
public static Collection<PluginConfigSpec<?>> commonInputSettings() {
    return Arrays.asList(ADD_FIELD_CONFIG, ENABLE_METRIC_CONFIG, CODEC_CONFIG, ID_CONFIG, TAGS_CONFIG, TYPE_CONFIG);
}","/**
 * @return Settings that are common to all input plugins.
 */
", ,/** * @return Settings that are common to all input plugins. */,68,71,[0],0,[0],0,[0],0,0,0,0,commonInputSettings(),co.elastic.logstash.api.PluginHelper,commonInputSettings/0,False,68,1,1,1,0,1,1,3,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,9,0,True
46,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\PluginHelper.java,co.elastic.logstash.api.PluginHelper,Collection<PluginConfigSpec<?>> commonInputSettings(Collection<PluginConfigSpec<?>>),"/**
 * Combines the provided list of settings with the settings that are common to all input plugins
 * ignoring any that are already present in the provided list. This allows plugins to override
 * defaults and other values on the common config settings.
 * @param settings provided list of settings.
 * @return combined list of settings.
 */
public static Collection<PluginConfigSpec<?>> commonInputSettings(Collection<PluginConfigSpec<?>> settings) {
    return combineSettings(settings, commonInputSettings());
}","/**
 * Combines the provided list of settings with the settings that are common to all input plugins
 * ignoring any that are already present in the provided list. This allows plugins to override
 * defaults and other values on the common config settings.
 * @param settings provided list of settings.
 * @return combined list of settings.
 */
", ,/** * Combines the provided list of settings with the settings that are common to all input plugins * ignoring any that are already present in the provided list. This allows plugins to override * defaults and other values on the common config settings. * @param settings provided list of settings. * @return combined list of settings. */,80,82,[0],0,[0],0,[0],0,0,0,0,commonInputSettings(Collection<PluginConfigSpec<?>>),co.elastic.logstash.api.PluginHelper,commonInputSettings/1[java.util.Collection<co.elastic.logstash.api.PluginConfigSpec<?>>],False,80,2,4,2,2,1,2,3,1,0,1,2,2,1,0,0,0,0,0,0,0,0,0,0,0,0,32,9,0,True
47,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\PluginHelper.java,co.elastic.logstash.api.PluginHelper,Collection<PluginConfigSpec<?>> commonOutputSettings(),"/**
 * @return Settings that are common to all output plugins.
 */
public static Collection<PluginConfigSpec<?>> commonOutputSettings() {
    return Arrays.asList(ENABLE_METRIC_CONFIG, CODEC_CONFIG, ID_CONFIG);
}","/**
 * @return Settings that are common to all output plugins.
 */
", ,/** * @return Settings that are common to all output plugins. */,87,89,[0],0,[0],0,[0],0,0,0,0,commonOutputSettings(),co.elastic.logstash.api.PluginHelper,commonOutputSettings/0,False,87,1,2,2,0,1,1,3,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,9,0,True
48,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\PluginHelper.java,co.elastic.logstash.api.PluginHelper,Collection<PluginConfigSpec<?>> commonOutputSettings(Collection<PluginConfigSpec<?>>),"/**
 * Combines the provided list of settings with the settings that are common to all output plugins
 * ignoring any that are already present in the provided list. This allows plugins to override
 * defaults and other values on the common config settings.
 * @param settings provided list of settings.
 * @return combined list of settings.
 */
public static Collection<PluginConfigSpec<?>> commonOutputSettings(Collection<PluginConfigSpec<?>> settings) {
    return combineSettings(settings, commonOutputSettings());
}","/**
 * Combines the provided list of settings with the settings that are common to all output plugins
 * ignoring any that are already present in the provided list. This allows plugins to override
 * defaults and other values on the common config settings.
 * @param settings provided list of settings.
 * @return combined list of settings.
 */
", ,/** * Combines the provided list of settings with the settings that are common to all output plugins * ignoring any that are already present in the provided list. This allows plugins to override * defaults and other values on the common config settings. * @param settings provided list of settings. * @return combined list of settings. */,98,100,[0],0,[0],0,[0],0,0,0,0,commonOutputSettings(Collection<PluginConfigSpec<?>>),co.elastic.logstash.api.PluginHelper,commonOutputSettings/1[java.util.Collection<co.elastic.logstash.api.PluginConfigSpec<?>>],False,98,2,3,1,2,1,2,3,1,0,1,2,2,1,0,0,0,0,0,0,0,0,0,0,0,0,32,9,0,True
49,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\PluginHelper.java,co.elastic.logstash.api.PluginHelper,Collection<PluginConfigSpec<?>> commonFilterSettings(),"/**
 * @return Settings that are common to all filter plugins.
 */
public static Collection<PluginConfigSpec<?>> commonFilterSettings() {
    return Arrays.asList(ADD_FIELD_CONFIG, ADD_TAG_CONFIG, ENABLE_METRIC_CONFIG, ID_CONFIG, PERIODIC_FLUSH_CONFIG, REMOVE_FIELD_CONFIG, REMOVE_TAG_CONFIG);
}","/**
 * @return Settings that are common to all filter plugins.
 */
", ,/** * @return Settings that are common to all filter plugins. */,105,108,[0],0,[0],0,[0],0,0,0,0,commonFilterSettings(),co.elastic.logstash.api.PluginHelper,commonFilterSettings/0,False,105,1,1,1,0,1,1,3,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,9,0,True
50,..\projects\logstash-8.5.2\logstash-core\src\main\java\co\elastic\logstash\api\PluginHelper.java,co.elastic.logstash.api.PluginHelper,Collection<PluginConfigSpec<?>> commonFilterSettings(Collection<PluginConfigSpec<?>>),"/**
 * Combines the provided list of settings with the settings that are common to all filter plugins
 * ignoring any that are already present in the provided list. This allows plugins to override
 * defaults and other values on the common config settings.
 * @param settings provided list of settings.
 * @return combined list of settings.
 */
public static Collection<PluginConfigSpec<?>> commonFilterSettings(Collection<PluginConfigSpec<?>> settings) {
    return combineSettings(settings, commonFilterSettings());
}","/**
 * Combines the provided list of settings with the settings that are common to all filter plugins
 * ignoring any that are already present in the provided list. This allows plugins to override
 * defaults and other values on the common config settings.
 * @param settings provided list of settings.
 * @return combined list of settings.
 */
", ,/** * Combines the provided list of settings with the settings that are common to all filter plugins * ignoring any that are already present in the provided list. This allows plugins to override * defaults and other values on the common config settings. * @param settings provided list of settings. * @return combined list of settings. */,117,119,[0],0,[0],0,[0],0,0,0,0,commonFilterSettings(Collection<PluginConfigSpec<?>>),co.elastic.logstash.api.PluginHelper,commonFilterSettings/1[java.util.Collection<co.elastic.logstash.api.PluginConfigSpec<?>>],False,117,2,3,1,2,1,2,3,1,0,1,2,2,1,0,0,0,0,0,0,0,0,0,0,0,0,32,9,0,True
51,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Accessors.java,org.logstash.Accessors,"void appendAtIndex(ConvertedList, Object, int, int)","private static void appendAtIndex(final ConvertedList list, final Object value, final int index, final int size) {
    // grow array by adding trailing null items
    // this strategy reflects legacy Ruby impl behaviour and is backed by specs
    // TODO: (colin) this is potentially dangerous, and could produce OOM using arbitrary big numbers
    // TODO: (colin) should be guard against this?
    for (int i = size; i < index; i++) {
        list.add(null);
    }
    list.add(value);
}", ,"// grow array by adding trailing null items
[[SEP]]// this strategy reflects legacy Ruby impl behaviour and is backed by specs
[[SEP]]// TODO: (colin) this is potentially dangerous, and could produce OOM using arbitrary big numbers
[[SEP]]// TODO: (colin) should be guard against this?
","// grow array by adding trailing null items// this strategy reflects legacy Ruby impl behaviour and is backed by specs// TODO: (colin) this is potentially dangerous, and could produce OOM using arbitrary big numbers// TODO: (colin) should be guard against this?",79,89,[0],0,"[0, 0, 1, 1]",1,[1],1,1,1,1,"appendAtIndex(ConvertedList, Object, int, int)",org.logstash.Accessors,"appendAtIndex/4[org.logstash.ConvertedList,java.lang.Object,int,int]",False,80,1,1,1,0,2,1,6,0,1,4,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,11,10,0,False
52,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Accessors.java,org.logstash.Accessors,"int listIndex(int, int)","/**
 * Returns a positive integer offset from a Ruby style positive or negative list index.
 * @param i List index
 * @param size the size of the list
 * @return the positive integer offset for the list given by index i
 */
public static int listIndex(int i, int size) {
    return i < 0 ? size + i : i;
}","/**
 * Returns a positive integer offset from a Ruby style positive or negative list index.
 * @param i List index
 * @param size the size of the list
 * @return the positive integer offset for the list given by index i
 */
", ,/** * Returns a positive integer offset from a Ruby style positive or negative list index. * @param i List index * @param size the size of the list * @return the positive integer offset for the list given by index i */,175,177,[0],0,[0],0,[0],0,0,0,0,"listIndex(int, int)",org.logstash.Accessors,"listIndex/2[int,int]",False,175,0,3,3,0,2,0,3,1,0,2,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,20,9,0,True
53,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Accessors.java,org.logstash.Accessors,"int listIndex(String, int)","/**
 * Returns a positive integer offset for a list of known size.
 * @param key List index (String matching /[0-9]+/)
 * @param size the size of the list
 * @return the positive integer offset for the list given by index i
 */
private static int listIndex(final String key, final int size) {
    return listIndex(Integer.parseInt(key), size);
}","/**
 * Returns a positive integer offset for a list of known size.
 * @param key List index (String matching /[0-9]+/)
 * @param size the size of the list
 * @return the positive integer offset for the list given by index i
 */
", ,/** * Returns a positive integer offset for a list of known size. * @param key List index (String matching /[0-9]+/) * @param size the size of the list * @return the positive integer offset for the list given by index i */,185,187,[0],0,[0],0,[0],0,0,0,0,"listIndex(String, int)",org.logstash.Accessors,"listIndex/2[java.lang.String,int]",False,185,1,3,2,1,1,2,3,1,0,2,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,18,10,0,True
54,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Cloner.java,org.logstash.Cloner,T deep(T),"@SuppressWarnings(""unchecked"")
public static <T> T deep(final T input) {
    if (input instanceof Map<?, ?>) {
        return (T) deepMap((Map<?, ?>) input);
    } else if (input instanceof List<?>) {
        return (T) deepList((List<?>) input);
    } else if (input instanceof RubyString) {
        // new instance but sharing ByteList (until either String is modified)
        return (T) ((RubyString) input).dup();
    } else if (input instanceof Collection<?>) {
        throw new ClassCastException(""unexpected Collection type "" + input.getClass());
    }
    return input;
}", ,"// new instance but sharing ByteList (until either String is modified)
",// new instance but sharing ByteList (until either String is modified),38,52,[0],0,[0],0,[0],0,0,0,0,deep(T),org.logstash.Cloner,deep/1[T],False,39,3,2,0,2,5,4,15,4,0,1,4,0,0,0,0,0,1,2,0,0,1,1,0,0,0,12,9,0,False
55,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ConvertedMap.java,org.logstash.ConvertedMap,String internStringForUseAsKey(String),"/**
 * Returns an equivalent interned string, possibly avoiding the
 * global intern pool.
 *
 * @param candidate the candidate {@link String}
 * @return an interned string from the global String intern pool
 */
static String internStringForUseAsKey(final String candidate) {
    // TODO: replace with LRU cache and/or isolated intern pool
    final String cached = KEY_CACHE.get(candidate);
    if (cached != null) {
        return cached;
    }
    final String interned = candidate.intern();
    if (KEY_CACHE.size() <= 10_000) {
        KEY_CACHE.put(interned, interned);
    }
    return interned;
}","/**
 * Returns an equivalent interned string, possibly avoiding the
 * global intern pool.
 *
 * @param candidate the candidate {@link String}
 * @return an interned string from the global String intern pool
 */
","// TODO: replace with LRU cache and/or isolated intern pool
","/** * Returns an equivalent interned string, possibly avoiding the * global intern pool. * * @param candidate the candidate {@link String} * @return an interned string from the global String intern pool */[[SEP]]// TODO: replace with LRU cache and/or isolated intern pool",59,69,[0],0,[1],1,"[0, 1]",1,1,1,1,internStringForUseAsKey(String),org.logstash.ConvertedMap,internStringForUseAsKey/1[java.lang.String],False,59,0,4,4,0,3,4,11,2,2,1,4,0,0,0,1,0,0,0,1,2,0,1,0,0,0,22,8,0,True
56,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ConvertedMap.java,org.logstash.ConvertedMap,void internStringsForUseAsKeys(String[]),"/**
 * Ensures that the provided {@code String[]} contains only
 * instances that have been {@link ConvertedMap::internStringForUseAsKey},
 * possibly replacing entries with equivalent interned strings.
 *
 * @param candidates an array of non-null strings
 */
static void internStringsForUseAsKeys(final String[] candidates) {
    for (int i = 0; i < candidates.length; i++) {
        candidates[i] = internStringForUseAsKey(candidates[i]);
    }
}","/**
 * Ensures that the provided {@code String[]} contains only
 * instances that have been {@link ConvertedMap::internStringForUseAsKey},
 * possibly replacing entries with equivalent interned strings.
 *
 * @param candidates an array of non-null strings
 */
", ,"/** * Ensures that the provided {@code String[]} contains only * instances that have been {@link ConvertedMap::internStringForUseAsKey}, * possibly replacing entries with equivalent interned strings. * * @param candidates an array of non-null strings */",78,82,[0],0,[0],0,[0],0,0,0,0,internStringsForUseAsKeys(String[]),org.logstash.ConvertedMap,internStringsForUseAsKeys/1[java.lang.String[]],False,78,1,2,1,1,2,1,5,0,1,1,1,1,1,1,0,0,0,0,1,2,0,1,0,0,0,30,8,0,True
57,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ConvertedMap.java,org.logstash.ConvertedMap,"void putInterned(String, Object)","/**
 * <p>Behaves like a standard {@link Map#put(Object, Object)} but without the return value.</p>
 * <p>Only produces correct results if the given {@code key} is an interned {@link String}.</p>
 * @param key Interned String
 * @param value Value to put
 */
public void putInterned(final String key, final Object value) {
    super.put(key, value);
}","/**
 * <p>Behaves like a standard {@link Map#put(Object, Object)} but without the return value.</p>
 * <p>Only produces correct results if the given {@code key} is an interned {@link String}.</p>
 * @param key Interned String
 * @param value Value to put
 */
", ,"/** * <p>Behaves like a standard {@link Map#put(Object, Object)} but without the return value.</p> * <p>Only produces correct results if the given {@code key} is an interned {@link String}.</p> * @param key Interned String * @param value Value to put */",140,142,[0],0,[0],0,[0],0,0,0,0,"putInterned(String, Object)",org.logstash.ConvertedMap,"putInterned/2[java.lang.String,java.lang.Object]",False,140,0,7,7,0,1,1,3,0,0,2,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,23,1,0,True
58,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ConvertedMap.java,org.logstash.ConvertedMap,String convertKey(RubyString),"/**
 * Converts a {@link RubyString} into a {@link String} that is guaranteed to be interned.
 * @param key RubyString to convert
 * @return Interned String
 */
private static String convertKey(final RubyString key) {
    return internStringForUseAsKey(key.asJavaString());
}","/**
 * Converts a {@link RubyString} into a {@link String} that is guaranteed to be interned.
 * @param key RubyString to convert
 * @return Interned String
 */
", ,/** * Converts a {@link RubyString} into a {@link String} that is guaranteed to be interned. * @param key RubyString to convert * @return Interned String */,157,159,[0],0,[0],0,[0],0,0,0,0,convertKey(RubyString),org.logstash.ConvertedMap,convertKey/1[org.logstash.RubyString],False,157,2,2,1,1,1,2,3,1,0,1,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,18,10,0,True
59,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\DLQEntry.java,org.logstash.DLQEntry,byte[] serialize(),"@Override
public byte[] serialize() throws IOException {
    byte[] entryTimeInBytes = entryTime.serialize();
    byte[] eventInBytes = this.event.serialize();
    byte[] pluginTypeBytes = pluginType.getBytes();
    byte[] pluginIdBytes = pluginId.getBytes();
    byte[] reasonBytes = reason.getBytes();
    ByteBuffer buffer = ByteBuffer.allocate(entryTimeInBytes.length + eventInBytes.length + pluginTypeBytes.length + pluginIdBytes.length + reasonBytes.length + // magic number represents the five byte[] + lengths
    (Integer.BYTES * 5));
    putLengthAndBytes(buffer, entryTimeInBytes);
    putLengthAndBytes(buffer, eventInBytes);
    putLengthAndBytes(buffer, pluginTypeBytes);
    putLengthAndBytes(buffer, pluginIdBytes);
    putLengthAndBytes(buffer, reasonBytes);
    return buffer.array();
}", ,"// magic number represents the five byte[] + lengths
",// magic number represents the five byte[] + lengths,69,88,[0],0,[0],0,[0],0,0,0,0,serialize(),org.logstash.DLQEntry,serialize/0,False,70,4,17,14,3,1,6,14,1,6,0,6,1,1,0,0,0,1,0,1,6,2,0,0,0,0,19,1,0,False
60,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Event.java,org.logstash.Event,"void setField(FieldReference, Object)","@SuppressWarnings(""unchecked"")
public void setField(final FieldReference field, final Object value) {
    switch(field.type()) {
        case FieldReference.META_PARENT:
            // ConvertedMap.newFromMap already does valuefication
            this.metadata = ConvertedMap.newFromMap((Map<String, Object>) value);
            break;
        case FieldReference.META_CHILD:
            Accessors.set(metadata, field, Valuefier.convert(value));
            break;
        default:
            Accessors.set(data, field, Valuefier.convert(value));
    }
}", ,"// ConvertedMap.newFromMap already does valuefication
",// ConvertedMap.newFromMap already does valuefication,201,214,[0],0,[0],0,[0],0,0,0,0,"setField(FieldReference, Object)",org.logstash.Event,"setField/2[org.logstash.FieldReference,java.lang.Object]",False,202,4,7,3,4,3,4,12,0,0,2,4,0,0,0,0,0,0,1,0,1,0,1,0,0,0,5,1,0,False
61,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Event.java,org.logstash.Event,Event[] fromJson(String),"/**
 * Map a JSON string into events.
 * @param json input string
 * @return events
 * @throws IOException when (JSON) parsing fails
 */
public static Event[] fromJson(final String json) throws IOException {
    return fromJson(json, BasicEventFactory.INSTANCE);
}","/**
 * Map a JSON string into events.
 * @param json input string
 * @return events
 * @throws IOException when (JSON) parsing fails
 */
", ,/** * Map a JSON string into events. * @param json input string * @return events * @throws IOException when (JSON) parsing fails */,267,269,[0],0,[0],0,[0],0,0,0,0,fromJson(String),org.logstash.Event,fromJson/1[java.lang.String],False,267,1,10,9,1,1,1,3,1,0,1,1,1,2,0,0,0,0,0,0,0,0,0,0,0,0,18,9,0,True
62,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Event.java,org.logstash.Event,"Event[] fromJson(String, EventFactory)","/**
 * Map a JSON string into events.
 * @param json input string
 * @param factory event factory
 * @return events
 * @throws IOException when (JSON) parsing fails
 */
@SuppressWarnings(""unchecked"")
public static Event[] fromJson(final String json, final EventFactory factory) throws IOException {
    // empty/blank json string does not generate an event
    if (json == null || isBlank(json)) {
        return NULL_ARRAY;
    }
    Object o = parseJson(json);
    // we currently only support Map or Array json objects
    if (o instanceof Map) {
        // NOTE: we need to assume the factory returns org.logstash.Event impl
        return new Event[] { (Event) factory.newEvent((Map<? extends Serializable, Object>) o) };
    }
    if (o instanceof List) {
        // Jackson returns an ArrayList
        return fromList((List<Map<String, Object>>) o, factory);
    }
    throw new IOException(""incompatible json object type="" + o.getClass().getName() + "" , only hash map or arrays are supported"");
}","/**
 * Map a JSON string into events.
 * @param json input string
 * @param factory event factory
 * @return events
 * @throws IOException when (JSON) parsing fails
 */
","// empty/blank json string does not generate an event
[[SEP]]// we currently only support Map or Array json objects
[[SEP]]// NOTE: we need to assume the factory returns org.logstash.Event impl
[[SEP]]// Jackson returns an ArrayList
",/** * Map a JSON string into events. * @param json input string * @param factory event factory * @return events * @throws IOException when (JSON) parsing fails */[[SEP]]// empty/blank json string does not generate an event[[SEP]]// we currently only support Map or Array json objects[[SEP]]// NOTE: we need to assume the factory returns org.logstash.Event impl[[SEP]]// Jackson returns an ArrayList,278,296,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,"fromJson(String, EventFactory)",org.logstash.Event,"fromJson/2[java.lang.String,co.elastic.logstash.api.EventFactory]",False,279,2,6,2,4,5,6,13,3,1,2,6,3,1,0,1,0,0,3,0,1,1,1,0,0,0,43,9,0,True
63,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Event.java,org.logstash.Event,Object remove(FieldReference),"public Object remove(final FieldReference field) {
    switch(field.type()) {
        case FieldReference.META_PARENT:
            // removal of metadata is actually emptying metadata.
            final ConvertedMap old_value = ConvertedMap.newFromMap(this.metadata);
            this.metadata = new ConvertedMap();
            return Javafier.deep(old_value);
        case FieldReference.META_CHILD:
            return Accessors.del(metadata, field);
        default:
            return Accessors.del(data, field);
    }
}", ,"// removal of metadata is actually emptying metadata.
",// removal of metadata is actually emptying metadata.,347,359,[0],0,[0],0,[0],0,0,0,0,remove(FieldReference),org.logstash.Event,remove/1[org.logstash.FieldReference],False,347,4,6,1,5,3,4,12,3,1,1,4,0,0,0,0,0,0,0,0,2,0,1,0,0,0,9,1,0,False
64,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Event.java,org.logstash.Event,Timestamp initTimestamp(Object),"private static Timestamp initTimestamp(Object o) {
    if (o == null || o instanceof RubyNil) {
        // most frequent
        return new Timestamp();
    } else {
        return parseTimestamp(o);
    }
}", ,"// most frequent
",// most frequent,397,404,[0],0,[0],0,[0],0,0,0,0,initTimestamp(Object),org.logstash.Event,initTimestamp/1[java.lang.Object],False,397,3,3,1,2,3,1,8,2,0,1,1,1,1,0,1,0,0,0,0,0,0,1,0,0,0,8,10,0,False
65,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Event.java,org.logstash.Event,Timestamp parseTimestamp(Object),"/**
 * Cold path of {@link Event#initTimestamp(Object)}.
 * @param o Object to parse Timestamp out of
 * @return Parsed {@link Timestamp} or {@code null} on failure
 */
private static Timestamp parseTimestamp(final Object o) {
    try {
        if (o instanceof String) {
            // second most frequent
            return new Timestamp((String) o);
        } else if (o instanceof RubyString) {
            return new Timestamp(o.toString());
        } else if (o instanceof JrubyTimestampExtLibrary.RubyTimestamp) {
            return ((JrubyTimestampExtLibrary.RubyTimestamp) o).getTimestamp();
        } else if (o instanceof Timestamp) {
            return (Timestamp) o;
        } else if (o instanceof DateTime) {
            return new Timestamp((DateTime) o);
        } else if (o instanceof Date) {
            return new Timestamp((Date) o);
        } else if (o instanceof RubySymbol) {
            return new Timestamp(((RubySymbol) o).asJavaString());
        } else {
            logger.warn(""Unrecognized "" + TIMESTAMP + "" value type="" + o.getClass().toString());
        }
    } catch (IllegalArgumentException e) {
        logger.warn(""Error parsing "" + TIMESTAMP + "" string value="" + o.toString());
    }
    return null;
}","/**
 * Cold path of {@link Event#initTimestamp(Object)}.
 * @param o Object to parse Timestamp out of
 * @return Parsed {@link Timestamp} or {@code null} on failure
 */
","// second most frequent
",/** * Cold path of {@link Event#initTimestamp(Object)}. * @param o Object to parse Timestamp out of * @return Parsed {@link Timestamp} or {@code null} on failure */[[SEP]]// second most frequent,411,435,[0],0,[0],0,"[0, 0]",0,0,0,0,parseTimestamp(Object),org.logstash.Event,parseTimestamp/1[java.lang.Object],False,411,6,6,1,5,9,6,32,8,0,1,6,0,0,0,0,1,2,4,0,0,2,2,0,0,0,34,10,2,True
66,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Event.java,org.logstash.Event,void tag(String),"@Override
public void tag(final String tag) {
    final Object tags = Accessors.get(data, TAGS_FIELD);
    // short circuit the null case where we know we won't need deduplication step below at the end
    if (tags == null) {
        initTag(tag);
    } else {
        existingTag(Javafier.deep(tags), tag);
    }
}", ,"// short circuit the null case where we know we won't need deduplication step below at the end
",// short circuit the null case where we know we won't need deduplication step below at the end,437,446,[0],0,[0],0,[0],0,0,0,1,tag(String),org.logstash.Event,tag/1[java.lang.String],False,438,3,10,6,4,2,4,9,0,1,1,4,2,3,0,1,0,0,0,0,1,0,1,0,0,0,7,1,0,False
67,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Event.java,org.logstash.Event,void initTag(String),"/**
 * Branch of {@link Event#tag(String)} that handles adding the first tag to this event.
 * @param tag Tag to add
 */
private void initTag(final String tag) {
    final ConvertedList list = new ConvertedList(1);
    list.add(RubyUtil.RUBY.newString(tag));
    Accessors.set(data, TAGS_FIELD, list);
}","/**
 * Branch of {@link Event#tag(String)} that handles adding the first tag to this event.
 * @param tag Tag to add
 */
", ,/** * Branch of {@link Event#tag(String)} that handles adding the first tag to this event. * @param tag Tag to add */,452,456,[0],0,[0],0,[0],0,0,0,0,initTag(String),org.logstash.Event,initTag/1[java.lang.String],False,452,2,3,1,2,1,3,5,0,1,1,3,0,0,0,0,0,0,0,1,1,0,0,0,0,0,15,2,0,True
68,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Event.java,org.logstash.Event,"void existingTag(Object, String)","/**
 * Branch of {@link Event#tag(String)} that handles adding to existing tags.
 * @param tags Existing Tag(s)
 * @param tag Tag to add
 */
@SuppressWarnings(""unchecked"")
private void existingTag(final Object tags, final String tag) {
    if (tags instanceof List) {
        appendTag((List<String>) tags, tag);
    } else {
        scalarTagFallback((String) tags, tag);
    }
}","/**
 * Branch of {@link Event#tag(String)} that handles adding to existing tags.
 * @param tags Existing Tag(s)
 * @param tag Tag to add
 */
", ,/** * Branch of {@link Event#tag(String)} that handles adding to existing tags. * @param tags Existing Tag(s) * @param tag Tag to add */,463,470,[0],0,[0],0,[0],0,0,0,0,"existingTag(Object, String)",org.logstash.Event,"existingTag/2[java.lang.Object,java.lang.String]",False,464,1,3,1,2,2,2,8,0,0,2,2,2,2,0,0,0,0,1,0,0,0,1,0,0,0,18,2,0,True
69,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Event.java,org.logstash.Event,"void appendTag(List<String>, String)","/**
 * Merge the given tag into the given list of existing tags if the list doesn't already contain
 * the tag.
 * @param tags Existing tag list
 * @param tag Tag to add
 */
private void appendTag(final List<String> tags, final String tag) {
    // TODO: we should eventually look into using alternate data structures to do more efficient dedup but that will require properly defining the tagging API too
    if (!tags.contains(tag)) {
        tags.add(tag);
        Accessors.set(data, TAGS_FIELD, ConvertedList.newFromList(tags));
    }
}","/**
 * Merge the given tag into the given list of existing tags if the list doesn't already contain
 * the tag.
 * @param tags Existing tag list
 * @param tag Tag to add
 */
","// TODO: we should eventually look into using alternate data structures to do more efficient dedup but that will require properly defining the tagging API too
",/** * Merge the given tag into the given list of existing tags if the list doesn't already contain * the tag. * @param tags Existing tag list * @param tag Tag to add */[[SEP]]// TODO: we should eventually look into using alternate data structures to do more efficient dedup but that will require properly defining the tagging API too,478,484,[0],0,[1],1,"[0, 1]",1,1,1,1,"appendTag(List<String>, String)",org.logstash.Event,"appendTag/2[java.util.List<java.lang.String>,java.lang.String]",False,478,2,4,2,2,2,4,6,0,0,2,4,0,0,0,0,0,0,0,0,0,0,1,0,0,0,16,2,0,True
70,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Event.java,org.logstash.Event,"void scalarTagFallback(String, String)","/**
 * Fallback for {@link Event#tag(String)} in case ""tags"" was populated by just a String value
 * and needs to be converted to a list before appending to it.
 * @param existing Existing Tag
 * @param tag Tag to add
 */
private void scalarTagFallback(final String existing, final String tag) {
    final List<String> tags = new ArrayList<>(2);
    tags.add(existing);
    appendTag(tags, tag);
}","/**
 * Fallback for {@link Event#tag(String)} in case ""tags"" was populated by just a String value
 * and needs to be converted to a list before appending to it.
 * @param existing Existing Tag
 * @param tag Tag to add
 */
", ,"/** * Fallback for {@link Event#tag(String)} in case ""tags"" was populated by just a String value * and needs to be converted to a list before appending to it. * @param existing Existing Tag * @param tag Tag to add */",492,496,[0],0,[0],0,[0],0,0,0,0,"scalarTagFallback(String, String)",org.logstash.Event,"scalarTagFallback/2[java.lang.String,java.lang.String]",False,492,1,2,1,1,1,2,5,0,1,2,2,1,1,0,0,0,0,0,1,1,0,0,0,0,0,23,2,0,True
71,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\FieldReference.java,org.logstash.FieldReference,FieldReference from(RubyString),"public static FieldReference from(final RubyString reference) {
    // atomicity between the get and put is not important
    final FieldReference result = RUBY_CACHE.get(reference);
    if (result != null) {
        return result;
    }
    final FieldReference parsed = from(reference.asJavaString());
    // exact size in a race condition is not important
    if (RUBY_CACHE.size() < 10_000) {
        RUBY_CACHE.put(reference.newFrozen(), parsed);
    }
    return parsed;
}", ,"// atomicity between the get and put is not important
[[SEP]]// exact size in a race condition is not important
",// atomicity between the get and put is not important[[SEP]]// exact size in a race condition is not important,137,150,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,from(RubyString),org.logstash.FieldReference,from/1[org.logstash.RubyString],False,137,2,5,4,1,3,6,11,2,2,1,6,1,0,0,1,0,0,0,1,2,0,1,0,0,0,11,9,0,False
72,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\FieldReference.java,org.logstash.FieldReference,FieldReference from(String),"public static FieldReference from(final String reference) {
    // atomicity between the get and put is not important
    final FieldReference result = CACHE.get(reference);
    if (result != null) {
        return result;
    }
    return parseToCache(reference);
}", ,"// atomicity between the get and put is not important
",// atomicity between the get and put is not important,152,159,[0],0,[0],0,[0],0,0,0,0,from(String),org.logstash.FieldReference,from/1[java.lang.String],False,152,1,48,47,1,2,2,7,2,1,1,2,1,2,0,1,0,0,0,0,1,0,1,0,0,0,9,9,0,False
73,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\FieldReference.java,org.logstash.FieldReference,int type(),"/**
 * Returns the type of this instance to allow for fast switch operations in
 * {@link Event#getUnconvertedField(FieldReference)} and
 * {@link Event#setField(FieldReference, Object)}.
 * @return Type of the FieldReference
 */
public int type() {
    return type;
}","/**
 * Returns the type of this instance to allow for fast switch operations in
 * {@link Event#getUnconvertedField(FieldReference)} and
 * {@link Event#setField(FieldReference, Object)}.
 * @return Type of the FieldReference
 */
", ,"/** * Returns the type of this instance to allow for fast switch operations in * {@link Event#getUnconvertedField(FieldReference)} and * {@link Event#setField(FieldReference, Object)}. * @return Type of the FieldReference */",176,178,[0],0,[0],0,[0],0,0,0,0,type(),org.logstash.FieldReference,type/0,False,176,0,4,4,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,1,0,True
74,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\FieldReference.java,org.logstash.FieldReference,FieldReference deduplicate(FieldReference),"/**
 * De-duplicates instances using {@link FieldReference#DEDUP}. This method must be
 * {@code synchronized} since we are running non-atomic get-put sequence on
 * {@link FieldReference#DEDUP}.
 * @param parsed FieldReference to de-duplicate
 * @return De-duplicated FieldReference
 */
private static synchronized FieldReference deduplicate(final FieldReference parsed) {
    FieldReference ret = DEDUP.get(parsed);
    if (ret == null) {
        DEDUP.put(parsed, parsed);
        ret = parsed;
    }
    return ret;
}","/**
 * De-duplicates instances using {@link FieldReference#DEDUP}. This method must be
 * {@code synchronized} since we are running non-atomic get-put sequence on
 * {@link FieldReference#DEDUP}.
 * @param parsed FieldReference to de-duplicate
 * @return De-duplicated FieldReference
 */
", ,/** * De-duplicates instances using {@link FieldReference#DEDUP}. This method must be * {@code synchronized} since we are running non-atomic get-put sequence on * {@link FieldReference#DEDUP}. * @param parsed FieldReference to de-duplicate * @return De-duplicated FieldReference */,208,215,[0],0,[0],0,[0],0,0,0,0,deduplicate(FieldReference),org.logstash.FieldReference,deduplicate/1[org.logstash.FieldReference],False,208,1,1,1,0,2,2,8,1,1,1,2,0,0,0,1,0,0,0,0,2,0,1,0,0,0,28,42,0,True
75,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\FieldReference.java,org.logstash.FieldReference,"int calculateHash(String, String[], int)","/**
 * Effective hashcode implementation using knowledge of field types.
 * @param key Key Field
 * @param path Path Field
 * @param type Type Field
 * @return Hash Code
 */
private static int calculateHash(final String key, final String[] path, final int type) {
    final int prime = 31;
    int hash = prime;
    for (final String element : path) {
        hash = prime * hash + element.hashCode();
    }
    hash = prime * hash + key.hashCode();
    return prime * hash + type;
}","/**
 * Effective hashcode implementation using knowledge of field types.
 * @param key Key Field
 * @param path Path Field
 * @param type Type Field
 * @return Hash Code
 */
", ,/** * Effective hashcode implementation using knowledge of field types. * @param key Key Field * @param path Path Field * @param type Type Field * @return Hash Code */,224,232,[0],0,[0],0,[0],0,0,0,0,"calculateHash(String, String[], int)",org.logstash.FieldReference,"calculateHash/3[java.lang.String,java.lang.String[],int]",False,224,0,1,1,0,2,1,9,1,2,3,1,0,0,1,0,0,0,0,1,4,6,1,0,0,0,20,10,0,True
76,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\FieldReference.java,org.logstash.FieldReference.StrictTokenizer,List<String> tokenize(CharSequence),"/**
 * @param reference a sequence of characters representing a reference to a field
 * @return a list of string path fragments.
 */
public List<String> tokenize(final CharSequence reference) {
    ArrayList<String> path = new ArrayList<>();
    final int length = reference.length();
    boolean potentiallyAmbiguousSyntaxDetected = false;
    boolean seenBracket = false;
    int depth = 0;
    int splitPoint = 0;
    char current = 0;
    char previous = 0;
    scan: for (int i = 0; i < length; i++) {
        previous = current;
        current = reference.charAt(i);
        switch(current) {
            case '[':
                seenBracket = true;
                if (splitPoint != i) {
                    // if the current split point isn't the previous character, we have ambiguous input,
                    // such as a mix of square-bracket and top-level unbracketed chunks, or an embedded
                    // field reference that doesn't wholly occupy an outer fragment, and cannot
                    // reasonably recover.
                    potentiallyAmbiguousSyntaxDetected = true;
                    break scan;
                }
                depth++;
                splitPoint = i + 1;
                continue scan;
            case ']':
                seenBracket = true;
                if (depth <= 0) {
                    // if we get to a close-bracket without having previously hit an open-bracket,
                    // we have an illegal field reference and cannot reasonably recover.
                    potentiallyAmbiguousSyntaxDetected = true;
                    break scan;
                }
                if (splitPoint == i && previous != ']') {
                    // if we have a zero-length fragment and are not closing an embedded fieldreference,
                    // we have an illegal field reference and cannot possibly recover.
                    potentiallyAmbiguousSyntaxDetected = true;
                    break scan;
                }
                if (splitPoint < i) {
                    // if we have something to add, add it.
                    path.add(reference.subSequence(splitPoint, i).toString());
                }
                depth--;
                splitPoint = i + 1;
                continue scan;
            default:
                if (seenBracket && previous == ']') {
                    // if we have seen a bracket and encounter one or more characters that are _not_ enclosed
                    // in brackets, we have illegal syntax and cannot reasonably recover.
                    potentiallyAmbiguousSyntaxDetected = true;
                    break scan;
                }
                continue scan;
        }
    }
    if (!seenBracket) {
        // if we saw no brackets, this is a top-level reference that can be emitted as-is without
        // further processing
        path.add(reference.toString());
        return path;
    } else if (depth > 0) {
        // when we hit the end-of-input while still in an open bracket, we have an invalid field reference
        potentiallyAmbiguousSyntaxDetected = true;
    }
    // if we have encountered ambiguous syntax and are not in strict-mode,
    // fall back to legacy parser.
    if (potentiallyAmbiguousSyntaxDetected) {
        throw new FieldReference.IllegalSyntaxException(String.format(""Invalid FieldReference: `%s`"", reference.toString()));
    }
    path.trimToSize();
    return path;
}","/**
 * @param reference a sequence of characters representing a reference to a field
 * @return a list of string path fragments.
 */
","// if we have encountered ambiguous syntax and are not in strict-mode,
[[SEP]]// if the current split point isn't the previous character, we have ambiguous input,
[[SEP]]// such as a mix of square-bracket and top-level unbracketed chunks, or an embedded
[[SEP]]// field reference that doesn't wholly occupy an outer fragment, and cannot
[[SEP]]// reasonably recover.
[[SEP]]// if we get to a close-bracket without having previously hit an open-bracket,
[[SEP]]// we have an illegal field reference and cannot reasonably recover.
[[SEP]]// if we have a zero-length fragment and are not closing an embedded fieldreference,
[[SEP]]// we have an illegal field reference and cannot possibly recover.
[[SEP]]// if we have something to add, add it.
[[SEP]]// if we have seen a bracket and encounter one or more characters that are _not_ enclosed
[[SEP]]// in brackets, we have illegal syntax and cannot reasonably recover.
[[SEP]]// if we saw no brackets, this is a top-level reference that can be emitted as-is without
[[SEP]]// further processing
[[SEP]]// when we hit the end-of-input while still in an open bracket, we have an invalid field reference
[[SEP]]// fall back to legacy parser.
","/** * @param reference a sequence of characters representing a reference to a field * @return a list of string path fragments. */[[SEP]]// if the current split point isn't the previous character, we have ambiguous input,// such as a mix of square-bracket and top-level unbracketed chunks, or an embedded// field reference that doesn't wholly occupy an outer fragment, and cannot// reasonably recover.[[SEP]]// if we get to a close-bracket without having previously hit an open-bracket,// we have an illegal field reference and cannot reasonably recover.[[SEP]]// if we have a zero-length fragment and are not closing an embedded fieldreference,// we have an illegal field reference and cannot possibly recover.[[SEP]]// if we have something to add, add it.[[SEP]]// if we have seen a bracket and encounter one or more characters that are _not_ enclosed// in brackets, we have illegal syntax and cannot reasonably recover.[[SEP]]// if we saw no brackets, this is a top-level reference that can be emitted as-is without// further processing[[SEP]]// when we hit the end-of-input while still in an open bracket, we have an invalid field reference[[SEP]]// if we have encountered ambiguous syntax and are not in strict-mode,// fall back to legacy parser.",271,355,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0, 0, 0, 0]",0,0,0,0,tokenize(CharSequence),org.logstash.FieldReference$StrictTokenizer,tokenize/1[java.lang.CharSequence],False,271,1,2,1,1,14,7,59,2,9,1,7,0,0,1,4,0,0,1,9,20,2,3,0,0,0,30,1,0,True
77,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\FileLockFactory.java,org.logstash.FileLockFactory,"FileLock obtainLock(Path, String)","public static FileLock obtainLock(Path dirPath, String lockName) throws IOException {
    if (!Files.isDirectory(dirPath)) {
        Files.createDirectories(dirPath);
    }
    Path lockPath = dirPath.resolve(lockName);
    try {
        Files.createFile(lockPath);
    } catch (IOException ignore) {
        // we must create the file to have a truly canonical path.
        // if it's already created, we don't care. if it can't be created, it will fail below.
    }
    // fails if the lock file does not exist
    final Path realLockPath = lockPath.toRealPath();
    if (!LOCK_HELD.add(realLockPath.toString())) {
        throw new LockException(""Lock held by this virtual machine on lock path: "" + realLockPath);
    }
    FileChannel channel = null;
    FileLock lock;
    try {
        channel = FileChannel.open(realLockPath, StandardOpenOption.CREATE, StandardOpenOption.WRITE);
        lock = channel.tryLock();
        if (lock == null) {
            throw new LockException(""Lock held by another program on lock path: "" + realLockPath);
        }
    } catch (IOException ex) {
        try {
            if (channel != null) {
                channel.close();
            }
        } catch (Throwable t) {
            // suppress any channel close exceptions
        }
        boolean removed = LOCK_HELD.remove(realLockPath.toString());
        if (!removed) {
            throw new LockException(""Lock path was cleared but never marked as held: "" + realLockPath, ex);
        }
        throw ex;
    }
    LOCK_MAP.put(lock, realLockPath.toString());
    return lock;
}", ,"// we must create the file to have a truly canonical path.
[[SEP]]// if it's already created, we don't care. if it can't be created, it will fail below.
[[SEP]]// fails if the lock file does not exist
[[SEP]]// suppress any channel close exceptions
","// we must create the file to have a truly canonical path.// if it's already created, we don't care. if it can't be created, it will fail below.[[SEP]]// fails if the lock file does not exist[[SEP]]// suppress any channel close exceptions",71,117,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,"obtainLock(Path, String)",org.logstash.FileLockFactory,"obtainLock/2[java.nio.file.Path,java.lang.String]",False,71,1,12,10,2,9,12,40,1,5,2,12,0,0,0,2,3,0,3,0,6,3,3,0,0,0,33,9,0,False
78,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\JavaVersionUtils.java,org.logstash.JavaVersionUtils,boolean isJavaAtLeast(int),"/**
 * Identifies whether we are running on a version greater than or equal to the version parameter specified.
 * @param version The version to test against. This must be the Major version of Java
 * @return True if running on Java whose major version is greater than or equal to the
 *         specified version.
 */
public static boolean isJavaAtLeast(int version) {
    final String value = System.getProperty(""java.specification.version"");
    final int actualVersion;
    // Java specification version prior to Java 9 were of the format `1.X`, and after the format `X`
    // See https://openjdk.java.net/jeps/223
    if (value.startsWith(""1."")) {
        actualVersion = Integer.parseInt(value.split(""\\."")[1]);
    } else {
        actualVersion = Integer.parseInt(value);
    }
    return actualVersion >= version;
}","/**
 * Identifies whether we are running on a version greater than or equal to the version parameter specified.
 * @param version The version to test against. This must be the Major version of Java
 * @return True if running on Java whose major version is greater than or equal to the
 *         specified version.
 */
","// Java specification version prior to Java 9 were of the format `1.X`, and after the format `X`
[[SEP]]// See https://openjdk.java.net/jeps/223
","/** * Identifies whether we are running on a version greater than or equal to the version parameter specified. * @param version The version to test against. This must be the Major version of Java * @return True if running on Java whose major version is greater than or equal to the *         specified version. */[[SEP]]// Java specification version prior to Java 9 were of the format `1.X`, and after the format `X`// See https://openjdk.java.net/jeps/223",33,44,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,isJavaAtLeast(int),org.logstash.JavaVersionUtils,isJavaAtLeast/1[int],False,33,0,1,1,0,3,4,11,1,2,1,4,0,0,0,0,0,0,3,1,3,0,1,0,0,0,33,9,0,True
79,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Javafier.java,org.logstash.Javafier,"Map<Class<?>, Valuefier.Converter> initConverters()","private static Map<Class<?>, Valuefier.Converter> initConverters() {
    final Map<Class<?>, Valuefier.Converter> converters = new ConcurrentHashMap<>(50, 0.2F, 1);
    converters.put(String.class, Valuefier.IDENTITY);
    converters.put(Float.class, Valuefier.IDENTITY);
    converters.put(RubyNil.class, value -> null);
    converters.put(Double.class, Valuefier.IDENTITY);
    converters.put(Long.class, Valuefier.IDENTITY);
    converters.put(Integer.class, Valuefier.IDENTITY);
    converters.put(Boolean.class, Valuefier.IDENTITY);
    converters.put(BigInteger.class, Valuefier.IDENTITY);
    converters.put(BigDecimal.class, Valuefier.IDENTITY);
    converters.put(Timestamp.class, Valuefier.IDENTITY);
    // Explicitly casting to RubyString or RubySymbol when we know its type for sure is faster
    // than having the JVM look up the type.
    converters.put(RubyString.class, value -> ((RubyString) value).toString());
    converters.put(RubySymbol.class, value -> ((RubySymbol) value).toString());
    converters.put(RubyBignum.class, value -> ((RubyBignum) value).getBigIntegerValue());
    converters.put(RubyBigDecimal.class, value -> ((RubyBigDecimal) value).getBigDecimalValue());
    converters.put(RubyBoolean.class, value -> ((RubyBoolean) value).isTrue());
    converters.put(RubyFixnum.class, value -> ((RubyFixnum) value).getLongValue());
    converters.put(RubyFloat.class, value -> ((RubyFloat) value).getDoubleValue());
    converters.put(ConvertedMap.class, value -> ((ConvertedMap) value).unconvert());
    converters.put(ConvertedList.class, value -> ((ConvertedList) value).unconvert());
    converters.put(JrubyTimestampExtLibrary.RubyTimestamp.class, value -> ((JrubyTimestampExtLibrary.RubyTimestamp) value).getTimestamp());
    return converters;
}", ,"// Explicitly casting to RubyString or RubySymbol when we know its type for sure is faster
[[SEP]]// than having the JVM look up the type.
",// Explicitly casting to RubyString or RubySymbol when we know its type for sure is faster// than having the JVM look up the type.,73,104,[0],0,"[0, 0]",0,[0],0,0,0,0,initConverters(),org.logstash.Javafier,initConverters/0,False,73,13,3,0,3,1,10,24,1,12,0,10,0,0,0,0,0,10,0,3,1,0,0,0,0,11,17,10,0,False
80,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\KeyNode.java,org.logstash.KeyNode,"String join(List<?>, String)","// TODO: (colin) this should be moved somewhere else to make it reusable
// this is a quick fix to compile on JDK7 a not use String.join that is
// only available in JDK8
public static String join(List<?> list, String delim) {
    int len = list.size();
    if (len == 0)
        return """";
    final StringBuilder result = new StringBuilder(toString(list.get(0), delim));
    for (int i = 1; i < len; i++) {
        result.append(delim);
        result.append(toString(list.get(i), delim));
    }
    return result.toString();
}","// only available in JDK8
", ,// TODO: (colin) this should be moved somewhere else to make it reusable// this is a quick fix to compile on JDK7 a not use String.join that is// only available in JDK8,34,45,[0],0,[0],0,[1],1,1,1,1,"join(List<?>, String)",org.logstash.KeyNode,"join/2[java.util.List<?>,java.lang.String]",False,34,1,10,9,1,3,5,10,2,3,2,5,1,1,1,1,0,0,1,3,3,0,1,0,0,0,8,9,0,False
81,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Logstash.java,org.logstash.Logstash,void main(String...),"/**
 * Main Entrypoint.
 * Requires environment {@code ""LS_HOME""} to be set to the Logstash root directory.
 * @param args Logstash CLI Arguments
 */
public static void main(final String... args) {
    final String lsHome = System.getenv(""LS_HOME"");
    if (lsHome == null) {
        throw new IllegalStateException(""LS_HOME environment variable must be set. This is likely a bug that should be reported."");
    }
    installGlobalUncaughtExceptionHandler();
    final Path home = Paths.get(lsHome).toAbsolutePath();
    try (final Logstash logstash = new Logstash(home, args, System.out, System.err, System.in)) {
        logstash.run();
    } catch (final IllegalStateException e) {
        Throwable t = e;
        String message = e.getMessage();
        if (message != null) {
            if (message.startsWith(UNCLEAN_SHUTDOWN_PREFIX) || message.startsWith(MUTATED_GEMFILE_ERROR)) {
                // be less verbose with uncleanShutdown's wrapping exception
                t = e.getCause();
            } else if (message.contains(""Could not load FFI Provider"")) {
                message = ""Error accessing temp directory: "" + System.getProperty(""java.io.tmpdir"") + "" this often occurs because the temp directory has been mounted with NOEXEC or"" + "" the Logstash user has insufficient permissions on the directory. \n"" + ""Possible workarounds include setting the -Djava.io.tmpdir property in the jvm.options"" + ""file to an alternate directory or correcting the Logstash user's permissions."";
            }
        }
        handleFatalError(message, t);
    } catch (final Throwable t) {
        handleFatalError("""", t);
    }
    System.exit(0);
}","/**
 * Main Entrypoint.
 * Requires environment {@code ""LS_HOME""} to be set to the Logstash root directory.
 * @param args Logstash CLI Arguments
 */
","// be less verbose with uncleanShutdown's wrapping exception
","/** * Main Entrypoint. * Requires environment {@code ""LS_HOME""} to be set to the Logstash root directory. * @param args Logstash CLI Arguments */[[SEP]]// be less verbose with uncleanShutdown's wrapping exception",60,96,[0],0,[0],0,"[0, 0]",0,0,0,0,main(String[]),org.logstash.Logstash,main/1[java.lang.String[]],False,60,1,4,0,4,8,12,28,0,5,1,12,3,3,0,2,1,0,10,1,7,1,3,0,0,0,86,9,0,True
82,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Logstash.java,org.logstash.Logstash,void halt(int),"private static void halt(final int status) {
    // we halt to prevent shutdown hooks from running
    Runtime.getRuntime().halt(status);
}", ,"// we halt to prevent shutdown hooks from running
",// we halt to prevent shutdown hooks from running,130,133,[0],0,[0],0,[0],0,0,0,0,halt(int),org.logstash.Logstash,halt/1[int],False,130,0,1,1,0,1,2,3,0,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,10,0,False
83,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Logstash.java,org.logstash.Logstash,void run(),"@Override
public void run() {
    // @todo: Refactor codebase to not rely on global constant for Ruby Runtime
    if (RubyUtil.RUBY != ruby) {
        throw new IllegalStateException(""More than one JRuby Runtime detected in the current JVM!"");
    }
    final RubyInstanceConfig config = ruby.getInstanceConfig();
    try (InputStream script = config.getScriptSource()) {
        Thread.currentThread().setContextClassLoader(ruby.getJRubyClassLoader());
        ruby.runFromMain(script, config.displayedFileName());
    } catch (final RaiseException ex) {
        final RubyException re = ex.getException();
        // If this is a production error this signifies an issue with the Gemfile, likely
        // that a logstash developer has made changes to their local Gemfile for plugin
        // development, etc. If this is the case, exit with a warning giving remediating
        // information for Logstash devs.
        if (isProductionError(re)) {
            bundlerStartupError(ex);
        }
        if (re instanceof RubySystemExit) {
            IRubyObject success = ((RubySystemExit) re).success_p();
            if (!success.isTrue()) {
                uncleanShutdown(ex);
            }
        } else {
            uncleanShutdown(ex);
        }
    } catch (final IOException ex) {
        uncleanShutdown(ex);
    }
}", ,"// @todo: Refactor codebase to not rely on global constant for Ruby Runtime
[[SEP]]// If this is a production error this signifies an issue with the Gemfile, likely
[[SEP]]// that a logstash developer has made changes to their local Gemfile for plugin
[[SEP]]// development, etc. If this is the case, exit with a warning giving remediating
[[SEP]]// information for Logstash devs.
","// @todo: Refactor codebase to not rely on global constant for Ruby Runtime[[SEP]]// If this is a production error this signifies an issue with the Gemfile, likely// that a logstash developer has made changes to their local Gemfile for plugin// development, etc. If this is the case, exit with a warning giving remediating// information for Logstash devs.",152,186,[0],0,"[1, 0, 0, 0, 0]",1,"[1, 0]",1,1,1,1,run(),org.logstash.Logstash,run/0,False,153,5,4,1,3,7,13,28,0,4,0,13,3,1,0,1,1,1,1,0,4,0,3,0,0,0,38,1,0,False
84,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Logstash.java,org.logstash.Logstash,boolean isProductionError(RubyException),"// Tests whether the RubyException is of type `Bundler::ProductionError`
private boolean isProductionError(RubyException re) {
    if (re instanceof RubyStandardError) {
        RubyClass metaClass = re.getMetaClass();
        return (metaClass.getName().equals(""Bundler::ProductionError""));
    }
    return false;
}","// Tests whether the RubyException is of type `Bundler::ProductionError`
", ,// Tests whether the RubyException is of type `Bundler::ProductionError`,189,195,[0],0,[0],0,[0],0,0,0,0,isProductionError(RubyException),org.logstash.Logstash,isProductionError/1[org.logstash.RubyException],False,189,3,1,1,0,2,3,7,2,1,1,3,0,0,0,0,0,1,1,0,1,0,1,0,0,0,10,2,0,False
85,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Logstash.java,org.logstash.Logstash,"RubyInstanceConfig initRubyConfig(Path, String...)","/**
 * Initialize a runtime configuration.
 * @param lsHome the LOGSTASH_HOME
 * @param args extra arguments (ARGV) to process
 * @return a runtime configuration instance
 */
public static RubyInstanceConfig initRubyConfig(final Path lsHome, final String... args) {
    return initRubyConfigImpl(lsHome, safePath(lsHome, ""vendor"", ""jruby""), args);
}","/**
 * Initialize a runtime configuration.
 * @param lsHome the LOGSTASH_HOME
 * @param args extra arguments (ARGV) to process
 * @return a runtime configuration instance
 */
", ,/** * Initialize a runtime configuration. * @param lsHome the LOGSTASH_HOME * @param args extra arguments (ARGV) to process * @return a runtime configuration instance */,208,211,[0],0,[0],0,[0],0,0,0,0,"initRubyConfig(Path, String[])",org.logstash.Logstash,"initRubyConfig/2[java.nio.file.Path,java.lang.String[]]",False,209,2,3,1,2,1,2,3,1,0,2,2,2,1,0,0,0,0,2,0,0,0,0,0,0,0,31,9,0,True
86,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Logstash.java,org.logstash.Logstash,"RubyInstanceConfig initRubyConfig(Path, Path, String...)","/**
 * Initialize a runtime configuration.
 * @param lsHome the LOGSTASH_HOME
 * @param args extra arguments (ARGV) to process
 * @return a runtime configuration instance
 */
public static RubyInstanceConfig initRubyConfig(final Path lsHome, final Path currentDir, final String... args) {
    return initRubyConfigImpl(currentDir, safePath(lsHome, ""vendor"", ""jruby""), args);
}","/**
 * Initialize a runtime configuration.
 * @param lsHome the LOGSTASH_HOME
 * @param args extra arguments (ARGV) to process
 * @return a runtime configuration instance
 */
", ,/** * Initialize a runtime configuration. * @param lsHome the LOGSTASH_HOME * @param args extra arguments (ARGV) to process * @return a runtime configuration instance */,219,224,[0],0,[0],0,[0],0,0,0,0,"initRubyConfig(Path, Path, String[])",org.logstash.Logstash,"initRubyConfig/3[java.nio.file.Path,java.nio.file.Path,java.lang.String[]]",False,221,2,2,0,2,1,2,3,1,0,3,2,2,1,0,0,0,0,2,0,0,0,0,0,0,0,31,9,0,True
87,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Logstash.java,org.logstash.Logstash,"RubyInstanceConfig buildConfig(Path, String[])","/**
 * Sets up the correct {@link RubyInstanceConfig} for a given Logstash installation and set of
 * CLI arguments.
 * @param home Logstash Root Path
 * @param args Commandline Arguments Passed to Logstash
 * @return RubyInstanceConfig
 */
private static RubyInstanceConfig buildConfig(final Path home, final String[] args) {
    final String[] arguments = new String[args.length + 1];
    System.arraycopy(args, 0, arguments, 1, args.length);
    arguments[0] = safePath(home, ""lib"", ""bootstrap"", ""environment.rb"");
    return initRubyConfig(home, arguments);
}","/**
 * Sets up the correct {@link RubyInstanceConfig} for a given Logstash installation and set of
 * CLI arguments.
 * @param home Logstash Root Path
 * @param args Commandline Arguments Passed to Logstash
 * @return RubyInstanceConfig
 */
", ,/** * Sets up the correct {@link RubyInstanceConfig} for a given Logstash installation and set of * CLI arguments. * @param home Logstash Root Path * @param args Commandline Arguments Passed to Logstash * @return RubyInstanceConfig */,243,248,[0],0,[0],0,[0],0,0,0,0,"buildConfig(Path, String[])",org.logstash.Logstash,"buildConfig/2[java.nio.file.Path,java.lang.String[]]",False,243,2,3,1,2,1,3,6,1,1,2,3,2,2,0,0,0,0,3,4,2,1,0,0,0,0,29,10,0,True
88,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Logstash.java,org.logstash.Logstash,"String safePath(Path, String...)","/**
 * Builds the correct path for a file under the given Logstash root and defined by its sub path
 * elements relative to the Logstash root.
 * Ensures that the file exists and throws an exception of it's missing.
 * This is done to avoid hard to interpret errors thrown by JRuby that could result from missing
 * Ruby bootstrap scripts.
 * @param home Logstash Root Path
 * @param subs Path elements relative to {@code home}
 * @return Absolute Path a File under the Logstash Root.
 */
private static String safePath(final Path home, final String... subs) {
    Path resolved = home;
    for (final String element : subs) {
        resolved = resolved.resolve(element);
    }
    if (!resolved.toFile().exists()) {
        throw new IllegalArgumentException(String.format(""Missing: %s."", resolved));
    }
    return resolved.toString();
}","/**
 * Builds the correct path for a file under the given Logstash root and defined by its sub path
 * elements relative to the Logstash root.
 * Ensures that the file exists and throws an exception of it's missing.
 * This is done to avoid hard to interpret errors thrown by JRuby that could result from missing
 * Ruby bootstrap scripts.
 * @param home Logstash Root Path
 * @param subs Path elements relative to {@code home}
 * @return Absolute Path a File under the Logstash Root.
 */
", ,/** * Builds the correct path for a file under the given Logstash root and defined by its sub path * elements relative to the Logstash root. * Ensures that the file exists and throws an exception of it's missing. * This is done to avoid hard to interpret errors thrown by JRuby that could result from missing * Ruby bootstrap scripts. * @param home Logstash Root Path * @param subs Path elements relative to {@code home} * @return Absolute Path a File under the Logstash Root. */,260,269,[0],0,[0],0,[0],0,0,0,0,"safePath(Path, String[])",org.logstash.Logstash,"safePath/2[java.nio.file.Path,java.lang.String[]]",False,260,0,3,3,0,3,5,10,1,1,2,5,0,0,1,0,0,0,1,0,2,0,1,0,0,0,51,10,0,True
89,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\RubyJavaIntegration.java,org.logstash.RubyJavaIntegration.RubyArrayOverride,"IRubyObject opEqq(ThreadContext, IRubyObject, IRubyObject)","/**
 * Enable class equivalence between Array and ArrayList so that ArrayList will work with
 * case o when Array.
 * @param context Ruby Context
 * @param rcvd Ruby class to compare against if not a Java ArrayList
 * @param obj Object to Compare Types with
 * @return True iff Ruby's `===` is fulfilled between {@code this} and {@code obj}
 */
@JRubyMethod(name = ""==="", meta = true)
public static IRubyObject opEqq(final ThreadContext context, final IRubyObject rcvd, final IRubyObject obj) {
    if (obj instanceof JavaProxy && Collection.class.isAssignableFrom(obj.getJavaClass())) {
        return context.tru;
    }
    return rcvd.op_eqq(context, obj);
}","/**
 * Enable class equivalence between Array and ArrayList so that ArrayList will work with
 * case o when Array.
 * @param context Ruby Context
 * @param rcvd Ruby class to compare against if not a Java ArrayList
 * @param obj Object to Compare Types with
 * @return True iff Ruby's `===` is fulfilled between {@code this} and {@code obj}
 */
", ,/** * Enable class equivalence between Array and ArrayList so that ArrayList will work with * case o when Array. * @param context Ruby Context * @param rcvd Ruby class to compare against if not a Java ArrayList * @param obj Object to Compare Types with * @return True iff Ruby's `===` is fulfilled between {@code this} and {@code obj} */,94,101,[0],0,[0],0,[0],0,0,0,0,"opEqq(ThreadContext, IRubyObject, IRubyObject)",org.logstash.RubyJavaIntegration$RubyArrayOverride,"opEqq/3[org.logstash.ThreadContext,org.logstash.IRubyObject,org.logstash.IRubyObject]",False,96,4,0,0,0,3,3,6,2,0,3,3,0,0,0,0,0,0,1,0,0,0,1,0,0,0,38,9,0,True
90,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\RubyJavaIntegration.java,org.logstash.RubyJavaIntegration.RubyHashOverride,"IRubyObject opEqq(ThreadContext, IRubyObject, IRubyObject)","/**
 * Enable class equivalence between Ruby's Hash and Java's Map.
 * @param context Ruby Context
 * @param rcvd Ruby class to compare against if not a Java Map
 * @param obj Object to Compare Types with
 * @return True iff Ruby's `===` is fulfilled between {@code this} and {@code obj}
 */
@JRubyMethod(name = ""==="", meta = true)
public static IRubyObject opEqq(final ThreadContext context, final IRubyObject rcvd, final IRubyObject obj) {
    if (obj instanceof JavaProxy && Map.class.isAssignableFrom(obj.getJavaClass())) {
        return context.tru;
    }
    return rcvd.op_eqq(context, obj);
}","/**
 * Enable class equivalence between Ruby's Hash and Java's Map.
 * @param context Ruby Context
 * @param rcvd Ruby class to compare against if not a Java Map
 * @param obj Object to Compare Types with
 * @return True iff Ruby's `===` is fulfilled between {@code this} and {@code obj}
 */
", ,/** * Enable class equivalence between Ruby's Hash and Java's Map. * @param context Ruby Context * @param rcvd Ruby class to compare against if not a Java Map * @param obj Object to Compare Types with * @return True iff Ruby's `===` is fulfilled between {@code this} and {@code obj} */,120,127,[0],0,[0],0,[0],0,0,0,0,"opEqq(ThreadContext, IRubyObject, IRubyObject)",org.logstash.RubyJavaIntegration$RubyHashOverride,"opEqq/3[org.logstash.ThreadContext,org.logstash.IRubyObject,org.logstash.IRubyObject]",False,122,4,0,0,0,3,3,6,2,0,3,3,0,0,0,0,0,0,1,0,0,0,1,0,0,0,32,9,0,True
91,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\RubyJavaIntegration.java,org.logstash.RubyJavaIntegration.JavaCollectionOverride,"IRubyObject and(ThreadContext, IRubyObject, IRubyObject)","/**
 * Support the Ruby intersection method on Java Collection.
 * @param context Thread context
 * @param self First Ruby collection object
 * @param other Second Ruby collection object
 * @return Ruby collection containing intersection of self and other
 */
@JRubyMethod(name = ""&"")
public static IRubyObject and(final ThreadContext context, final IRubyObject self, final IRubyObject other) {
    final Collection<?> dup = new LinkedHashSet<>(JavaUtil.unwrapIfJavaObject(self));
    dup.retainAll(JavaUtil.unwrapIfJavaObject(other));
    return JavaUtil.convertJavaToUsableRubyObject(context.runtime, dup);
}","/**
 * Support the Ruby intersection method on Java Collection.
 * @param context Thread context
 * @param self First Ruby collection object
 * @param other Second Ruby collection object
 * @return Ruby collection containing intersection of self and other
 */
", ,/** * Support the Ruby intersection method on Java Collection. * @param context Thread context * @param self First Ruby collection object * @param other Second Ruby collection object * @return Ruby collection containing intersection of self and other */,193,199,[0],0,[0],0,[0],0,0,0,0,"and(ThreadContext, IRubyObject, IRubyObject)",org.logstash.RubyJavaIntegration$JavaCollectionOverride,"and/3[org.logstash.ThreadContext,org.logstash.IRubyObject,org.logstash.IRubyObject]",False,195,3,0,0,0,1,3,5,1,1,3,3,0,0,0,0,0,0,1,0,1,0,0,0,0,0,23,9,0,True
92,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\RubyJavaIntegration.java,org.logstash.RubyJavaIntegration.JavaCollectionOverride,"IRubyObject or(ThreadContext, IRubyObject, IRubyObject)","/**
 * Support the Ruby union method on Java Collection.
 * @param context Thread context
 * @param self First Ruby collection object
 * @param other Second Ruby collection object
 * @return Ruby collection containing union of self and other
 */
@JRubyMethod(name = ""|"")
public static IRubyObject or(final ThreadContext context, final IRubyObject self, final IRubyObject other) {
    final Collection<?> dup = new LinkedHashSet<>(JavaUtil.unwrapIfJavaObject(self));
    dup.addAll(JavaUtil.unwrapIfJavaObject(other));
    return JavaUtil.convertJavaToUsableRubyObject(context.runtime, dup);
}","/**
 * Support the Ruby union method on Java Collection.
 * @param context Thread context
 * @param self First Ruby collection object
 * @param other Second Ruby collection object
 * @return Ruby collection containing union of self and other
 */
", ,/** * Support the Ruby union method on Java Collection. * @param context Thread context * @param self First Ruby collection object * @param other Second Ruby collection object * @return Ruby collection containing union of self and other */,208,214,[0],0,[0],0,[0],0,0,0,0,"or(ThreadContext, IRubyObject, IRubyObject)",org.logstash.RubyJavaIntegration$JavaCollectionOverride,"or/3[org.logstash.ThreadContext,org.logstash.IRubyObject,org.logstash.IRubyObject]",False,210,3,0,0,0,1,3,5,1,1,3,3,0,0,0,0,0,0,1,0,1,0,0,0,0,0,24,9,0,True
93,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\RubyJavaIntegration.java,org.logstash.RubyJavaIntegration.RubyMapProxyOverride,"IRubyObject containsKey(ThreadContext, IRubyObject, IRubyObject)","/**
 * This is a temporary fix to solve a bug in JRuby where classes implementing the Map interface, like LinkedHashMap
 * have a bug in the has_key? method that is implemented in the Enumerable module that is somehow mixed in the Map interface.
 * this bug makes has_key? (and all its aliases) return false for a key that has a nil value.
 * Only LinkedHashMap is patched here because patching the Map interface is not working.
 * TODO find proper fix, and submit upstream
 * relevant JRuby files:
 * https://github.com/jruby/jruby/blob/master/core/src/main/ruby/jruby/java/java_ext/java.util.rb
 * https://github.com/jruby/jruby/blob/master/core/src/main/java/org/jruby/java/proxies/MapJavaProxy.java
 *
 * @param context Thread context
 * @param self Ruby map object
 * @param key Key to find
 * @return RubyBoolean of true if the map contains the key
 */
@JRubyMethod(name = { ""has_key?"", ""include?"", ""member?"", ""key?"" })
public static IRubyObject containsKey(final ThreadContext context, final IRubyObject self, final IRubyObject key) {
    return JavaUtil.<Map<?, ?>>unwrapIfJavaObject(self).containsKey(key.toJava(Object.class)) ? context.tru : context.fals;
}","/**
 * This is a temporary fix to solve a bug in JRuby where classes implementing the Map interface, like LinkedHashMap
 * have a bug in the has_key? method that is implemented in the Enumerable module that is somehow mixed in the Map interface.
 * this bug makes has_key? (and all its aliases) return false for a key that has a nil value.
 * Only LinkedHashMap is patched here because patching the Map interface is not working.
 * TODO find proper fix, and submit upstream
 * relevant JRuby files:
 * https://github.com/jruby/jruby/blob/master/core/src/main/ruby/jruby/java/java_ext/java.util.rb
 * https://github.com/jruby/jruby/blob/master/core/src/main/java/org/jruby/java/proxies/MapJavaProxy.java
 *
 * @param context Thread context
 * @param self Ruby map object
 * @param key Key to find
 * @return RubyBoolean of true if the map contains the key
 */
", ,"/** * This is a temporary fix to solve a bug in JRuby where classes implementing the Map interface, like LinkedHashMap * have a bug in the has_key? method that is implemented in the Enumerable module that is somehow mixed in the Map interface. * this bug makes has_key? (and all its aliases) return false for a key that has a nil value. * Only LinkedHashMap is patched here because patching the Map interface is not working. * TODO find proper fix, and submit upstream * relevant JRuby files: * https://github.com/jruby/jruby/blob/master/core/src/main/ruby/jruby/java/java_ext/java.util.rb * https://github.com/jruby/jruby/blob/master/core/src/main/java/org/jruby/java/proxies/MapJavaProxy.java * * @param context Thread context * @param self Ruby map object * @param key Key to find * @return RubyBoolean of true if the map contains the key */",265,270,[1],1,[0],0,[1],1,1,1,1,"containsKey(ThreadContext, IRubyObject, IRubyObject)",org.logstash.RubyJavaIntegration$RubyMapProxyOverride,"containsKey/3[org.logstash.ThreadContext,org.logstash.IRubyObject,org.logstash.IRubyObject]",False,267,3,0,0,0,2,3,3,1,0,3,3,0,0,0,0,0,0,4,0,0,0,0,0,0,0,78,9,0,True
94,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\RubyUtil.java,org.logstash.RubyUtil,"RaiseException newRubyIOError(Ruby, Throwable)","/**
 * Wraps a Java exception in a JRuby IOError NativeException.
 * This preserves the Java stacktrace and bubble up as a Ruby IOError
 * @param runtime the Ruby runtime context
 * @param e the Java exception to wrap
 * @return RaiseException the wrapped IOError
 */
@SuppressWarnings(""deprecation"")
public static RaiseException newRubyIOError(Ruby runtime, Throwable e) {
    // will preserve Java stacktrace & bubble up as a Ruby IOError
    return new RaiseException(e, new org.jruby.NativeException(runtime, runtime.getIOError(), e));
}","/**
 * Wraps a Java exception in a JRuby IOError NativeException.
 * This preserves the Java stacktrace and bubble up as a Ruby IOError
 * @param runtime the Ruby runtime context
 * @param e the Java exception to wrap
 * @return RaiseException the wrapped IOError
 */
","// will preserve Java stacktrace & bubble up as a Ruby IOError
",/** * Wraps a Java exception in a JRuby IOError NativeException. * This preserves the Java stacktrace and bubble up as a Ruby IOError * @param runtime the Ruby runtime context * @param e the Java exception to wrap * @return RaiseException the wrapped IOError */[[SEP]]// will preserve Java stacktrace & bubble up as a Ruby IOError,604,608,[0],0,[0],0,"[0, 0]",0,0,0,0,"newRubyIOError(Ruby, Throwable)",org.logstash.RubyUtil,"newRubyIOError/2[org.logstash.Ruby,java.lang.Throwable]",False,605,3,4,4,0,1,1,3,1,0,2,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,27,9,0,True
95,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\RubyUtil.java,org.logstash.RubyUtil,"RubyClass setupLogstashClass(ObjectAllocator, Class<?>)","/**
 * Sets up a Java-defined {@link RubyClass} in the Logstash Ruby module.
 * @param allocator Allocator of the class
 * @param jclass Underlying Java class that is annotated by {@link JRubyClass}
 * @return RubyClass
 */
private static RubyClass setupLogstashClass(final ObjectAllocator allocator, final Class<?> jclass) {
    return setupLogstashClass(RUBY.getObject(), allocator, jclass);
}","/**
 * Sets up a Java-defined {@link RubyClass} in the Logstash Ruby module.
 * @param allocator Allocator of the class
 * @param jclass Underlying Java class that is annotated by {@link JRubyClass}
 * @return RubyClass
 */
", ,/** * Sets up a Java-defined {@link RubyClass} in the Logstash Ruby module. * @param allocator Allocator of the class * @param jclass Underlying Java class that is annotated by {@link JRubyClass} * @return RubyClass */,616,619,[0],0,[0],0,[0],0,0,0,0,"setupLogstashClass(ObjectAllocator, Class<?>)",org.logstash.RubyUtil,"setupLogstashClass/2[org.logstash.ObjectAllocator,java.lang.Class<?>]",False,617,3,2,1,1,1,2,3,1,0,2,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,22,10,0,True
96,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\RubyUtil.java,org.logstash.RubyUtil,"RubyClass setupLogstashClass(RubyClass, ObjectAllocator, Class<?>)","/**
 * Sets up a Java-defined {@link RubyClass} in the Logstash Ruby module.
 * @param parent Parent RubyClass
 * @param allocator Allocator of the class
 * @param jclass Underlying Java class that is annotated by {@link JRubyClass}
 * @return RubyClass
 */
private static RubyClass setupLogstashClass(final RubyClass parent, final ObjectAllocator allocator, final Class<?> jclass) {
    final RubyClass clazz = RUBY.defineClassUnder(jclass.getAnnotation(JRubyClass.class).name()[0], parent, allocator, LOGSTASH_MODULE);
    clazz.defineAnnotatedMethods(jclass);
    return clazz;
}","/**
 * Sets up a Java-defined {@link RubyClass} in the Logstash Ruby module.
 * @param parent Parent RubyClass
 * @param allocator Allocator of the class
 * @param jclass Underlying Java class that is annotated by {@link JRubyClass}
 * @return RubyClass
 */
", ,/** * Sets up a Java-defined {@link RubyClass} in the Logstash Ruby module. * @param parent Parent RubyClass * @param allocator Allocator of the class * @param jclass Underlying Java class that is annotated by {@link JRubyClass} * @return RubyClass */,628,635,[0],0,[0],0,[0],0,0,0,0,"setupLogstashClass(RubyClass, ObjectAllocator, Class<?>)",org.logstash.RubyUtil,"setupLogstashClass/3[org.logstash.RubyClass,org.logstash.ObjectAllocator,java.lang.Class<?>]",False,629,3,2,2,0,1,4,5,1,1,3,4,0,0,0,0,0,0,0,1,1,0,0,0,0,0,25,10,0,True
97,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\RubyUtil.java,org.logstash.RubyUtil,IRubyObject toRubyObject(Object),"/**
 * Convert a Java object to a Ruby representative.
 * @param javaObject the object to convert (might be null)
 * @return a Ruby wrapper
 */
public static IRubyObject toRubyObject(Object javaObject) {
    return JavaUtil.convertJavaToRuby(RUBY, javaObject);
}","/**
 * Convert a Java object to a Ruby representative.
 * @param javaObject the object to convert (might be null)
 * @return a Ruby wrapper
 */
", ,/** * Convert a Java object to a Ruby representative. * @param javaObject the object to convert (might be null) * @return a Ruby wrapper */,642,644,[0],0,[0],0,[0],0,0,0,0,toRubyObject(Object),org.logstash.RubyUtil,toRubyObject/1[java.lang.Object],False,642,1,4,4,0,1,1,3,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,9,0,True
98,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\RubyUtil.java,org.logstash.RubyUtil,T nilSafeCast(IRubyObject),"/**
 * Cast an IRubyObject that may be nil to a specific class
 * @param objectOrNil an object of either type {@code <T>} or nil.
 * @param <T> the type to cast non-nil values to
 * @return The given value, cast to {@code <T>}, or null.
 */
public static <T extends IRubyObject> T nilSafeCast(final IRubyObject objectOrNil) {
    if (objectOrNil == null || objectOrNil.isNil()) {
        return null;
    }
    @SuppressWarnings(""unchecked"")
    final T objectAsCasted = (T) objectOrNil;
    return objectAsCasted;
}","/**
 * Cast an IRubyObject that may be nil to a specific class
 * @param objectOrNil an object of either type {@code <T>} or nil.
 * @param <T> the type to cast non-nil values to
 * @return The given value, cast to {@code <T>}, or null.
 */
", ,"/** * Cast an IRubyObject that may be nil to a specific class * @param objectOrNil an object of either type {@code <T>} or nil. * @param <T> the type to cast non-nil values to * @return The given value, cast to {@code <T>}, or null. */",652,659,[0],0,[0],0,[0],0,0,0,0,nilSafeCast(IRubyObject),org.logstash.RubyUtil,nilSafeCast/1[org.logstash.IRubyObject],False,652,2,1,1,0,3,1,7,2,1,1,1,0,0,0,1,0,0,1,0,1,0,1,0,0,0,30,9,0,True
99,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Rubyfier.java,org.logstash.Rubyfier,"RubyHash deepMap(Ruby, Map<?, ?>)","private static RubyHash deepMap(final Ruby runtime, final Map<?, ?> map) {
    final RubyHash hash = RubyHash.newHash(runtime);
    // Note: RubyHash.put calls JavaUtil.convertJavaToUsableRubyObject on keys and values
    map.forEach((key, value) -> hash.put(key, deep(runtime, value)));
    return hash;
}", ,"// Note: RubyHash.put calls JavaUtil.convertJavaToUsableRubyObject on keys and values
",// Note: RubyHash.put calls JavaUtil.convertJavaToUsableRubyObject on keys and values,94,99,[0],0,[0],0,[0],0,0,0,0,"deepMap(Ruby, Map<?, ?>)",org.logstash.Rubyfier,"deepMap/2[org.logstash.Ruby,java.util.Map<?,?>]",False,94,3,2,1,1,1,4,5,1,3,2,4,1,2,0,0,0,0,0,0,1,0,0,0,0,1,7,10,0,False
100,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Rubyfier.java,org.logstash.Rubyfier,"IRubyObject fallbackConvert(Ruby, Object, Class<?>)","/**
 * Same principle as {@link Valuefier#fallbackConvert(Object, Class)}.
 */
private static IRubyObject fallbackConvert(final Ruby runtime, final Object o, final Class<?> cls) {
    for (final Map.Entry<Class<?>, Rubyfier.Converter> entry : CONVERTER_MAP.entrySet()) {
        if (entry.getKey().isAssignableFrom(cls)) {
            final Rubyfier.Converter found = entry.getValue();
            CONVERTER_MAP.put(cls, found);
            return found.convert(runtime, o);
        }
    }
    throw new MissingConverterException(cls);
}","/**
 * Same principle as {@link Valuefier#fallbackConvert(Object, Class)}.
 */
", ,"/** * Same principle as {@link Valuefier#fallbackConvert(Object, Class)}. */",142,152,[0],0,[0],0,[0],0,0,0,0,"fallbackConvert(Ruby, Object, Class<?>)",org.logstash.Rubyfier,"fallbackConvert/3[org.logstash.Ruby,java.lang.Object,java.lang.Class<?>]",False,143,4,3,1,2,3,6,10,1,1,3,6,0,0,1,0,0,0,0,0,1,0,2,0,0,0,14,10,0,True
101,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\StringInterpolation.java,org.logstash.StringInterpolation,"String evaluate(Event, String)","public static String evaluate(final Event event, final String template) throws JsonProcessingException {
    int open = template.indexOf(""%{"");
    int close = template.indexOf('}', open);
    if (open == -1 || close == -1) {
        return template;
    }
    final StringBuilder builder = STRING_BUILDER.get();
    int pos = 0;
    final int len = template.length();
    while (open > -1 && close > -1) {
        if (open > 0) {
            builder.append(template, pos, open);
        }
        if (template.regionMatches(open + 2, ""+%s"", 0, close - open - 2)) {
            // UNIX-style @timestamp formatter:
            // - `%{+%s}` -> 1234567890
            Timestamp t = event.getTimestamp();
            builder.append(t == null ? """" : t.toInstant().getEpochSecond());
        } else if (template.charAt(open + 2) == '{' && (close < len) && template.charAt(close + 1) == '}') {
            // JAVA-style @timestamp formatter:
            // - `%{{yyyy-MM-dd}}` -> `2021-08-11`
            // - `%{{YYYY-'W'ww}}` -> `2021-W32`
            // consume extra closing squiggle
            close = close + 1;
            final Timestamp t = event.getTimestamp();
            if (t != null) {
                final String javaTimeFormatPattern = template.substring(open + 3, close - 1);
                final java.time.format.DateTimeFormatter javaDateTimeFormatter = DateTimeFormatter.ofPattern(javaTimeFormatPattern).withZone(ZoneOffset.UTC);
                final String formattedTimestamp = javaDateTimeFormatter.format(t.toInstant());
                builder.append(formattedTimestamp);
            }
        } else if (template.charAt(open + 2) == '+') {
            // JODA-style @timestamp formatter:
            // - `%{+YYYY.MM.dd}` -> `2021-08-11`
            // - `%{+xxxx-'W'ww}  -> `2021-W32`
            final Timestamp t = event.getTimestamp();
            if (t != null) {
                final String jodaTimeFormatPattern = template.substring(open + 3, close);
                final org.joda.time.format.DateTimeFormatter jodaDateTimeFormatter = DateTimeFormat.forPattern(jodaTimeFormatPattern).withZone(DateTimeZone.UTC);
                final DateTime jodaTimestamp = new DateTime(t.toInstant().toEpochMilli(), DateTimeZone.UTC);
                final String formattedTimestamp = jodaTimestamp.toString(jodaDateTimeFormatter);
                builder.append(formattedTimestamp);
            }
        } else {
            final String found = template.substring(open + 2, close);
            final Object value = event.getField(found);
            if (value != null) {
                if (value instanceof List) {
                    builder.append(KeyNode.join((List) value, "",""));
                } else if (value instanceof Map) {
                    builder.append(ObjectMappers.JSON_MAPPER.writeValueAsString(value));
                } else {
                    builder.append(value.toString());
                }
            } else {
                builder.append(""%{"").append(found).append('}');
            }
        }
        pos = close + 1;
        open = template.indexOf(""%{"", pos);
        close = template.indexOf('}', open);
    }
    if (pos < len) {
        builder.append(template, pos, len);
    }
    return builder.toString();
}", ,"// UNIX-style @timestamp formatter:
[[SEP]]// - `%{+%s}` -> 1234567890
[[SEP]]// JAVA-style @timestamp formatter:
[[SEP]]// - `%{{yyyy-MM-dd}}` -> `2021-08-11`
[[SEP]]// - `%{{YYYY-'W'ww}}` -> `2021-W32`
[[SEP]]// consume extra closing squiggle
[[SEP]]// JODA-style @timestamp formatter:
[[SEP]]// - `%{+YYYY.MM.dd}` -> `2021-08-11`
[[SEP]]// - `%{+xxxx-'W'ww}  -> `2021-W32`
",// UNIX-style @timestamp formatter:// - `%{+%s}` -> 1234567890[[SEP]]// JAVA-style @timestamp formatter:// - `%{{yyyy-MM-dd}}` -> `2021-08-11`// - `%{{YYYY-'W'ww}}` -> `2021-W32`// consume extra closing squiggle[[SEP]]// JODA-style @timestamp formatter:// - `%{+YYYY.MM.dd}` -> `2021-08-11`// - `%{+xxxx-'W'ww}  -> `2021-W32`,64,129,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,"evaluate(Event, String)",org.logstash.StringInterpolation,"evaluate/2[org.logstash.Event,java.lang.String]",False,64,5,21,17,4,18,27,64,2,17,2,27,0,0,1,9,0,1,6,18,21,11,4,0,0,0,29,9,0,False
102,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Timestamp.java,org.logstash.Timestamp,org.joda.time.DateTime getTime(),"/**
 * @deprecated This method returns JodaTime which is deprecated in favor of JDK Instant.
 *   * <p> Use {@link Timestamp#toInstant()} instead. </p>
 */
@Deprecated
public org.joda.time.DateTime getTime() {
    if (time == null) {
        time = new org.joda.time.DateTime(instant.toEpochMilli(), org.joda.time.DateTimeZone.UTC);
    }
    return time;
}","/**
 * @deprecated This method returns JodaTime which is deprecated in favor of JDK Instant.
 *   * <p> Use {@link Timestamp#toInstant()} instead. </p>
 */
", ,/** * @deprecated This method returns JodaTime which is deprecated in favor of JDK Instant. *   * <p> Use {@link Timestamp#toInstant()} instead. </p> */,99,105,[1],1,[0],0,[1],1,0,0,0,getTime(),org.logstash.Timestamp,getTime/0,False,100,2,2,2,0,2,1,6,1,0,0,1,0,0,0,1,0,0,0,0,1,0,1,0,0,0,19,1,0,True
103,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Timestamp.java,org.logstash.Timestamp,String toString(),"public String toString() {
    // ensure minimum precision of 3 decimal places by using our own 3-decimal-place formatter when we have no nanos.
    final DateTimeFormatter formatter = (instant.getNano() == 0 ? ISO_INSTANT_MILLIS : DateTimeFormatter.ISO_INSTANT);
    return formatter.format(instant);
}", ,"// ensure minimum precision of 3 decimal places by using our own 3-decimal-place formatter when we have no nanos.
",// ensure minimum precision of 3 decimal places by using our own 3-decimal-place formatter when we have no nanos.,115,119,[0],0,[0],0,[0],0,0,0,0,toString(),org.logstash.Timestamp,toString/0,False,115,0,34,34,0,2,2,4,1,1,0,2,0,0,0,1,0,1,0,1,1,0,0,0,0,0,16,1,0,False
104,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Timestamp.java,org.logstash.Timestamp,long usec(),"/**
 * @return the fraction of a second as microseconds from 0 to 999,999; not the number of microseconds since epoch
 */
public long usec() {
    return instant.getNano() / 1000;
}","/**
 * @return the fraction of a second as microseconds from 0 to 999,999; not the number of microseconds since epoch
 */
", ,"/** * @return the fraction of a second as microseconds from 0 to 999,999; not the number of microseconds since epoch */",128,130,[0],0,[0],0,[0],0,0,0,0,usec(),org.logstash.Timestamp,usec/0,False,128,0,2,2,0,1,1,3,1,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,14,1,0,True
105,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Timestamp.java,org.logstash.Timestamp,long nsec(),"/**
 * @return the fraction of a second as nanoseconds from 0 to 999,999,999; not the number of nanoseconds since epoch
 */
public long nsec() {
    return instant.getNano();
}","/**
 * @return the fraction of a second as nanoseconds from 0 to 999,999,999; not the number of nanoseconds since epoch
 */
", ,"/** * @return the fraction of a second as nanoseconds from 0 to 999,999,999; not the number of nanoseconds since epoch */",135,137,[0],0,[0],0,[0],0,0,0,0,nsec(),org.logstash.Timestamp,nsec/0,False,135,0,2,2,0,1,1,3,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,1,0,True
106,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Timestamp.java,org.logstash.Timestamp,"Optional<Instant> tryFallbackParse(String, DateTimeFormatter, UnaryOperator<DateTimeFormatter>)","/**
 * Attempts to parse the input if-and-only-if the provided {@code formatterTransformer}
 * effectively transforms the provided {@code baseFormatter}. This is intended to be a
 * fallback method for a maybe-modified formatter to prevent re-parsing the same input
 * with the same formatter multiple times.
 *
 * @param iso8601 an ISO8601-ish string
 * @param baseFormatter the base formatter
 * @param formatterTransformer a transformation operator (such as using DateTimeFormat#withDecimalStyle)
 * @return an {@code Optional}, which contains a value if-and-only-if the effective format is different
 *         from the base format and successfully parsed the input
 */
private static Optional<Instant> tryFallbackParse(final String iso8601, final DateTimeFormatter baseFormatter, final UnaryOperator<DateTimeFormatter> formatterTransformer) {
    final DateTimeFormatter modifiedFormatter = formatterTransformer.apply(baseFormatter);
    if (modifiedFormatter.equals(baseFormatter)) {
        return Optional.empty();
    }
    return tryParse(iso8601, modifiedFormatter);
}","/**
 * Attempts to parse the input if-and-only-if the provided {@code formatterTransformer}
 * effectively transforms the provided {@code baseFormatter}. This is intended to be a
 * fallback method for a maybe-modified formatter to prevent re-parsing the same input
 * with the same formatter multiple times.
 *
 * @param iso8601 an ISO8601-ish string
 * @param baseFormatter the base formatter
 * @param formatterTransformer a transformation operator (such as using DateTimeFormat#withDecimalStyle)
 * @return an {@code Optional}, which contains a value if-and-only-if the effective format is different
 *         from the base format and successfully parsed the input
 */
", ,"/** * Attempts to parse the input if-and-only-if the provided {@code formatterTransformer} * effectively transforms the provided {@code baseFormatter}. This is intended to be a * fallback method for a maybe-modified formatter to prevent re-parsing the same input * with the same formatter multiple times. * * @param iso8601 an ISO8601-ish string * @param baseFormatter the base formatter * @param formatterTransformer a transformation operator (such as using DateTimeFormat#withDecimalStyle) * @return an {@code Optional}, which contains a value if-and-only-if the effective format is different *         from the base format and successfully parsed the input */",210,217,[0],0,[0],0,[0],0,0,0,0,"tryFallbackParse(String, DateTimeFormatter, UnaryOperator<DateTimeFormatter>)",org.logstash.Timestamp,"tryFallbackParse/3[java.lang.String,java.time.format.DateTimeFormatter,java.util.function.UnaryOperator<java.time.format.DateTimeFormatter>]",False,212,1,2,1,1,2,4,7,2,1,3,4,1,1,0,0,0,0,0,0,1,0,1,0,0,0,56,10,0,True
107,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Util.java,org.logstash.Util,"void mergeLists(List<Object>, List<Object>, LinkedHashSet<Object>)","/**
 * Merges elements in the source list into the target list, adding only those in the source
 * list that are not yet contained in the target list while keeping the target list ordered
 * according to last added.
 * @param target Target List
 * @param source Source List
 * @param buffer {@link LinkedHashSet} used as sort buffer
 */
private static void mergeLists(final List<Object> target, final List<Object> source, final LinkedHashSet<Object> buffer) {
    buffer.addAll(target);
    buffer.addAll(source);
    target.clear();
    target.addAll(buffer);
}","/**
 * Merges elements in the source list into the target list, adding only those in the source
 * list that are not yet contained in the target list while keeping the target list ordered
 * according to last added.
 * @param target Target List
 * @param source Source List
 * @param buffer {@link LinkedHashSet} used as sort buffer
 */
", ,"/** * Merges elements in the source list into the target list, adding only those in the source * list that are not yet contained in the target list while keeping the target list ordered * according to last added. * @param target Target List * @param source Source List * @param buffer {@link LinkedHashSet} used as sort buffer */",89,95,[0],0,[0],0,[0],0,0,0,0,"mergeLists(List<Object>, List<Object>, LinkedHashSet<Object>)",org.logstash.Util,"mergeLists/3[java.util.List<java.lang.Object>,java.util.List<java.lang.Object>,java.util.LinkedHashSet<java.lang.Object>]",False,90,0,1,1,0,1,3,6,0,0,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,33,10,0,True
108,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\Valuefier.java,org.logstash.Valuefier,"Object fallbackConvert(Object, Class<?>)","/**
 * Fallback for types not covered by {@link Valuefier#convert(Object)} as a result of no
 * {@link Valuefier.Converter} having been cached for the given class. Uses the fact that
 * the only subclasses of the keys in {@link Valuefier#CONVERTER_MAP} as set up by
 * {@link Valuefier#initConverters()} can be converted here and hence find the appropriate
 * super class for unknown types by checking each entry in {@link Valuefier#CONVERTER_MAP} for
 * being a supertype of the given class.
 * @param o Object to convert
 * @param cls Class of given object {@code o}
 * @return Conversion result equivalent to what {@link Valuefier#convert(Object)} would return
 */
private static Object fallbackConvert(final Object o, final Class<?> cls) {
    for (final Map.Entry<Class<?>, Valuefier.Converter> entry : CONVERTER_MAP.entrySet()) {
        if (entry.getKey().isAssignableFrom(cls)) {
            final Valuefier.Converter found = entry.getValue();
            CONVERTER_MAP.put(cls, found);
            return found.convert(o);
        }
    }
    throw new MissingConverterException(cls);
}","/**
 * Fallback for types not covered by {@link Valuefier#convert(Object)} as a result of no
 * {@link Valuefier.Converter} having been cached for the given class. Uses the fact that
 * the only subclasses of the keys in {@link Valuefier#CONVERTER_MAP} as set up by
 * {@link Valuefier#initConverters()} can be converted here and hence find the appropriate
 * super class for unknown types by checking each entry in {@link Valuefier#CONVERTER_MAP} for
 * being a supertype of the given class.
 * @param o Object to convert
 * @param cls Class of given object {@code o}
 * @return Conversion result equivalent to what {@link Valuefier#convert(Object)} would return
 */
", ,/** * Fallback for types not covered by {@link Valuefier#convert(Object)} as a result of no * {@link Valuefier.Converter} having been cached for the given class. Uses the fact that * the only subclasses of the keys in {@link Valuefier#CONVERTER_MAP} as set up by * {@link Valuefier#initConverters()} can be converted here and hence find the appropriate * super class for unknown types by checking each entry in {@link Valuefier#CONVERTER_MAP} for * being a supertype of the given class. * @param o Object to convert * @param cls Class of given object {@code o} * @return Conversion result equivalent to what {@link Valuefier#convert(Object)} would return */,114,123,[0],0,[0],0,[0],0,0,0,0,"fallbackConvert(Object, Class<?>)",org.logstash.Valuefier,"fallbackConvert/2[java.lang.Object,java.lang.Class<?>]",False,114,2,3,1,2,3,6,10,1,1,2,6,0,0,1,0,0,0,0,0,1,0,2,0,0,0,55,10,0,True
109,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\AckedReadBatch.java,org.logstash.ackedqueue.AckedReadBatch,Collection<RubyEvent> events(),"@Override
public Collection<RubyEvent> events() {
    // This does not filter cancelled events because it is
    // only used in the WorkerLoop where there are no cancelled
    // events yet.
    return events;
}", ,"// This does not filter cancelled events because it is
[[SEP]]// only used in the WorkerLoop where there are no cancelled
[[SEP]]// events yet.
",// This does not filter cancelled events because it is// only used in the WorkerLoop where there are no cancelled// events yet.,81,87,[0],0,"[0, 0, 0]",0,[0],0,0,0,0,events(),org.logstash.ackedqueue.AckedReadBatch,events/0,False,82,1,0,0,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,False
110,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Batch.java,org.logstash.ackedqueue.Batch,void close(),"// close acks the batch ackable events
@Override
public void close() throws IOException {
    if (closed.getAndSet(true) == false) {
        if (firstSeqNum >= 0L) {
            this.queue.ack(firstSeqNum, elements.size());
        }
    } else {
        // TODO: how should we handle double-closing?
        throw new IOException(""double closing batch"");
    }
}","// close acks the batch ackable events
","// TODO: how should we handle double-closing?
",// close acks the batch ackable events[[SEP]]// TODO: how should we handle double-closing?,56,66,[0],0,[1],1,"[0, 1]",1,1,1,1,close(),org.logstash.ackedqueue.Batch,close/0,False,57,1,16,15,1,3,3,10,0,0,0,3,0,0,0,1,0,0,1,1,0,0,2,0,0,0,12,1,0,False
111,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Batch.java,org.logstash.ackedqueue.Batch,"List<Queueable> deserializeElements(List<byte[]>, Queue)","/**
 * @param serialized Collection of serialized elements
 * @param q {@link Queue} instance
 * @return Collection of deserialized {@link Queueable} elements
 */
private static List<Queueable> deserializeElements(List<byte[]> serialized, Queue q) {
    final List<Queueable> deserialized = new ArrayList<>(serialized.size());
    for (final byte[] element : serialized) {
        deserialized.add(q.deserialize(element));
    }
    return deserialized;
}","/**
 * @param serialized Collection of serialized elements
 * @param q {@link Queue} instance
 * @return Collection of deserialized {@link Queueable} elements
 */
", ,/** * @param serialized Collection of serialized elements * @param q {@link Queue} instance * @return Collection of deserialized {@link Queueable} elements */,86,92,[0],0,[0],0,[0],0,0,0,0,"deserializeElements(List<byte[]>, Queue)",org.logstash.ackedqueue.Batch,"deserializeElements/2[java.util.List<byte[]>,org.logstash.ackedqueue.Queue]",False,86,3,2,1,1,2,3,7,1,1,2,3,0,0,1,0,0,0,0,0,1,0,1,0,0,0,12,10,0,True
112,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Checkpoint.java,org.logstash.ackedqueue.Checkpoint,boolean isFullyAcked(),"// @return true if this checkpoint indicates a fulle acked page
public boolean isFullyAcked() {
    return this.elementCount > 0 && this.firstUnackedSeqNum >= this.minSeqNum + this.elementCount;
}","// @return true if this checkpoint indicates a fulle acked page
", ,// @return true if this checkpoint indicates a fulle acked page,69,71,[0],0,[0],0,[0],0,0,0,0,isFullyAcked(),org.logstash.ackedqueue.Checkpoint,isFullyAcked/0,False,69,0,4,4,0,3,0,3,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,3,1,0,False
113,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Checkpoint.java,org.logstash.ackedqueue.Checkpoint,long maxSeqNum(),"// @return the highest seqNum in this page or -1 for an initial checkpoint
public long maxSeqNum() {
    return this.minSeqNum + this.elementCount - 1;
}","// @return the highest seqNum in this page or -1 for an initial checkpoint
", ,// @return the highest seqNum in this page or -1 for an initial checkpoint,74,76,[0],0,[0],0,[0],0,0,0,0,maxSeqNum(),org.logstash.ackedqueue.Checkpoint,maxSeqNum/0,False,74,0,3,3,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,1,0,2,0,0,0,0,3,1,0,False
114,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,SequencedList<byte[]> read(int),"/**
 * @param limit the maximum number of elements to read, actual number readcan be smaller
 * @return {@link SequencedList} collection of serialized elements read
 * @throws IOException if an IO error occurs
 */
public SequencedList<byte[]> read(int limit) throws IOException {
    // first make sure this page is activated, activating previously activated is harmless
    this.pageIO.activate();
    SequencedList<byte[]> serialized = this.pageIO.read(this.firstUnreadSeqNum, limit);
    assert serialized.getSeqNums().get(0) == this.firstUnreadSeqNum : String.format(""firstUnreadSeqNum=%d != first result seqNum=%d"", this.firstUnreadSeqNum, serialized.getSeqNums().get(0));
    this.firstUnreadSeqNum += serialized.getElements().size();
    return serialized;
}","/**
 * @param limit the maximum number of elements to read, actual number readcan be smaller
 * @return {@link SequencedList} collection of serialized elements read
 * @throws IOException if an IO error occurs
 */
","// first make sure this page is activated, activating previously activated is harmless
","/** * @param limit the maximum number of elements to read, actual number readcan be smaller * @return {@link SequencedList} collection of serialized elements read * @throws IOException if an IO error occurs */[[SEP]]// first make sure this page is activated, activating previously activated is harmless",74,85,[0],0,[0],0,"[0, 0]",0,0,0,0,read(int),org.logstash.ackedqueue.Page,read/1[int],False,74,4,9,4,5,2,7,7,1,1,1,7,0,0,0,1,0,0,1,2,2,0,0,0,0,0,27,1,0,True
115,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,"void write(byte[], long, int)","public void write(byte[] bytes, long seqNum, int checkpointMaxWrites) throws IOException {
    if (!this.writable) {
        throw new IllegalStateException(String.format(""page=%d is not writable"", this.pageNum));
    }
    this.pageIO.write(bytes, seqNum);
    if (this.minSeqNum <= 0) {
        this.minSeqNum = seqNum;
        this.firstUnreadSeqNum = seqNum;
    }
    this.elementCount++;
    // force a checkpoint if we wrote checkpointMaxWrites elements since last checkpoint
    // the initial condition of an ""empty"" checkpoint, maxSeqNum() will return -1
    if (checkpointMaxWrites > 0 && (seqNum >= this.lastCheckpoint.maxSeqNum() + checkpointMaxWrites)) {
        // did we write more than checkpointMaxWrites elements? if so checkpoint now
        checkpoint();
    }
}", ,"// force a checkpoint if we wrote checkpointMaxWrites elements since last checkpoint
[[SEP]]// the initial condition of an ""empty"" checkpoint, maxSeqNum() will return -1
[[SEP]]// did we write more than checkpointMaxWrites elements? if so checkpoint now
","// force a checkpoint if we wrote checkpointMaxWrites elements since last checkpoint// the initial condition of an ""empty"" checkpoint, maxSeqNum() will return -1[[SEP]]// did we write more than checkpointMaxWrites elements? if so checkpoint now",87,106,[0],0,"[0, 0, 0]",0,"[0, 0]",0,0,0,0,"write(byte[], long, int)",org.logstash.ackedqueue.Page,"write/3[byte[],long,int]",False,87,4,8,5,3,5,4,14,0,0,3,4,1,5,0,0,0,1,1,2,2,1,1,0,0,0,13,1,0,False
116,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,boolean isEmpty(),"/**
 * Page is considered empty if it does not contain any element or if all elements are acked.
 *
 * TODO: note that this should be the same as isFullyAcked once fixed per https://github.com/elastic/logstash/issues/7570
 *
 * @return true if the page has no element or if all elements are acked.
 */
public boolean isEmpty() {
    return this.elementCount == 0 || isFullyAcked();
}","/**
 * Page is considered empty if it does not contain any element or if all elements are acked.
 *
 * TODO: note that this should be the same as isFullyAcked once fixed per https://github.com/elastic/logstash/issues/7570
 *
 * @return true if the page has no element or if all elements are acked.
 */
", ,/** * Page is considered empty if it does not contain any element or if all elements are acked. * * TODO: note that this should be the same as isFullyAcked once fixed per https://github.com/elastic/logstash/issues/7570 * * @return true if the page has no element or if all elements are acked. */,115,117,[1],1,[0],0,[1],1,1,1,1,isEmpty(),org.logstash.ackedqueue.Page,isEmpty/0,False,115,1,3,2,1,2,1,3,1,0,0,1,1,1,0,1,0,0,0,1,0,0,0,0,0,0,34,1,0,True
117,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,boolean isFullyRead(),"public boolean isFullyRead() {
    return unreadCount() <= 0;
    // return this.elementCount <= 0 || this.firstUnreadSeqNum > maxSeqNum();
}", ,"// return this.elementCount <= 0 || this.firstUnreadSeqNum > maxSeqNum();
",// return this.elementCount <= 0 || this.firstUnreadSeqNum > maxSeqNum();,119,122,[0],0,[0],0,[0],0,0,0,0,isFullyRead(),org.logstash.ackedqueue.Page,isFullyRead/0,False,119,1,10,9,1,2,1,3,1,0,0,1,1,2,0,0,0,0,0,1,0,0,0,0,0,0,5,1,0,False
118,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,"boolean ack(long, int, int)","/**
 * update the page acking bitset. trigger checkpoint on the page if it is fully acked or if we acked more than the
 * configured threshold checkpointMaxAcks.
 * note that if the fully acked tail page is the first unacked page, it is not really necessary to also checkpoint
 * the head page to update firstUnackedPageNum because it will be updated in the next upcoming head page checkpoint
 * and in a crash condition, the Queue open recovery will detect and purge fully acked pages
 *
 * @param firstSeqNum Lowest sequence number to ack
 * @param count Number of elements to ack
 * @param checkpointMaxAcks number of acks before forcing a checkpoint
 * @return true if Page and its checkpoint were purged as a result of being fully acked
 * @throws IOException if an IO error occurs
 */
public boolean ack(long firstSeqNum, int count, int checkpointMaxAcks) throws IOException {
    assert firstSeqNum >= this.minSeqNum : String.format(""seqNum=%d is smaller than minSeqnum=%d"", firstSeqNum, this.minSeqNum);
    final long maxSeqNum = firstSeqNum + count;
    assert maxSeqNum <= this.minSeqNum + this.elementCount : String.format(""seqNum=%d is greater than minSeqnum=%d + elementCount=%d = %d"", maxSeqNum, this.minSeqNum, this.elementCount, this.minSeqNum + this.elementCount);
    final int offset = Ints.checkedCast(firstSeqNum - this.minSeqNum);
    ackedSeqNums.flip(offset, offset + count);
    // checkpoint if totally acked or we acked more than checkpointMaxAcks elements in this page since last checkpoint
    // note that fully acked pages cleanup is done at queue level in Queue.ack()
    final long firstUnackedSeqNum = firstUnackedSeqNum();
    final boolean done = isFullyAcked();
    if (done) {
        checkpoint();
        // purge fully acked tail page
        if (!this.writable) {
            purge();
            final CheckpointIO cpIO = queue.getCheckpointIO();
            cpIO.purge(cpIO.tailFileName(pageNum));
        }
        assert firstUnackedSeqNum >= this.minSeqNum + this.elementCount - 1 : String.format(""invalid firstUnackedSeqNum=%d for minSeqNum=%d and elementCount=%d and cardinality=%d"", firstUnackedSeqNum, this.minSeqNum, this.elementCount, this.ackedSeqNums.cardinality());
    } else if (checkpointMaxAcks > 0 && firstUnackedSeqNum >= this.lastCheckpoint.getFirstUnackedSeqNum() + checkpointMaxAcks) {
        // did we acked more than checkpointMaxAcks elements? if so checkpoint now
        checkpoint();
    }
    return done;
}","/**
 * update the page acking bitset. trigger checkpoint on the page if it is fully acked or if we acked more than the
 * configured threshold checkpointMaxAcks.
 * note that if the fully acked tail page is the first unacked page, it is not really necessary to also checkpoint
 * the head page to update firstUnackedPageNum because it will be updated in the next upcoming head page checkpoint
 * and in a crash condition, the Queue open recovery will detect and purge fully acked pages
 *
 * @param firstSeqNum Lowest sequence number to ack
 * @param count Number of elements to ack
 * @param checkpointMaxAcks number of acks before forcing a checkpoint
 * @return true if Page and its checkpoint were purged as a result of being fully acked
 * @throws IOException if an IO error occurs
 */
","// checkpoint if totally acked or we acked more than checkpointMaxAcks elements in this page since last checkpoint
[[SEP]]// note that fully acked pages cleanup is done at queue level in Queue.ack()
[[SEP]]// purge fully acked tail page
[[SEP]]// did we acked more than checkpointMaxAcks elements? if so checkpoint now
","/** * update the page acking bitset. trigger checkpoint on the page if it is fully acked or if we acked more than the * configured threshold checkpointMaxAcks. * note that if the fully acked tail page is the first unacked page, it is not really necessary to also checkpoint * the head page to update firstUnackedPageNum because it will be updated in the next upcoming head page checkpoint * and in a crash condition, the Queue open recovery will detect and purge fully acked pages * * @param firstSeqNum Lowest sequence number to ack * @param count Number of elements to ack * @param checkpointMaxAcks number of acks before forcing a checkpoint * @return true if Page and its checkpoint were purged as a result of being fully acked * @throws IOException if an IO error occurs */[[SEP]]// checkpoint if totally acked or we acked more than checkpointMaxAcks elements in this page since last checkpoint// note that fully acked pages cleanup is done at queue level in Queue.ack()[[SEP]]// purge fully acked tail page[[SEP]]// did we acked more than checkpointMaxAcks elements? if so checkpoint now",147,180,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,"ack(long, int, int)",org.logstash.ackedqueue.Page,"ack/3[long,int,int]",False,147,4,10,2,8,7,12,22,1,5,3,12,4,5,0,0,0,0,3,2,5,8,2,0,0,0,89,1,0,True
119,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,void headPageCheckpoint(),"private void headPageCheckpoint() throws IOException {
    if (this.elementCount > this.lastCheckpoint.getElementCount()) {
        // fsync & checkpoint if data written since last checkpoint
        this.pageIO.ensurePersisted();
        this.forceCheckpoint();
    } else {
        Checkpoint checkpoint = new Checkpoint(this.pageNum, this.queue.firstUnackedPageNum(), this.firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
        if (!checkpoint.equals(this.lastCheckpoint)) {
            // checkpoint only if it changed since last checkpoint
            // non-dry code with forceCheckpoint() to avoid unnecessary extra new Checkpoint object creation
            CheckpointIO io = this.queue.getCheckpointIO();
            io.write(io.headFileName(), checkpoint);
            this.lastCheckpoint = checkpoint;
        }
    }
}", ,"// fsync & checkpoint if data written since last checkpoint
[[SEP]]// checkpoint only if it changed since last checkpoint
[[SEP]]// non-dry code with forceCheckpoint() to avoid unnecessary extra new Checkpoint object creation
",// fsync & checkpoint if data written since last checkpoint[[SEP]]// checkpoint only if it changed since last checkpoint// non-dry code with forceCheckpoint() to avoid unnecessary extra new Checkpoint object creation,190,208,[0],0,"[0, 0, 0]",0,"[0, 0]",0,0,0,0,headPageCheckpoint(),org.logstash.ackedqueue.Page,headPageCheckpoint/0,False,190,5,12,2,10,3,9,14,0,2,0,9,2,2,0,0,0,0,0,0,3,0,2,0,0,0,8,2,0,False
120,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,void tailPageCheckpoint(),"public void tailPageCheckpoint() throws IOException {
    // since this is a tail page and no write can happen in this page, there is no point in performing a fsync on this page, just stamp checkpoint
    CheckpointIO io = this.queue.getCheckpointIO();
    this.lastCheckpoint = io.write(io.tailFileName(this.pageNum), this.pageNum, 0, this.firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
}", ,"// since this is a tail page and no write can happen in this page, there is no point in performing a fsync on this page, just stamp checkpoint
","// since this is a tail page and no write can happen in this page, there is no point in performing a fsync on this page, just stamp checkpoint",210,214,[0],0,[0],0,[0],0,0,0,0,tailPageCheckpoint(),org.logstash.ackedqueue.Page,tailPageCheckpoint/0,False,210,3,6,2,4,1,4,4,0,1,0,4,1,1,0,0,0,0,0,1,2,0,0,0,0,0,7,1,0,False
121,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,void ensurePersistedUpto(long),"public void ensurePersistedUpto(long seqNum) throws IOException {
    long lastCheckpointUptoSeqNum = this.lastCheckpoint.getMinSeqNum() + this.lastCheckpoint.getElementCount();
    // if the last checkpoint for this headpage already included the given seqNum, no need to fsync/checkpoint
    if (seqNum > lastCheckpointUptoSeqNum) {
        // head page checkpoint does a data file fsync
        checkpoint();
    }
}", ,"// if the last checkpoint for this headpage already included the given seqNum, no need to fsync/checkpoint
[[SEP]]// head page checkpoint does a data file fsync
","// if the last checkpoint for this headpage already included the given seqNum, no need to fsync/checkpoint[[SEP]]// head page checkpoint does a data file fsync",217,225,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,ensurePersistedUpto(long),org.logstash.ackedqueue.Page,ensurePersistedUpto/1[long],False,217,2,4,1,3,2,3,6,0,1,1,3,1,5,0,0,0,0,0,0,1,1,1,0,0,0,12,1,0,False
122,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,void behead(),"public void behead() throws IOException {
    assert this.writable == true : ""cannot behead a tail page"";
    headPageCheckpoint();
    this.writable = false;
    this.lastCheckpoint = new Checkpoint(0, 0, 0, 0, 0);
    // first thing that must be done after beheading is to create a new checkpoint for that new tail page
    // tail page checkpoint does NOT includes a fsync
    tailPageCheckpoint();
}", ,"// first thing that must be done after beheading is to create a new checkpoint for that new tail page
[[SEP]]// tail page checkpoint does NOT includes a fsync
",// first thing that must be done after beheading is to create a new checkpoint for that new tail page// tail page checkpoint does NOT includes a fsync,234,245,[0],0,"[0, 0]",0,[0],0,0,0,0,behead(),org.logstash.ackedqueue.Page,behead/0,False,234,2,5,2,3,2,2,7,0,0,0,2,2,4,0,1,0,0,1,5,2,0,0,0,0,0,12,1,0,False
123,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,void deactivate(),"/**
 * signal that this page is not active and resources can be released
 * @throws IOException if an IO error occurs
 */
public void deactivate() throws IOException {
    this.getPageIO().deactivate();
}","/**
 * signal that this page is not active and resources can be released
 * @throws IOException if an IO error occurs
 */
", ,/** * signal that this page is not active and resources can be released * @throws IOException if an IO error occurs */,251,253,[0],0,[0],0,[0],0,0,0,0,deactivate(),org.logstash.ackedqueue.Page,deactivate/0,False,251,2,3,1,2,1,2,3,0,0,0,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,18,1,0,True
124,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,boolean hasCapacity(int),"/**
 * verify if data size plus overhead is not greater than the page capacity
 *
 * @param byteSize the date size to verify
 * @return true if data plus overhead fit in page
 */
public boolean hasCapacity(int byteSize) {
    return this.pageIO.persistedByteCount(byteSize) <= this.pageIO.getCapacity();
}","/**
 * verify if data size plus overhead is not greater than the page capacity
 *
 * @param byteSize the date size to verify
 * @return true if data plus overhead fit in page
 */
", ,/** * verify if data size plus overhead is not greater than the page capacity * * @param byteSize the date size to verify * @return true if data plus overhead fit in page */,265,267,[0],0,[0],0,[0],0,0,0,0,hasCapacity(int),org.logstash.ackedqueue.Page,hasCapacity/1[int],False,265,1,3,1,2,2,2,3,1,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,21,1,0,True
125,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,void purge(),"public void purge() throws IOException {
    // page IO purge calls close
    this.pageIO.purge();
}", ,"// page IO purge calls close
",// page IO purge calls close,274,276,[0],0,[0],0,[0],0,0,0,0,purge(),org.logstash.ackedqueue.Page,purge/0,False,274,1,3,2,1,1,1,3,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,1,0,False
126,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Page.java,org.logstash.ackedqueue.Page,long firstUnackedSeqNum(),"protected long firstUnackedSeqNum() {
    // TODO: eventually refactor to use new bithandling class
    return this.ackedSeqNums.nextClearBit(0) + this.minSeqNum;
}", ,"// TODO: eventually refactor to use new bithandling class
",// TODO: eventually refactor to use new bithandling class,298,301,[0],0,[1],1,[1],1,1,1,1,firstUnackedSeqNum(),org.logstash.ackedqueue.Page,firstUnackedSeqNum/0,False,298,0,4,4,0,1,1,3,1,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,4,4,0,False
127,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\PageFactory.java,org.logstash.ackedqueue.PageFactory,"Page newHeadPage(int, Queue, PageIO)","/**
 * create a new head page object and new page.{@literal {pageNum}} empty valid data file
 *
 * @param pageNum the new head page page number
 * @param queue the {@link Queue} instance
 * @param pageIO the {@link PageIO} delegate
 * @return {@link Page} the new head page
 */
public static Page newHeadPage(int pageNum, Queue queue, PageIO pageIO) {
    return new Page(pageNum, queue, 0, 0, 0, new BitSet(), pageIO, true);
}","/**
 * create a new head page object and new page.{@literal {pageNum}} empty valid data file
 *
 * @param pageNum the new head page page number
 * @param queue the {@link Queue} instance
 * @param pageIO the {@link PageIO} delegate
 * @return {@link Page} the new head page
 */
", ,/** * create a new head page object and new page.{@literal {pageNum}} empty valid data file * * @param pageNum the new head page page number * @param queue the {@link Queue} instance * @param pageIO the {@link PageIO} delegate * @return {@link Page} the new head page */,38,40,[0],0,[0],0,[0],0,0,0,0,"newHeadPage(int, Queue, PageIO)",org.logstash.ackedqueue.PageFactory,"newHeadPage/3[int,org.logstash.ackedqueue.Queue,org.logstash.ackedqueue.io.PageIO]",False,38,3,3,2,1,1,0,3,1,0,3,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0,24,9,0,True
128,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\PageFactory.java,org.logstash.ackedqueue.PageFactory,"Page newHeadPage(Checkpoint, Queue, PageIO)","/**
 * create a new head page from an existing {@link Checkpoint} and open page.{@literal {pageNum}} empty valid data file
 *
 * @param checkpoint existing head page {@link Checkpoint}
 * @param queue the {@link Queue} instance
 * @param pageIO the {@link PageIO} delegate
 * @return {@link Page} the new head page
 */
public static Page newHeadPage(Checkpoint checkpoint, Queue queue, PageIO pageIO) throws IOException {
    final Page p = new Page(checkpoint.getPageNum(), queue, checkpoint.getMinSeqNum(), checkpoint.getElementCount(), checkpoint.getFirstUnackedSeqNum(), new BitSet(), pageIO, true);
    try {
        assert checkpoint.getMinSeqNum() == pageIO.getMinSeqNum() && checkpoint.getElementCount() == pageIO.getElementCount() : String.format(""checkpoint minSeqNum=%d or elementCount=%d is different than pageIO minSeqNum=%d or elementCount=%d"", checkpoint.getMinSeqNum(), checkpoint.getElementCount(), pageIO.getMinSeqNum(), pageIO.getElementCount());
        // this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset
        if (checkpoint.getFirstUnackedSeqNum() > checkpoint.getMinSeqNum()) {
            p.ackedSeqNums.flip(0, (int) (checkpoint.getFirstUnackedSeqNum() - checkpoint.getMinSeqNum()));
        }
        return p;
    } catch (Exception e) {
        p.close();
        throw e;
    }
}","/**
 * create a new head page from an existing {@link Checkpoint} and open page.{@literal {pageNum}} empty valid data file
 *
 * @param checkpoint existing head page {@link Checkpoint}
 * @param queue the {@link Queue} instance
 * @param pageIO the {@link PageIO} delegate
 * @return {@link Page} the new head page
 */
","// this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset
","/** * create a new head page from an existing {@link Checkpoint} and open page.{@literal {pageNum}} empty valid data file * * @param checkpoint existing head page {@link Checkpoint} * @param queue the {@link Queue} instance * @param pageIO the {@link PageIO} delegate * @return {@link Page} the new head page */[[SEP]]// this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset",50,75,[0],0,[0],0,"[0, 0]",0,0,0,0,"newHeadPage(Checkpoint, Queue, PageIO)",org.logstash.ackedqueue.PageFactory,"newHeadPage/3[org.logstash.ackedqueue.Checkpoint,org.logstash.ackedqueue.Queue,org.logstash.ackedqueue.io.PageIO]",False,50,4,9,1,8,5,9,14,1,1,3,9,0,0,0,2,1,1,1,1,1,1,2,0,0,0,40,9,0,True
129,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\PageFactory.java,org.logstash.ackedqueue.PageFactory,"Page newTailPage(Checkpoint, Queue, PageIO)","/**
 * create a new tail page for an exiting Checkpoint and data file
 *
 * @param checkpoint existing tail page {@link Checkpoint}
 * @param queue the {@link Queue} instance
 * @param pageIO the {@link PageIO} delegate
 * @return {@link Page} the new tail page
 */
public static Page newTailPage(Checkpoint checkpoint, Queue queue, PageIO pageIO) throws IOException {
    final Page p = new Page(checkpoint.getPageNum(), queue, checkpoint.getMinSeqNum(), checkpoint.getElementCount(), checkpoint.getFirstUnackedSeqNum(), new BitSet(), pageIO, false);
    try {
        // this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset
        if (checkpoint.getFirstUnackedSeqNum() > checkpoint.getMinSeqNum()) {
            p.ackedSeqNums.flip(0, (int) (checkpoint.getFirstUnackedSeqNum() - checkpoint.getMinSeqNum()));
        }
        return p;
    } catch (Exception e) {
        p.close();
        throw e;
    }
}","/**
 * create a new tail page for an exiting Checkpoint and data file
 *
 * @param checkpoint existing tail page {@link Checkpoint}
 * @param queue the {@link Queue} instance
 * @param pageIO the {@link PageIO} delegate
 * @return {@link Page} the new tail page
 */
","// this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset
","/** * create a new tail page for an exiting Checkpoint and data file * * @param checkpoint existing tail page {@link Checkpoint} * @param queue the {@link Queue} instance * @param pageIO the {@link PageIO} delegate * @return {@link Page} the new tail page */[[SEP]]// this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset",85,108,[0],0,[0],0,"[0, 0]",0,0,0,0,"newTailPage(Checkpoint, Queue, PageIO)",org.logstash.ackedqueue.PageFactory,"newTailPage/3[org.logstash.ackedqueue.Checkpoint,org.logstash.ackedqueue.Queue,org.logstash.ackedqueue.io.PageIO]",False,85,4,7,1,6,3,6,13,1,1,3,6,0,0,0,0,1,1,0,1,1,1,2,0,0,0,27,9,0,True
130,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\PqRepair.java,org.logstash.ackedqueue.PqRepair,"void recreateCheckpoint(Path, int)","private static void recreateCheckpoint(final Path pageFile, final int number) throws IOException {
    final ByteBuffer buffer = ByteBuffer.allocateDirect(MmapPageIOV2.SEQNUM_SIZE + MmapPageIOV2.LENGTH_SIZE);
    LOGGER.info(""Recreating missing checkpoint for page {}"", pageFile);
    try (final FileChannel page = FileChannel.open(pageFile)) {
        page.read(buffer);
        final byte version = buffer.get(0);
        if (version != MmapPageIOV1.VERSION_ONE && version != MmapPageIOV2.VERSION_TWO) {
            throw new IllegalStateException(String.format(""Pagefile %s contains version byte %d, this tool only supports versions 1 and 2."", pageFile, version));
        }
        buffer.position(1);
        buffer.compact();
        page.read(buffer);
        final long firstSeqNum = buffer.getLong(0);
        final long maxSize = page.size();
        long position = page.position();
        position += (long) buffer.getInt(8) + (long) MmapPageIOV2.CHECKSUM_SIZE;
        int count = 1;
        while (position < maxSize - MmapPageIOV2.MIN_CAPACITY) {
            page.position(position);
            buffer.clear();
            page.read(buffer);
            position += (long) buffer.getInt(8) + (long) MmapPageIOV2.CHECKSUM_SIZE;
            ++count;
        }
        // Writing 0 for the first unacked page num is ok here, since this value is only
        // used by the head checkpoint
        new FileCheckpointIO(pageFile.getParent()).write(String.format(""checkpoint.%d"", number), number, 0, firstSeqNum, firstSeqNum, count);
    }
}", ,"// Writing 0 for the first unacked page num is ok here, since this value is only
[[SEP]]// used by the head checkpoint
","// Writing 0 for the first unacked page num is ok here, since this value is only// used by the head checkpoint",149,189,[0],0,"[0, 0]",0,[0],0,0,0,0,"recreateCheckpoint(Path, int)",org.logstash.ackedqueue.PqRepair,"recreateCheckpoint/2[java.nio.file.Path,int]",False,150,2,4,1,3,4,16,27,0,7,2,16,0,0,1,2,1,0,3,7,9,4,2,0,0,0,31,10,1,False
131,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\PqRepair.java,org.logstash.ackedqueue.PqRepair,"void fixZeroSizePages(Map<Integer, Path>, Map<Integer, Path>)","/**
 * Deletes all pages that are too small in size to hold at least one event and hence are
 * certainly corrupted as well as their associated checkpoints.
 * @param pages Pages
 * @param checkpoints Checkpoints
 * @throws IOException On Failure
 */
private static void fixZeroSizePages(final Map<Integer, Path> pages, final Map<Integer, Path> checkpoints) throws IOException {
    final int[] knownPagenums = extractPagenums(pages);
    for (final int number : knownPagenums) {
        final Path pagePath = pages.get(number);
        if (pagePath.toFile().length() < (long) MmapPageIOV2.MIN_CAPACITY) {
            LOGGER.info(""Deleting empty page found at {}"", pagePath);
            Files.delete(pagePath);
            pages.remove(number);
            final Path cpPath = checkpoints.remove(number);
            if (cpPath != null) {
                LOGGER.info(""Deleting checkpoint {} because it has no associated page"", cpPath);
                Files.delete(cpPath);
            }
        }
    }
}","/**
 * Deletes all pages that are too small in size to hold at least one event and hence are
 * certainly corrupted as well as their associated checkpoints.
 * @param pages Pages
 * @param checkpoints Checkpoints
 * @throws IOException On Failure
 */
", ,/** * Deletes all pages that are too small in size to hold at least one event and hence are * certainly corrupted as well as their associated checkpoints. * @param pages Pages * @param checkpoints Checkpoints * @throws IOException On Failure */,212,230,[0],0,[0],0,[0],0,0,0,0,"fixZeroSizePages(Map<Integer, Path>, Map<Integer, Path>)",org.logstash.ackedqueue.PqRepair,"fixZeroSizePages/2[java.util.Map<java.lang.Integer,java.nio.file.Path>,java.util.Map<java.lang.Integer,java.nio.file.Path>]",False,213,3,3,1,2,4,7,16,0,3,2,7,1,1,1,1,0,0,2,0,3,0,3,0,0,0,49,10,2,True
132,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,void open(),"/**
 * Open an existing {@link Queue} or create a new one in the configured path.
 * @throws IOException if an IO error occurs
 */
public void open() throws IOException {
    if (!this.closed.get()) {
        throw new IOException(""queue already opened"");
    }
    lock.lock();
    try {
        try {
            // verify exclusive access to the dirPath
            this.dirLock = FileLockFactory.obtainLock(this.dirPath, LOCK_NAME);
        } catch (LockException e) {
            throw new LockException(""The queue failed to obtain exclusive access, cause: "" + e.getMessage());
        }
        try {
            openPages();
            this.closed.set(false);
        } catch (IOException e) {
            // upon any exception while opening the queue and after dirlock has been obtained
            // we need to make sure to release the dirlock. Calling the close method on a partially
            // open queue has no effect because the closed flag is still true.
            releaseLockAndSwallow();
            throw (e);
        }
    } finally {
        lock.unlock();
    }
}","/**
 * Open an existing {@link Queue} or create a new one in the configured path.
 * @throws IOException if an IO error occurs
 */
","// verify exclusive access to the dirPath
[[SEP]]// upon any exception while opening the queue and after dirlock has been obtained
[[SEP]]// we need to make sure to release the dirlock. Calling the close method on a partially
[[SEP]]// open queue has no effect because the closed flag is still true.
",/** * Open an existing {@link Queue} or create a new one in the configured path. * @throws IOException if an IO error occurs */[[SEP]]// verify exclusive access to the dirPath[[SEP]]// upon any exception while opening the queue and after dirlock has been obtained// we need to make sure to release the dirlock. Calling the close method on a partially// open queue has no effect because the closed flag is still true.,167,192,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,open(),org.logstash.ackedqueue.Queue,open/0,False,167,3,45,41,4,4,8,25,0,0,0,8,2,3,0,0,3,1,2,0,1,1,2,0,0,0,30,1,0,True
133,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,void openPages(),"private void openPages() throws IOException {
    final int headPageNum;
    // Upgrade to serialization format V2
    QueueUpgrade.upgradeQueueDirectoryToV2(dirPath);
    Checkpoint headCheckpoint;
    try {
        headCheckpoint = this.checkpointIO.read(checkpointIO.headFileName());
    } catch (NoSuchFileException e) {
        // if there is no head checkpoint, create a new headpage and checkpoint it and exit method
        logger.debug(""No head checkpoint found at: {}, creating new head page"", checkpointIO.headFileName());
        this.ensureDiskAvailable(this.maxBytes, 0);
        this.seqNum = 0;
        headPageNum = 0;
        newCheckpointedHeadpage(headPageNum);
        this.closed.set(false);
        return;
    }
    // at this point we have a head checkpoint to figure queue recovery
    // as we load pages, compute actually disk needed substracting existing pages size to the required maxBytes
    long pqSizeBytes = 0;
    // reconstruct all tail pages state upto but excluding the head page
    for (int pageNum = headCheckpoint.getFirstUnackedPageNum(); pageNum < headCheckpoint.getPageNum(); pageNum++) {
        final String cpFileName = checkpointIO.tailFileName(pageNum);
        if (!dirPath.resolve(cpFileName).toFile().exists()) {
            continue;
        }
        final Checkpoint cp = this.checkpointIO.read(cpFileName);
        logger.debug(""opening tail page: {}, in: {}, with checkpoint: {}"", pageNum, this.dirPath, cp);
        PageIO pageIO = new MmapPageIOV2(pageNum, this.pageCapacity, this.dirPath);
        // important to NOT pageIO.open() just yet, we must first verify if it is fully acked in which case
        // we can purge it and we don't care about its integrity for example if it is of zero-byte file size.
        if (cp.isFullyAcked()) {
            purgeTailPage(cp, pageIO);
        } else {
            pageIO.open(cp.getMinSeqNum(), cp.getElementCount());
            addTailPage(PageFactory.newTailPage(cp, this, pageIO));
            pqSizeBytes += pageIO.getCapacity();
        }
        // track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum
        if (cp.maxSeqNum() > this.seqNum) {
            this.seqNum = cp.maxSeqNum();
        }
    }
    // delete zero byte page and recreate checkpoint if corrupted page is detected
    if (cleanedUpFullyAckedCorruptedPage(headCheckpoint, pqSizeBytes)) {
        return;
    }
    // transform the head page into a tail page only if the headpage is non-empty
    // in both cases it will be checkpointed to track any changes in the firstUnackedPageNum when reconstructing the tail pages
    logger.debug(""opening head page: {}, in: {}, with checkpoint: {}"", headCheckpoint.getPageNum(), this.dirPath, headCheckpoint);
    PageIO pageIO = new MmapPageIOV2(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
    // optimistically recovers the head page data file and set minSeqNum and elementCount to the actual read/recovered data
    pageIO.recover();
    pqSizeBytes += (long) pageIO.getHead();
    ensureDiskAvailable(this.maxBytes, pqSizeBytes);
    if (pageIO.getMinSeqNum() != headCheckpoint.getMinSeqNum() || pageIO.getElementCount() != headCheckpoint.getElementCount()) {
        // the recovered page IO shows different minSeqNum or elementCount than the checkpoint, use the page IO attributes
        logger.warn(""recovered head data page {} is different than checkpoint, using recovered page information"", headCheckpoint.getPageNum());
        logger.debug(""head checkpoint minSeqNum={} or elementCount={} is different than head pageIO minSeqNum={} or elementCount={}"", headCheckpoint.getMinSeqNum(), headCheckpoint.getElementCount(), pageIO.getMinSeqNum(), pageIO.getElementCount());
        long firstUnackedSeqNum = headCheckpoint.getFirstUnackedSeqNum();
        if (firstUnackedSeqNum < pageIO.getMinSeqNum()) {
            logger.debug(""head checkpoint firstUnackedSeqNum={} is < head pageIO minSeqNum={}, using pageIO minSeqNum"", firstUnackedSeqNum, pageIO.getMinSeqNum());
            firstUnackedSeqNum = pageIO.getMinSeqNum();
        }
        headCheckpoint = new Checkpoint(headCheckpoint.getPageNum(), headCheckpoint.getFirstUnackedPageNum(), firstUnackedSeqNum, pageIO.getMinSeqNum(), pageIO.getElementCount());
    }
    this.headPage = PageFactory.newHeadPage(headCheckpoint, this, pageIO);
    if (this.headPage.getMinSeqNum() <= 0 && this.headPage.getElementCount() <= 0) {
        // head page is empty, let's keep it as-is
        // but checkpoint it to update the firstUnackedPageNum if it changed
        this.headPage.checkpoint();
    } else {
        // head page is non-empty, transform it into a tail page
        this.headPage.behead();
        if (headCheckpoint.isFullyAcked()) {
            purgeTailPage(headCheckpoint, pageIO);
        } else {
            addTailPage(this.headPage);
        }
        // track the seqNum as we add this new tail page, prevent empty tailPage with a minSeqNum of 0 to reset seqNum
        if (headCheckpoint.maxSeqNum() > this.seqNum) {
            this.seqNum = headCheckpoint.maxSeqNum();
        }
        // create a new empty head page
        headPageNum = headCheckpoint.getPageNum() + 1;
        newCheckpointedHeadpage(headPageNum);
    }
    // only activate the first tail page
    if (tailPages.size() > 0) {
        this.tailPages.get(0).getPageIO().activate();
    }
    // TODO: here do directory traversal and cleanup lingering pages? could be a background operations to not delay queue start?
}", ,"// at this point we have a head checkpoint to figure queue recovery
[[SEP]]// transform the head page into a tail page only if the headpage is non-empty
[[SEP]]// in both cases it will be checkpointed to track any changes in the firstUnackedPageNum when reconstructing the tail pages
[[SEP]]// TODO: here do directory traversal and cleanup lingering pages? could be a background operations to not delay queue start?
[[SEP]]// Upgrade to serialization format V2
[[SEP]]// if there is no head checkpoint, create a new headpage and checkpoint it and exit method
[[SEP]]// as we load pages, compute actually disk needed substracting existing pages size to the required maxBytes
[[SEP]]// reconstruct all tail pages state upto but excluding the head page
[[SEP]]// important to NOT pageIO.open() just yet, we must first verify if it is fully acked in which case
[[SEP]]// we can purge it and we don't care about its integrity for example if it is of zero-byte file size.
[[SEP]]// track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum
[[SEP]]// delete zero byte page and recreate checkpoint if corrupted page is detected
[[SEP]]// optimistically recovers the head page data file and set minSeqNum and elementCount to the actual read/recovered data
[[SEP]]// the recovered page IO shows different minSeqNum or elementCount than the checkpoint, use the page IO attributes
[[SEP]]// head page is empty, let's keep it as-is
[[SEP]]// but checkpoint it to update the firstUnackedPageNum if it changed
[[SEP]]// head page is non-empty, transform it into a tail page
[[SEP]]// track the seqNum as we add this new tail page, prevent empty tailPage with a minSeqNum of 0 to reset seqNum
[[SEP]]// create a new empty head page
[[SEP]]// only activate the first tail page
","// Upgrade to serialization format V2[[SEP]]// if there is no head checkpoint, create a new headpage and checkpoint it and exit method[[SEP]]// at this point we have a head checkpoint to figure queue recovery// as we load pages, compute actually disk needed substracting existing pages size to the required maxBytes[[SEP]]// reconstruct all tail pages state upto but excluding the head page[[SEP]]// important to NOT pageIO.open() just yet, we must first verify if it is fully acked in which case// we can purge it and we don't care about its integrity for example if it is of zero-byte file size.[[SEP]]// track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum[[SEP]]// delete zero byte page and recreate checkpoint if corrupted page is detected[[SEP]]// transform the head page into a tail page only if the headpage is non-empty// in both cases it will be checkpointed to track any changes in the firstUnackedPageNum when reconstructing the tail pages[[SEP]]// optimistically recovers the head page data file and set minSeqNum and elementCount to the actual read/recovered data[[SEP]]// the recovered page IO shows different minSeqNum or elementCount than the checkpoint, use the page IO attributes[[SEP]]// head page is empty, let's keep it as-is// but checkpoint it to update the firstUnackedPageNum if it changed[[SEP]]// head page is non-empty, transform it into a tail page[[SEP]]// track the seqNum as we add this new tail page, prevent empty tailPage with a minSeqNum of 0 to reset seqNum[[SEP]]// create a new empty head page[[SEP]]// only activate the first tail page[[SEP]]// TODO: here do directory traversal and cleanup lingering pages? could be a background operations to not delay queue start?",194,310,[0],0,"[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",1,"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",1,1,1,1,openPages(),org.logstash.ackedqueue.Queue,openPages/0,False,194,9,38,1,37,15,41,77,2,9,0,41,5,2,1,2,1,0,6,9,18,1,2,0,0,0,56,2,6,False
134,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,"boolean cleanedUpFullyAckedCorruptedPage(Checkpoint, long)","/**
 * When the queue is fully acked and zero byte page is found, delete corrupted page and recreate checkpoint head
 * @param headCheckpoint
 * @param pqSizeBytes
 * @return true when corrupted page is found and cleaned
 * @throws IOException
 */
private boolean cleanedUpFullyAckedCorruptedPage(Checkpoint headCheckpoint, long pqSizeBytes) throws IOException {
    if (headCheckpoint.isFullyAcked()) {
        PageIO pageIO = new MmapPageIOV2(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
        if (pageIO.isCorruptedPage()) {
            logger.debug(""Queue is fully acked. Found zero byte page.{}. Recreate checkpoint.head and delete corrupted page"", headCheckpoint.getPageNum());
            this.checkpointIO.purge(checkpointIO.headFileName());
            pageIO.purge();
            if (headCheckpoint.maxSeqNum() > this.seqNum) {
                this.seqNum = headCheckpoint.maxSeqNum();
            }
            newCheckpointedHeadpage(headCheckpoint.getPageNum() + 1);
            pqSizeBytes += (long) pageIO.getHead();
            ensureDiskAvailable(this.maxBytes, pqSizeBytes);
            return true;
        }
    }
    return false;
}","/**
 * When the queue is fully acked and zero byte page is found, delete corrupted page and recreate checkpoint head
 * @param headCheckpoint
 * @param pqSizeBytes
 * @return true when corrupted page is found and cleaned
 * @throws IOException
 */
", ,"/** * When the queue is fully acked and zero byte page is found, delete corrupted page and recreate checkpoint head * @param headCheckpoint * @param pqSizeBytes * @return true when corrupted page is found and cleaned * @throws IOException */",319,340,[0],0,[0],0,[0],0,0,0,0,"cleanedUpFullyAckedCorruptedPage(Checkpoint, long)",org.logstash.ackedqueue.Queue,"cleanedUpFullyAckedCorruptedPage/2[org.logstash.ackedqueue.Checkpoint,long]",False,319,6,13,1,12,4,11,18,2,1,2,11,2,1,0,0,0,0,1,1,3,1,3,0,0,0,41,2,1,True
135,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,"void purgeTailPage(Checkpoint, PageIO)","/**
 * delete files for the given page
 *
 * @param checkpoint the tail page {@link Checkpoint}
 * @param pageIO the tail page {@link PageIO}
 * @throws IOException
 */
private void purgeTailPage(Checkpoint checkpoint, PageIO pageIO) throws IOException {
    try {
        pageIO.purge();
    } catch (NoSuchFileException e) {
        /* ignore */
        logger.debug(""tail page does not exist: {}"", pageIO);
    }
    // we want to keep all the ""middle"" checkpoints between the first unacked tail page and the head page
    // to always have a contiguous sequence of checkpoints which helps figuring queue integrity. for this
    // we will remove any prepended fully acked tail pages but keep all other checkpoints between the first
    // unacked tail page and the head page. we did however purge the data file to free disk resources.
    if (this.tailPages.size() == 0) {
        // this is the first tail page and it is fully acked so just purge it
        this.checkpointIO.purge(this.checkpointIO.tailFileName(checkpoint.getPageNum()));
    }
}","/**
 * delete files for the given page
 *
 * @param checkpoint the tail page {@link Checkpoint}
 * @param pageIO the tail page {@link PageIO}
 * @throws IOException
 */
","// we want to keep all the ""middle"" checkpoints between the first unacked tail page and the head page
[[SEP]]// to always have a contiguous sequence of checkpoints which helps figuring queue integrity. for this
[[SEP]]// we will remove any prepended fully acked tail pages but keep all other checkpoints between the first
[[SEP]]// unacked tail page and the head page. we did however purge the data file to free disk resources.
[[SEP]]/* ignore */
[[SEP]]// this is the first tail page and it is fully acked so just purge it
","/** * delete files for the given page * * @param checkpoint the tail page {@link Checkpoint} * @param pageIO the tail page {@link PageIO} * @throws IOException */[[SEP]]/* ignore */[[SEP]]// we want to keep all the ""middle"" checkpoints between the first unacked tail page and the head page// to always have a contiguous sequence of checkpoints which helps figuring queue integrity. for this// we will remove any prepended fully acked tail pages but keep all other checkpoints between the first// unacked tail page and the head page. we did however purge the data file to free disk resources.[[SEP]]// this is the first tail page and it is fully acked so just purge it",349,365,[0],0,"[0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,"purgeTailPage(Checkpoint, PageIO)",org.logstash.ackedqueue.Queue,"purgeTailPage/2[org.logstash.ackedqueue.Checkpoint,org.logstash.ackedqueue.io.PageIO]",False,349,4,6,1,5,3,6,11,0,0,2,6,0,0,0,1,1,0,1,1,0,0,1,0,0,0,20,2,1,True
136,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,void addTailPage(Page),"/**
 * add a not fully-acked tail page into this queue structures and un-mmap it.
 *
 * @param page the tail {@link Page}
 * @throws IOException
 */
private void addTailPage(Page page) throws IOException {
    this.tailPages.add(page);
    this.unreadTailPages.add(page);
    this.unreadCount += page.unreadCount();
    // for now deactivate all tail pages, we will only reactivate the first one at the end
    page.getPageIO().deactivate();
}","/**
 * add a not fully-acked tail page into this queue structures and un-mmap it.
 *
 * @param page the tail {@link Page}
 * @throws IOException
 */
","// for now deactivate all tail pages, we will only reactivate the first one at the end
","/** * add a not fully-acked tail page into this queue structures and un-mmap it. * * @param page the tail {@link Page} * @throws IOException */[[SEP]]// for now deactivate all tail pages, we will only reactivate the first one at the end",373,380,[0],0,[0],0,"[0, 0]",0,0,0,0,addTailPage(Page),org.logstash.ackedqueue.Queue,addTailPage/1[org.logstash.ackedqueue.Page],False,373,2,4,1,3,1,4,6,0,0,1,4,0,0,0,0,0,0,0,0,1,0,0,0,0,0,19,2,0,True
137,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,void newCheckpointedHeadpage(int),"/**
 * create a new empty headpage for the given pageNum and immediately checkpoint it
 *
 * @param pageNum the page number of the new head page
 * @throws IOException
 */
private void newCheckpointedHeadpage(int pageNum) throws IOException {
    PageIO headPageIO = new MmapPageIOV2(pageNum, this.pageCapacity, this.dirPath);
    headPageIO.create();
    logger.debug(""created new head page: {}"", headPageIO);
    this.headPage = PageFactory.newHeadPage(pageNum, this, headPageIO);
    this.headPage.forceCheckpoint();
}","/**
 * create a new empty headpage for the given pageNum and immediately checkpoint it
 *
 * @param pageNum the page number of the new head page
 * @throws IOException
 */
", ,/** * create a new empty headpage for the given pageNum and immediately checkpoint it * * @param pageNum the page number of the new head page * @throws IOException */,388,394,[0],0,[0],0,[0],0,0,0,0,newCheckpointedHeadpage(int),org.logstash.ackedqueue.Queue,newCheckpointedHeadpage/1[int],False,388,5,8,3,5,1,4,7,0,1,1,4,0,0,0,0,0,0,1,0,2,0,0,0,0,0,24,2,1,True
138,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,long write(Queueable),"/**
 * write a {@link Queueable} element to the queue. Note that the element will always be written and the queue full
 * condition will be checked and waited on **after** the write operation.
 *
 * @param element the {@link Queueable} element to write
 * @return the written sequence number
 * @throws IOException if an IO error occurs
 */
public long write(Queueable element) throws IOException {
    byte[] data = element.serialize();
    // the write strategy with regard to the isFull() state is to assume there is space for this element
    // and write it, then after write verify if we just filled the queue and wait on the notFull condition
    // *after* the write which is both safer for a crash condition, and the queue closing sequence. In the former case
    // holding an element in memory while waiting for the notFull condition would mean always having the current write
    // element at risk in the always-full queue state. In the later, when closing a full queue, it would be impossible
    // to write the current element.
    lock.lock();
    try {
        if (!this.headPage.hasCapacity(data.length)) {
            throw new IOException(""data to be written is bigger than page capacity"");
        }
        // create a new head page if the current does not have sufficient space left for data to be written
        if (!this.headPage.hasSpace(data.length)) {
            // TODO: verify queue state integrity WRT Queue.open()/recover() at each step of this process
            int newHeadPageNum = this.headPage.pageNum + 1;
            if (this.headPage.isFullyAcked()) {
                // here we can just purge the data file and avoid beheading since we do not need
                // to add this fully hacked page into tailPages. a new head page will just be created.
                // TODO: we could possibly reuse the same page file but just rename it?
                this.headPage.purge();
            } else {
                behead();
            }
            // create new head page
            newCheckpointedHeadpage(newHeadPageNum);
        }
        long seqNum = this.seqNum += 1;
        this.headPage.write(data, seqNum, this.checkpointMaxWrites);
        this.unreadCount++;
        notEmpty.signal();
        // now check if we reached a queue full state and block here until it is not full
        // for the next write or the queue was closed.
        while (isFull() && !isClosed()) {
            try {
                notFull.await();
            } catch (InterruptedException e) {
                logger.debug(""interrupted waiting for queue to not be full"", e);
                // the thread interrupt() has been called while in the await() blocking call.
                // at this point the interrupted flag is reset and Thread.interrupted() will return false
                // to any upstream calls on it. for now our choice is to return normally and set back
                // the Thread.interrupted() flag so it can be checked upstream.
                // this is a bit tricky in the case of the queue full condition blocking state.
                // TODO: we will want to avoid initiating a new write operation if Thread.interrupted() was called.
                // set back the interrupted flag
                Thread.currentThread().interrupt();
                return seqNum;
            }
        }
        return seqNum;
    } finally {
        lock.unlock();
    }
}","/**
 * write a {@link Queueable} element to the queue. Note that the element will always be written and the queue full
 * condition will be checked and waited on **after** the write operation.
 *
 * @param element the {@link Queueable} element to write
 * @return the written sequence number
 * @throws IOException if an IO error occurs
 */
","// the write strategy with regard to the isFull() state is to assume there is space for this element
[[SEP]]// and write it, then after write verify if we just filled the queue and wait on the notFull condition
[[SEP]]// *after* the write which is both safer for a crash condition, and the queue closing sequence. In the former case
[[SEP]]// holding an element in memory while waiting for the notFull condition would mean always having the current write
[[SEP]]// element at risk in the always-full queue state. In the later, when closing a full queue, it would be impossible
[[SEP]]// to write the current element.
[[SEP]]// now check if we reached a queue full state and block here until it is not full
[[SEP]]// create a new head page if the current does not have sufficient space left for data to be written
[[SEP]]// TODO: verify queue state integrity WRT Queue.open()/recover() at each step of this process
[[SEP]]// here we can just purge the data file and avoid beheading since we do not need
[[SEP]]// to add this fully hacked page into tailPages. a new head page will just be created.
[[SEP]]// TODO: we could possibly reuse the same page file but just rename it?
[[SEP]]// create new head page
[[SEP]]// for the next write or the queue was closed.
[[SEP]]// the thread interrupt() has been called while in the await() blocking call.
[[SEP]]// at this point the interrupted flag is reset and Thread.interrupted() will return false
[[SEP]]// to any upstream calls on it. for now our choice is to return normally and set back
[[SEP]]// the Thread.interrupted() flag so it can be checked upstream.
[[SEP]]// this is a bit tricky in the case of the queue full condition blocking state.
[[SEP]]// TODO: we will want to avoid initiating a new write operation if Thread.interrupted() was called.
[[SEP]]// set back the interrupted flag
","/** * write a {@link Queueable} element to the queue. Note that the element will always be written and the queue full * condition will be checked and waited on **after** the write operation. * * @param element the {@link Queueable} element to write * @return the written sequence number * @throws IOException if an IO error occurs */[[SEP]]// the write strategy with regard to the isFull() state is to assume there is space for this element// and write it, then after write verify if we just filled the queue and wait on the notFull condition// *after* the write which is both safer for a crash condition, and the queue closing sequence. In the former case// holding an element in memory while waiting for the notFull condition would mean always having the current write// element at risk in the always-full queue state. In the later, when closing a full queue, it would be impossible// to write the current element.[[SEP]]// create a new head page if the current does not have sufficient space left for data to be written[[SEP]]// TODO: verify queue state integrity WRT Queue.open()/recover() at each step of this process[[SEP]]// here we can just purge the data file and avoid beheading since we do not need// to add this fully hacked page into tailPages. a new head page will just be created.// TODO: we could possibly reuse the same page file but just rename it?[[SEP]]// create new head page[[SEP]]// now check if we reached a queue full state and block here until it is not full// for the next write or the queue was closed.[[SEP]]// the thread interrupt() has been called while in the await() blocking call.// at this point the interrupted flag is reset and Thread.interrupted() will return false// to any upstream calls on it. for now our choice is to return normally and set back// the Thread.interrupted() flag so it can be checked upstream.// this is a bit tricky in the case of the queue full condition blocking state.// TODO: we will want to avoid initiating a new write operation if Thread.interrupted() was called.// set back the interrupted flag",404,472,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]",1,"[0, 0, 0, 1, 1, 0, 0, 1]",1,1,1,1,write(Queueable),org.logstash.ackedqueue.Queue,write/1[org.logstash.ackedqueue.Queueable],False,404,5,43,32,11,7,17,37,2,3,1,17,4,3,1,0,2,0,2,2,4,1,3,0,0,0,46,1,1,True
139,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,void behead(),"/**
 * mark head page as read-only (behead) and add it to the tailPages and unreadTailPages collections accordingly
 * also deactivate it if it's not next-in-line for reading
 *
 * @throws IOException if an IO error occurs
 */
private void behead() throws IOException {
    // beheading includes checkpoint+fsync if required
    this.headPage.behead();
    this.tailPages.add(this.headPage);
    if (!this.headPage.isFullyRead()) {
        if (!this.unreadTailPages.isEmpty()) {
            // there are already other unread pages so this new one is not next in line and we can deactivate
            this.headPage.deactivate();
        }
        this.unreadTailPages.add(this.headPage);
    } else {
        // it is fully read so we can deactivate
        this.headPage.deactivate();
    }
}","/**
 * mark head page as read-only (behead) and add it to the tailPages and unreadTailPages collections accordingly
 * also deactivate it if it's not next-in-line for reading
 *
 * @throws IOException if an IO error occurs
 */
","// beheading includes checkpoint+fsync if required
[[SEP]]// there are already other unread pages so this new one is not next in line and we can deactivate
[[SEP]]// it is fully read so we can deactivate
",/** * mark head page as read-only (behead) and add it to the tailPages and unreadTailPages collections accordingly * also deactivate it if it's not next-in-line for reading * * @throws IOException if an IO error occurs */[[SEP]]// beheading includes checkpoint+fsync if required[[SEP]]// there are already other unread pages so this new one is not next in line and we can deactivate[[SEP]]// it is fully read so we can deactivate,480,495,[0],0,"[0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,behead(),org.logstash.ackedqueue.Queue,behead/0,False,480,1,4,1,3,3,5,13,0,0,0,5,0,0,0,0,0,0,0,0,0,0,2,0,0,0,31,2,0,True
140,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,boolean isFull(),"/**
 * <p>Checks if the Queue is full, with ""full"" defined as either of:</p>
 * <p>Assuming a maximum size of the queue larger than 0 is defined:</p>
 * <ul>
 *     <li>The sum of the size of all allocated pages is more than the allowed maximum Queue
 *     size</li>
 *     <li>The sum of the size of all allocated pages equal to the allowed maximum Queue size
 *     and the current head page has no remaining capacity.</li>
 * </ul>
 * <p>or assuming a max unread count larger than 0, is defined ""full"" is also defined as:</p>
 * <ul>
 *     <li>The current number of unread events exceeds or is equal to the configured maximum
 *     number of allowed unread events.</li>
 * </ul>
 * @return True iff the queue is full
 */
public boolean isFull() {
    lock.lock();
    try {
        return isMaxBytesReached() || isMaxUnreadReached();
    } finally {
        lock.unlock();
    }
}","/**
 * <p>Checks if the Queue is full, with ""full"" defined as either of:</p>
 * <p>Assuming a maximum size of the queue larger than 0 is defined:</p>
 * <ul>
 *     <li>The sum of the size of all allocated pages is more than the allowed maximum Queue
 *     size</li>
 *     <li>The sum of the size of all allocated pages equal to the allowed maximum Queue size
 *     and the current head page has no remaining capacity.</li>
 * </ul>
 * <p>or assuming a max unread count larger than 0, is defined ""full"" is also defined as:</p>
 * <ul>
 *     <li>The current number of unread events exceeds or is equal to the configured maximum
 *     number of allowed unread events.</li>
 * </ul>
 * @return True iff the queue is full
 */
", ,"/** * <p>Checks if the Queue is full, with ""full"" defined as either of:</p> * <p>Assuming a maximum size of the queue larger than 0 is defined:</p> * <ul> *     <li>The sum of the size of all allocated pages is more than the allowed maximum Queue *     size</li> *     <li>The sum of the size of all allocated pages equal to the allowed maximum Queue size *     and the current head page has no remaining capacity.</li> * </ul> * <p>or assuming a max unread count larger than 0, is defined ""full"" is also defined as:</p> * <ul> *     <li>The current number of unread events exceeds or is equal to the configured maximum *     number of allowed unread events.</li> * </ul> * @return True iff the queue is full */",513,520,[0],0,[0],0,[0],0,0,0,0,isFull(),org.logstash.ackedqueue.Queue,isFull/0,False,513,1,9,7,2,1,4,9,1,0,0,4,2,2,0,0,1,0,0,0,0,0,1,0,0,0,53,1,0,True
141,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,boolean isEmpty(),"/**
 * Queue is considered empty if it does not contain any tail page and the headpage has no element or all
 * elements are acked
 *
 * TODO: note that this should be the same as isFullyAcked once fixed per https://github.com/elastic/logstash/issues/7570
 *
 * @return true if the queue has no tail page and the head page is empty.
 */
public boolean isEmpty() {
    lock.lock();
    try {
        return this.tailPages.isEmpty() && this.headPage.isEmpty();
    } finally {
        lock.unlock();
    }
}","/**
 * Queue is considered empty if it does not contain any tail page and the headpage has no element or all
 * elements are acked
 *
 * TODO: note that this should be the same as isFullyAcked once fixed per https://github.com/elastic/logstash/issues/7570
 *
 * @return true if the queue has no tail page and the head page is empty.
 */
", ,/** * Queue is considered empty if it does not contain any tail page and the headpage has no element or all * elements are acked * * TODO: note that this should be the same as isFullyAcked once fixed per https://github.com/elastic/logstash/issues/7570 * * @return true if the queue has no tail page and the head page is empty. */,543,551,[1],1,[0],0,[1],1,1,1,1,isEmpty(),org.logstash.ackedqueue.Queue,isEmpty/0,False,543,1,3,2,1,1,4,9,1,0,0,4,0,0,0,0,1,0,0,0,0,0,1,0,0,0,40,1,0,True
142,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,boolean isFullyAcked(),"/**
 * @return true if the queue is fully acked, which implies that it is fully read which works as an ""empty"" state.
 */
public boolean isFullyAcked() {
    lock.lock();
    try {
        return this.tailPages.isEmpty() ? this.headPage.isFullyAcked() : false;
    } finally {
        lock.unlock();
    }
}","/**
 * @return true if the queue is fully acked, which implies that it is fully read which works as an ""empty"" state.
 */
", ,"/** * @return true if the queue is fully acked, which implies that it is fully read which works as an ""empty"" state. */",556,563,[0],0,[0],0,[0],0,0,0,0,isFullyAcked(),org.logstash.ackedqueue.Queue,isFullyAcked/0,False,556,1,6,5,1,2,4,9,1,0,0,4,0,0,0,0,1,0,0,0,0,0,1,0,0,0,16,1,0,True
143,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,void ensurePersistedUpto(long),"/**
 * guarantee persistence up to a given sequence number.
 *
 * @param seqNum the element sequence number upper bound for which persistence should be guaranteed (by fsync'ing)
 * @throws IOException if an IO error occurs
 */
public void ensurePersistedUpto(long seqNum) throws IOException {
    lock.lock();
    try {
        this.headPage.ensurePersistedUpto(seqNum);
    } finally {
        lock.unlock();
    }
}","/**
 * guarantee persistence up to a given sequence number.
 *
 * @param seqNum the element sequence number upper bound for which persistence should be guaranteed (by fsync'ing)
 * @throws IOException if an IO error occurs
 */
", ,/** * guarantee persistence up to a given sequence number. * * @param seqNum the element sequence number upper bound for which persistence should be guaranteed (by fsync'ing) * @throws IOException if an IO error occurs */,571,578,[0],0,[0],0,[0],0,0,0,0,ensurePersistedUpto(long),org.logstash.ackedqueue.Queue,ensurePersistedUpto/1[long],False,571,1,4,3,1,1,3,9,0,0,1,3,0,0,0,0,1,0,0,0,0,0,1,0,0,0,28,1,0,True
144,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,Batch nonBlockReadBatch(int),"/**
 * non-blocking queue read
 *
 * @param limit read the next batch of size up to this limit. the returned batch size can be smaller than the requested limit if fewer elements are available
 * @return {@link Batch} the batch containing 1 or more element up to the required limit or null of no elements were available
 * @throws IOException if an IO error occurs
 */
public synchronized Batch nonBlockReadBatch(int limit) throws IOException {
    lock.lock();
    try {
        Page p = nextReadPage();
        return (isHeadPage(p) && p.isFullyRead()) ? null : readPageBatch(p, limit, 0L);
    } finally {
        lock.unlock();
    }
}","/**
 * non-blocking queue read
 *
 * @param limit read the next batch of size up to this limit. the returned batch size can be smaller than the requested limit if fewer elements are available
 * @return {@link Batch} the batch containing 1 or more element up to the required limit or null of no elements were available
 * @throws IOException if an IO error occurs
 */
", ,/** * non-blocking queue read * * @param limit read the next batch of size up to this limit. the returned batch size can be smaller than the requested limit if fewer elements are available * @return {@link Batch} the batch containing 1 or more element up to the required limit or null of no elements were available * @throws IOException if an IO error occurs */,587,595,[0],0,[0],0,[0],0,0,0,0,nonBlockReadBatch(int),org.logstash.ackedqueue.Queue,nonBlockReadBatch/1[int],False,587,3,18,14,4,3,6,10,1,1,1,6,3,3,0,0,1,1,0,1,1,0,1,0,0,0,43,33,0,True
145,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,"Batch readBatch(int, long)","/**
 * @param limit size limit of the batch to read. returned {@link Batch} can be smaller.
 * @param timeout the maximum time to wait in milliseconds on write operations
 * @return the read {@link Batch} or null if no element upon timeout
 * @throws IOException if an IO error occurs
 */
public synchronized Batch readBatch(int limit, long timeout) throws IOException {
    lock.lock();
    try {
        return readPageBatch(nextReadPage(), limit, timeout);
    } finally {
        lock.unlock();
    }
}","/**
 * @param limit size limit of the batch to read. returned {@link Batch} can be smaller.
 * @param timeout the maximum time to wait in milliseconds on write operations
 * @return the read {@link Batch} or null if no element upon timeout
 * @throws IOException if an IO error occurs
 */
", ,/** * @param limit size limit of the batch to read. returned {@link Batch} can be smaller. * @param timeout the maximum time to wait in milliseconds on write operations * @return the read {@link Batch} or null if no element upon timeout * @throws IOException if an IO error occurs */,604,611,[0],0,[0],0,[0],0,0,0,0,"readBatch(int, long)",org.logstash.ackedqueue.Queue,"readBatch/2[int,long]",False,604,2,19,17,2,1,4,9,1,0,2,4,2,3,0,0,1,0,0,0,0,0,1,0,0,0,34,33,0,True
146,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,"Batch readPageBatch(Page, int, long)","/**
 * read a {@link Batch} from the given {@link Page}. If the page is a head page, try to maximize the
 * batch size by waiting for writes.
 * @param p the {@link Page} to read from.
 * @param limit size limit of the batch to read.
 * @param timeout  the maximum time to wait in milliseconds on write operations.
 * @return {@link Batch} with read elements or null if nothing was read
 * @throws IOException if an IO error occurs
 */
private Batch readPageBatch(Page p, int limit, long timeout) throws IOException {
    int left = limit;
    final List<byte[]> elements = new ArrayList<>(limit);
    // NOTE: the tricky thing here is that upon entering this method, if p is initially a head page
    // it could become a tail page upon returning from the notEmpty.await call.
    long firstSeqNum = -1L;
    while (left > 0) {
        if (isHeadPage(p) && p.isFullyRead()) {
            boolean elapsed;
            // a head page is fully read but can be written to so let's wait for more data
            try {
                elapsed = !notEmpty.await(timeout, TimeUnit.MILLISECONDS);
            } catch (InterruptedException e) {
                // set back the interrupted flag
                Thread.currentThread().interrupt();
                break;
            }
            if ((elapsed && p.isFullyRead()) || isClosed()) {
                break;
            }
        }
        if (!p.isFullyRead()) {
            boolean wasFull = isMaxUnreadReached();
            final SequencedList<byte[]> serialized = p.read(left);
            int n = serialized.getElements().size();
            assert n > 0 : ""page read returned 0 elements"";
            elements.addAll(serialized.getElements());
            if (firstSeqNum == -1L) {
                firstSeqNum = serialized.getSeqNums().get(0);
            }
            this.unreadCount -= n;
            left -= n;
            if (wasFull) {
                notFull.signalAll();
            }
        }
        if (isTailPage(p) && p.isFullyRead()) {
            break;
        }
    }
    if (isTailPage(p) && p.isFullyRead()) {
        removeUnreadPage(p);
    }
    return new Batch(elements, firstSeqNum, this);
}","/**
 * read a {@link Batch} from the given {@link Page}. If the page is a head page, try to maximize the
 * batch size by waiting for writes.
 * @param p the {@link Page} to read from.
 * @param limit size limit of the batch to read.
 * @param timeout  the maximum time to wait in milliseconds on write operations.
 * @return {@link Batch} with read elements or null if nothing was read
 * @throws IOException if an IO error occurs
 */
","// NOTE: the tricky thing here is that upon entering this method, if p is initially a head page
[[SEP]]// it could become a tail page upon returning from the notEmpty.await call.
[[SEP]]// a head page is fully read but can be written to so let's wait for more data
[[SEP]]// set back the interrupted flag
","/** * read a {@link Batch} from the given {@link Page}. If the page is a head page, try to maximize the * batch size by waiting for writes. * @param p the {@link Page} to read from. * @param limit size limit of the batch to read. * @param timeout  the maximum time to wait in milliseconds on write operations. * @return {@link Batch} with read elements or null if nothing was read * @throws IOException if an IO error occurs */[[SEP]]// NOTE: the tricky thing here is that upon entering this method, if p is initially a head page// it could become a tail page upon returning from the notEmpty.await call.[[SEP]]// a head page is fully read but can be written to so let's wait for more data[[SEP]]// set back the interrupted flag",622,675,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,"readPageBatch(Page, int, long)",org.logstash.ackedqueue.Queue,"readPageBatch/3[org.logstash.ackedqueue.Page,int,long]",False,622,6,13,2,11,15,16,42,1,7,3,16,5,2,1,1,1,1,1,5,10,0,3,0,0,0,61,2,0,True
147,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,int binaryFindPageForSeqnum(long),"/**
 * perform a binary search through tail pages to find in which page this seqNum falls into
 *
 * @param seqNum the sequence number to search for in the tail pages
 * @return Index of the found {@link Page} in {@link #tailPages}
 */
private int binaryFindPageForSeqnum(final long seqNum) {
    int lo = 0;
    int hi = this.tailPages.size() - 1;
    while (lo <= hi) {
        final int mid = lo + (hi - lo) / 2;
        final Page p = this.tailPages.get(mid);
        final long pMinSeq = p.getMinSeqNum();
        if (seqNum < pMinSeq) {
            hi = mid - 1;
        } else if (seqNum >= pMinSeq + (long) p.getElementCount()) {
            lo = mid + 1;
        } else {
            return mid;
        }
    }
    throw new IllegalArgumentException(String.format(""Sequence number %d not found in any page"", seqNum));
}","/**
 * perform a binary search through tail pages to find in which page this seqNum falls into
 *
 * @param seqNum the sequence number to search for in the tail pages
 * @return Index of the found {@link Page} in {@link #tailPages}
 */
", ,/** * perform a binary search through tail pages to find in which page this seqNum falls into * * @param seqNum the sequence number to search for in the tail pages * @return Index of the found {@link Page} in {@link #tailPages} */,683,701,[0],0,[0],0,[0],0,0,0,0,binaryFindPageForSeqnum(long),org.logstash.ackedqueue.Queue,binaryFindPageForSeqnum/1[long],False,683,1,3,1,2,4,5,19,1,5,1,5,0,0,1,0,0,1,1,5,7,7,2,0,0,0,37,2,0,True
148,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,"void ack(long, int)","/**
 * ack a list of seqNums that are assumed to be all part of the same page, leveraging the fact that batches are also created from
 * same-page elements. A fully acked page will trigger a checkpoint for that page. Also if a page has more than checkpointMaxAcks
 * acks since last checkpoint it will also trigger a checkpoint.
 *
 * @param firstAckSeqNum First Sequence Number to Ack
 * @param ackCount Number of Elements to Ack
 * @throws IOException if an IO error occurs
 */
public void ack(final long firstAckSeqNum, final int ackCount) throws IOException {
    // as a first implementation we assume that all batches are created from the same page
    lock.lock();
    try {
        if (containsSeq(headPage, firstAckSeqNum)) {
            this.headPage.ack(firstAckSeqNum, ackCount, this.checkpointMaxAcks);
        } else {
            final int resultIndex = binaryFindPageForSeqnum(firstAckSeqNum);
            if (tailPages.get(resultIndex).ack(firstAckSeqNum, ackCount, this.checkpointMaxAcks)) {
                this.tailPages.remove(resultIndex);
                notFull.signalAll();
            }
            this.headPage.checkpoint();
        }
    } finally {
        lock.unlock();
    }
}","/**
 * ack a list of seqNums that are assumed to be all part of the same page, leveraging the fact that batches are also created from
 * same-page elements. A fully acked page will trigger a checkpoint for that page. Also if a page has more than checkpointMaxAcks
 * acks since last checkpoint it will also trigger a checkpoint.
 *
 * @param firstAckSeqNum First Sequence Number to Ack
 * @param ackCount Number of Elements to Ack
 * @throws IOException if an IO error occurs
 */
","// as a first implementation we assume that all batches are created from the same page
","/** * ack a list of seqNums that are assumed to be all part of the same page, leveraging the fact that batches are also created from * same-page elements. A fully acked page will trigger a checkpoint for that page. Also if a page has more than checkpointMaxAcks * acks since last checkpoint it will also trigger a checkpoint. * * @param firstAckSeqNum First Sequence Number to Ack * @param ackCount Number of Elements to Ack * @throws IOException if an IO error occurs */[[SEP]]// as a first implementation we assume that all batches are created from the same page",712,729,[0],0,[0],0,"[0, 0]",0,0,0,0,"ack(long, int)",org.logstash.ackedqueue.Queue,"ack/2[long,int]",False,712,2,6,2,4,3,9,19,0,1,2,9,2,1,0,0,1,0,0,0,1,0,3,0,0,0,61,1,0,True
149,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,Queueable deserialize(byte[]),"/**
 *  deserialize a byte array into the required element class.
 *
 * @param bytes the byte array to deserialize
 * @return {@link Queueable} the deserialized byte array into the required Queueable interface implementation concrete class
 */
public Queueable deserialize(byte[] bytes) {
    try {
        return (Queueable) this.deserializeMethod.invoke(this.elementClass, bytes);
    } catch (IllegalAccessException | InvocationTargetException e) {
        throw new QueueRuntimeException(""deserialize invocation error"", e);
    }
}","/**
 *  deserialize a byte array into the required element class.
 *
 * @param bytes the byte array to deserialize
 * @return {@link Queueable} the deserialized byte array into the required Queueable interface implementation concrete class
 */
", ,/** *  deserialize a byte array into the required element class. * * @param bytes the byte array to deserialize * @return {@link Queueable} the deserialized byte array into the required Queueable interface implementation concrete class */,741,747,[0],0,[0],0,[0],0,0,0,0,deserialize(byte[]),org.logstash.ackedqueue.Queue,deserialize/1[byte[]],False,741,3,2,1,1,2,1,8,1,0,1,1,0,0,0,0,1,0,1,0,0,0,1,0,0,0,22,1,0,True
150,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,void close(),"@Override
public void close() throws IOException {
    // TODO: review close strategy and exception handling and resiliency of first closing tail pages if crash in the middle
    if (closed.getAndSet(true) == false) {
        lock.lock();
        try {
            // TODO: not sure if we need to do this here since the headpage close will also call ensurePersisted
            ensurePersistedUpto(this.seqNum);
            for (Page p : this.tailPages) {
                p.close();
            }
            this.headPage.close();
            // release all referenced objects
            this.tailPages.clear();
            this.unreadTailPages.clear();
            this.headPage = null;
            // unblock blocked reads which will return null by checking of isClosed()
            // no data will be lost because the actual read has not been performed
            notEmpty.signalAll();
            // unblock blocked writes. a write is blocked *after* the write has been performed so
            // unblocking is safe and will return from the write call
            notFull.signalAll();
        } finally {
            releaseLockAndSwallow();
            lock.unlock();
        }
    }
}", ,"// TODO: review close strategy and exception handling and resiliency of first closing tail pages if crash in the middle
[[SEP]]// unblock blocked reads which will return null by checking of isClosed()
[[SEP]]// unblock blocked writes. a write is blocked *after* the write has been performed so
[[SEP]]// TODO: not sure if we need to do this here since the headpage close will also call ensurePersisted
[[SEP]]// release all referenced objects
[[SEP]]// no data will be lost because the actual read has not been performed
[[SEP]]// unblocking is safe and will return from the write call
",// TODO: review close strategy and exception handling and resiliency of first closing tail pages if crash in the middle[[SEP]]// TODO: not sure if we need to do this here since the headpage close will also call ensurePersisted[[SEP]]// release all referenced objects[[SEP]]// unblock blocked reads which will return null by checking of isClosed()// no data will be lost because the actual read has not been performed[[SEP]]// unblock blocked writes. a write is blocked *after* the write has been performed so// unblocking is safe and will return from the write call,749,781,[0],0,"[1, 0, 0, 1, 0, 0, 0]",1,"[1, 1, 0, 0, 0]",1,1,1,1,close(),org.logstash.ackedqueue.Queue,close/0,False,750,2,7,4,3,3,8,21,0,0,0,8,2,1,1,1,1,0,0,0,1,0,3,0,0,0,16,1,0,False
151,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,void releaseLockAndSwallow(),"private void releaseLockAndSwallow() {
    try {
        FileLockFactory.releaseLock(this.dirLock);
    } catch (IOException e) {
        // log error and ignore
        logger.error(""Queue close releaseLock failed, error={}"", e.getMessage());
    }
}", ,"// log error and ignore
",// log error and ignore,783,790,[0],0,[0],0,[0],0,0,0,0,releaseLockAndSwallow(),org.logstash.ackedqueue.Queue,releaseLockAndSwallow/0,False,783,2,4,2,2,2,3,8,0,0,0,3,0,0,0,0,1,0,1,0,0,0,1,0,0,0,10,2,1,False
152,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,Page nextReadPage(),"/**
 * return the {@link Page} for the next read operation.
 * @return {@link Page} will be either a read-only tail page or the head page.
 */
public Page nextReadPage() {
    lock.lock();
    try {
        // look at head page if no unreadTailPages
        return (this.unreadTailPages.isEmpty()) ? this.headPage : this.unreadTailPages.get(0);
    } finally {
        lock.unlock();
    }
}","/**
 * return the {@link Page} for the next read operation.
 * @return {@link Page} will be either a read-only tail page or the head page.
 */
","// look at head page if no unreadTailPages
",/** * return the {@link Page} for the next read operation. * @return {@link Page} will be either a read-only tail page or the head page. */[[SEP]]// look at head page if no unreadTailPages,796,804,[0],0,[0],0,"[0, 0]",0,0,0,0,nextReadPage(),org.logstash.ackedqueue.Queue,nextReadPage/0,False,796,1,2,2,0,2,4,9,1,0,0,4,0,0,0,0,1,1,0,1,0,0,1,0,0,0,14,1,0,True
153,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,void removeUnreadPage(Page),"private void removeUnreadPage(Page p) {
    if (!this.unreadTailPages.isEmpty()) {
        Page firstUnread = this.unreadTailPages.get(0);
        assert p.pageNum <= firstUnread.pageNum : String.format(""fully read pageNum=%d is greater than first unread pageNum=%d"", p.pageNum, firstUnread.pageNum);
        if (firstUnread == p) {
            // it is possible that when starting to read from a head page which is beheaded will not be inserted in the unreadTailPages list
            this.unreadTailPages.remove(0);
        }
    }
}", ,"// it is possible that when starting to read from a head page which is beheaded will not be inserted in the unreadTailPages list
",// it is possible that when starting to read from a head page which is beheaded will not be inserted in the unreadTailPages list,806,815,[0],0,[0],0,[0],0,0,0,0,removeUnreadPage(Page),org.logstash.ackedqueue.Queue,removeUnreadPage/1[org.logstash.ackedqueue.Page],False,806,1,1,1,0,3,4,9,0,1,1,4,0,0,0,1,0,0,1,2,1,0,2,0,0,0,13,2,0,False
154,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,boolean isHeadPage(Page),"/**
 * @param p the {@link Page} to verify if it is the head page
 * @return true if the given {@link Page} is the head page
 */
private boolean isHeadPage(Page p) {
    return p == this.headPage;
}","/**
 * @param p the {@link Page} to verify if it is the head page
 * @return true if the given {@link Page} is the head page
 */
", ,/** * @param p the {@link Page} to verify if it is the head page * @return true if the given {@link Page} is the head page */,860,862,[0],0,[0],0,[0],0,0,0,0,isHeadPage(Page),org.logstash.ackedqueue.Queue,isHeadPage/1[org.logstash.ackedqueue.Page],False,860,1,3,3,0,2,0,3,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,12,2,0,True
155,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\Queue.java,org.logstash.ackedqueue.Queue,boolean isTailPage(Page),"/**
 * @param p the {@link Page} to verify if it is a tail page
 * @return true if the given {@link Page} is a tail page
 */
private boolean isTailPage(Page p) {
    return !isHeadPage(p);
}","/**
 * @param p the {@link Page} to verify if it is a tail page
 * @return true if the given {@link Page} is a tail page
 */
", ,/** * @param p the {@link Page} to verify if it is a tail page * @return true if the given {@link Page} is a tail page */,868,870,[0],0,[0],0,[0],0,0,0,0,isTailPage(Page),org.logstash.ackedqueue.Queue,isTailPage/1[org.logstash.ackedqueue.Page],False,868,2,2,1,1,1,1,3,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,14,2,0,True
156,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\QueueFactoryExt.java,org.logstash.ackedqueue.QueueFactoryExt,"AbstractWrappedQueueExt create(ThreadContext, IRubyObject, IRubyObject)","@JRubyMethod(meta = true)
public static AbstractWrappedQueueExt create(final ThreadContext context, final IRubyObject recv, final IRubyObject settings) throws IOException {
    final String type = getSetting(context, settings, ""queue.type"").asJavaString();
    if (""persisted"".equals(type)) {
        final Path queuePath = Paths.get(getSetting(context, settings, ""path.queue"").asJavaString(), getSetting(context, settings, ""pipeline.id"").asJavaString());
        // Files.createDirectories raises a FileAlreadyExistsException
        // if pipeline queue path is a symlink, so worth checking against Files.exists
        if (Files.exists(queuePath) == false) {
            Files.createDirectories(queuePath);
        }
        return new JRubyWrappedAckedQueueExt(context.runtime, RubyUtil.WRAPPED_ACKED_QUEUE_CLASS).initialize(context, new IRubyObject[] { context.runtime.newString(queuePath.toString()), getSetting(context, settings, ""queue.page_capacity""), getSetting(context, settings, ""queue.max_events""), getSetting(context, settings, ""queue.checkpoint.writes""), getSetting(context, settings, ""queue.checkpoint.acks""), getSetting(context, settings, ""queue.checkpoint.interval""), getSetting(context, settings, ""queue.checkpoint.retry""), getSetting(context, settings, ""queue.max_bytes"") });
    } else if (""memory"".equals(type)) {
        return new JrubyWrappedSynchronousQueueExt(context.runtime, RubyUtil.WRAPPED_SYNCHRONOUS_QUEUE_CLASS).initialize(context, context.runtime.newFixnum(getSetting(context, settings, ""pipeline.batch.size"").convertToInteger().getIntValue() * getSetting(context, settings, ""pipeline.workers"").convertToInteger().getIntValue()));
    } else {
        throw context.runtime.newRaiseException(RubyUtil.CONFIGURATION_ERROR_CLASS, String.format(""Invalid setting `%s` for `queue.type`, supported types are: 'memory' or 'persisted'"", type));
    }
}", ,"// Files.createDirectories raises a FileAlreadyExistsException
[[SEP]]// if pipeline queue path is a symlink, so worth checking against Files.exists
","// Files.createDirectories raises a FileAlreadyExistsException// if pipeline queue path is a symlink, so worth checking against Files.exists",51,100,[0],0,"[0, 0]",0,[0],0,0,0,0,"create(ThreadContext, IRubyObject, IRubyObject)",org.logstash.ackedqueue.QueueFactoryExt,"create/3[org.logstash.ackedqueue.ThreadContext,org.logstash.ackedqueue.IRubyObject,org.logstash.ackedqueue.IRubyObject]",False,53,7,6,1,5,4,15,16,2,2,3,15,1,1,0,1,0,0,15,0,2,1,2,0,0,0,29,9,0,False
157,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\ByteBufferCleaner.java,org.logstash.ackedqueue.io.ByteBufferCleaner,void clean(MappedByteBuffer),"/**
 * Forces garbage collection of given buffer.
 * @param buffer ByteBuffer to GC
 */
void clean(MappedByteBuffer buffer);","/**
 * Forces garbage collection of given buffer.
 * @param buffer ByteBuffer to GC
 */
", ,/** * Forces garbage collection of given buffer. * @param buffer ByteBuffer to GC */,35,35,[0],0,[0],0,[0],0,0,0,0,clean(MappedByteBuffer),org.logstash.ackedqueue.io.ByteBufferCleaner,clean/1[java.nio.MappedByteBuffer],False,31,0,2,2,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,13,0,0,True
158,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\CheckpointIO.java,org.logstash.ackedqueue.io.CheckpointIO,"Checkpoint write(String, int, int, long, long, int)","// @return Checkpoint the written checkpoint object
Checkpoint write(String fileName, int pageNum, int firstUnackedPageNum, long firstUnackedSeqNum, long minSeqNum, int elementCount) throws IOException;","// @return Checkpoint the written checkpoint object
", ,// @return Checkpoint the written checkpoint object,32,32,[0],0,[0],0,[0],0,0,0,0,"write(String, int, int, long, long, int)",org.logstash.ackedqueue.io.CheckpointIO,"write/6[java.lang.String,int,int,long,long,int]",False,32,1,2,2,0,1,0,1,0,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,0,0,False
159,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\CheckpointIO.java,org.logstash.ackedqueue.io.CheckpointIO,String headFileName(),"// @return the head page checkpoint file name
String headFileName();","// @return the head page checkpoint file name
", ,// @return the head page checkpoint file name,41,41,[0],0,[0],0,[0],0,0,0,0,headFileName(),org.logstash.ackedqueue.io.CheckpointIO,headFileName/0,False,41,0,4,4,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,0,False
160,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\CheckpointIO.java,org.logstash.ackedqueue.io.CheckpointIO,String tailFileName(int),"// @return the tail page checkpoint file name for given page number
String tailFileName(int pageNum);","// @return the tail page checkpoint file name for given page number
", ,// @return the tail page checkpoint file name for given page number,44,44,[0],0,[0],0,[0],0,0,0,0,tailFileName(int),org.logstash.ackedqueue.io.CheckpointIO,tailFileName/1[int],False,44,0,5,5,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,0,0,False
161,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\FileCheckpointIO.java,org.logstash.ackedqueue.io.FileCheckpointIO,"void write(String, Checkpoint)","@Override
public void write(String fileName, Checkpoint checkpoint) throws IOException {
    write(checkpoint, buffer);
    buffer.flip();
    final Path tmpPath = dirPath.resolve(fileName + "".tmp"");
    try (FileOutputStream out = new FileOutputStream(tmpPath.toFile())) {
        out.getChannel().write(buffer);
        out.getFD().sync();
    }
    // Windows can have problem doing file move See: https://github.com/elastic/logstash/issues/12345
    // retry a couple of times to make it works. The first two runs has no break. The rest of reties are exponential backoff.
    final Path path = dirPath.resolve(fileName);
    try {
        Files.move(tmpPath, path, StandardCopyOption.ATOMIC_MOVE);
    } catch (IOException ex) {
        if (retry) {
            try {
                logger.debug(""CheckpointIO retry moving '{}' to '{}'"", tmpPath, path);
                backoff.retryable(() -> Files.move(tmpPath, path, StandardCopyOption.ATOMIC_MOVE));
            } catch (ExponentialBackoff.RetryException re) {
                throw new IOException(""Error writing checkpoint"", re);
            }
        } else {
            logger.error(""Error writing checkpoint without retry: "" + ex);
            throw ex;
        }
    }
}", ,"// Windows can have problem doing file move See: https://github.com/elastic/logstash/issues/12345
[[SEP]]// retry a couple of times to make it works. The first two runs has no break. The rest of reties are exponential backoff.
",// Windows can have problem doing file move See: https://github.com/elastic/logstash/issues/12345// retry a couple of times to make it works. The first two runs has no break. The rest of reties are exponential backoff.,100,128,[0],0,"[0, 0]",0,[0],0,0,0,0,"write(String, Checkpoint)",org.logstash.ackedqueue.io.FileCheckpointIO,"write/2[java.lang.String,org.logstash.ackedqueue.Checkpoint]",False,101,4,5,1,4,4,12,28,0,3,2,12,1,1,0,0,3,0,4,0,3,2,3,0,0,1,22,1,2,False
162,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\FileCheckpointIO.java,org.logstash.ackedqueue.io.FileCheckpointIO,String headFileName(),"// @return the head page checkpoint file name
@Override
public String headFileName() {
    return HEAD_CHECKPOINT;
}","// @return the head page checkpoint file name
", ,// @return the head page checkpoint file name,138,141,[0],0,[0],0,[0],0,0,0,0,headFileName(),org.logstash.ackedqueue.io.FileCheckpointIO,headFileName/0,False,139,0,0,0,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,1,0,False
163,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\FileCheckpointIO.java,org.logstash.ackedqueue.io.FileCheckpointIO,String tailFileName(int),"// @return the tail page checkpoint file name for given page number
@Override
public String tailFileName(int pageNum) {
    return TAIL_CHECKPOINT + pageNum;
}","// @return the tail page checkpoint file name for given page number
", ,// @return the tail page checkpoint file name for given page number,144,147,[0],0,[0],0,[0],0,0,0,0,tailFileName(int),org.logstash.ackedqueue.io.FileCheckpointIO,tailFileName/1[int],False,145,0,0,0,0,1,0,3,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,17,1,0,False
164,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\FileCheckpointIO.java,org.logstash.ackedqueue.io.FileCheckpointIO,Checkpoint read(ByteBuffer),"public static Checkpoint read(ByteBuffer data) throws IOException {
    int version = (int) data.getShort();
    // TODO - build reader for this version
    int pageNum = data.getInt();
    int firstUnackedPageNum = data.getInt();
    long firstUnackedSeqNum = data.getLong();
    long minSeqNum = data.getLong();
    int elementCount = data.getInt();
    final CRC32 crc32 = new CRC32();
    crc32.update(data.array(), 0, BUFFER_SIZE - Integer.BYTES);
    int calcCrc32 = (int) crc32.getValue();
    int readCrc32 = data.getInt();
    if (readCrc32 != calcCrc32) {
        throw new IOException(String.format(""Checkpoint checksum mismatch, expected: %d, actual: %d"", calcCrc32, readCrc32));
    }
    if (version != Checkpoint.VERSION) {
        throw new IOException(""Unknown file format version: "" + version);
    }
    return new Checkpoint(pageNum, firstUnackedPageNum, firstUnackedSeqNum, minSeqNum, elementCount);
}", ,"// TODO - build reader for this version
",// TODO - build reader for this version,149,169,[0],0,[1],1,[1],1,1,1,1,read(ByteBuffer),org.logstash.ackedqueue.io.FileCheckpointIO,read/1[java.nio.ByteBuffer],False,149,1,3,2,1,3,7,19,1,9,1,7,0,0,0,2,0,0,2,1,9,2,1,0,0,0,27,9,0,False
165,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\IntVector.java,org.logstash.ackedqueue.io.IntVector,void add(int),"/**
 * Store the {@code int} to the underlying {@code int[]}, resizing it if necessary.
 * @param num Int to store
 */
public void add(final int num) {
    if (data.length < count + 1) {
        final int[] old = data;
        data = new int[data.length << 1];
        System.arraycopy(old, 0, data, 0, old.length);
    }
    data[count++] = num;
}","/**
 * Store the {@code int} to the underlying {@code int[]}, resizing it if necessary.
 * @param num Int to store
 */
", ,"/** * Store the {@code int} to the underlying {@code int[]}, resizing it if necessary. * @param num Int to store */",38,45,[0],0,[0],0,[0],0,0,0,0,add(int),org.logstash.ackedqueue.io.IntVector,add/1[int],False,38,1,4,4,0,2,1,8,0,1,1,1,0,0,0,0,0,0,0,4,3,2,1,0,0,0,13,1,0,True
166,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\IntVector.java,org.logstash.ackedqueue.io.IntVector,int get(int),"/**
 * Get value stored at given index.
 * @param index Array index (only values < {@link IntVector#count} are valid)
 * @return Int
 */
public int get(final int index) {
    return data[index];
}","/**
 * Get value stored at given index.
 * @param index Array index (only values < {@link IntVector#count} are valid)
 * @return Int
 */
", ,/** * Get value stored at given index. * @param index Array index (only values < {@link IntVector#count} are valid) * @return Int */,52,54,[0],0,[0],0,[0],0,0,0,0,get(int),org.logstash.ackedqueue.io.IntVector,get/1[int],False,52,0,3,3,0,1,0,3,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,13,1,0,True
167,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\IntVector.java,org.logstash.ackedqueue.io.IntVector,int size(),"/**
 * @return Number of elements stored in this instance
 */
public int size() {
    return count;
}","/**
 * @return Number of elements stored in this instance
 */
", ,/** * @return Number of elements stored in this instance */,59,61,[0],0,[0],0,[0],0,0,0,0,size(),org.logstash.ackedqueue.io.IntVector,size/0,False,59,0,2,2,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,1,0,True
168,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\LongVector.java,org.logstash.ackedqueue.io.LongVector,void add(long),"/**
 * Store the {@code long} to the underlying {@code long[]}, resizing it if necessary.
 * @param num Long to store
 */
public void add(final long num) {
    if (data.length < count + 1) {
        final long[] old = data;
        data = new long[(data.length << 1) + 1];
        System.arraycopy(old, 0, data, 0, old.length);
    }
    data[count++] = num;
}","/**
 * Store the {@code long} to the underlying {@code long[]}, resizing it if necessary.
 * @param num Long to store
 */
", ,"/** * Store the {@code long} to the underlying {@code long[]}, resizing it if necessary. * @param num Long to store */",44,51,[0],0,[0],0,[0],0,0,0,0,add(long),org.logstash.ackedqueue.io.LongVector,add/1[long],False,44,1,4,4,0,2,1,8,0,1,1,1,0,0,0,0,0,1,0,5,3,3,1,0,0,0,13,1,0,True
169,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\LongVector.java,org.logstash.ackedqueue.io.LongVector,void add(LongVector),"/**
 * Store the {@code long[]} to the underlying {@code long[]}, resizing it if necessary.
 * @param nums {@code long[]} to store
 */
public void add(final LongVector nums) {
    if (data.length < count + nums.size()) {
        final long[] old = data;
        data = new long[(data.length << 1) + nums.size()];
        System.arraycopy(old, 0, data, 0, old.length);
    }
    for (int i = 0; i < nums.size(); i++) {
        data[count + i] = nums.get(i);
    }
    count += nums.size();
}","/**
 * Store the {@code long[]} to the underlying {@code long[]}, resizing it if necessary.
 * @param nums {@code long[]} to store
 */
", ,"/** * Store the {@code long[]} to the underlying {@code long[]}, resizing it if necessary. * @param nums {@code long[]} to store */",57,67,[0],0,[0],0,[0],0,0,0,0,add(LongVector),org.logstash.ackedqueue.io.LongVector,add/1[org.logstash.ackedqueue.io.LongVector],False,57,2,3,1,2,3,3,11,0,2,1,3,2,1,1,0,0,1,0,4,5,4,1,0,0,0,15,1,0,True
170,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\LongVector.java,org.logstash.ackedqueue.io.LongVector,long get(int),"/**
 * Get value stored at given index.
 * @param index Array index (only values smaller than {@link LongVector#count} are valid)
 * @return Int
 */
public long get(final int index) {
    return data[index];
}","/**
 * Get value stored at given index.
 * @param index Array index (only values smaller than {@link LongVector#count} are valid)
 * @return Int
 */
", ,/** * Get value stored at given index. * @param index Array index (only values smaller than {@link LongVector#count} are valid) * @return Int */,74,76,[0],0,[0],0,[0],0,0,0,0,get(int),org.logstash.ackedqueue.io.LongVector,get/1[int],False,74,0,6,6,0,1,0,3,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,1,0,True
171,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\LongVector.java,org.logstash.ackedqueue.io.LongVector,int size(),"/**
 * @return Number of elements stored in this instance
 */
public int size() {
    return count;
}","/**
 * @return Number of elements stored in this instance
 */
", ,/** * @return Number of elements stored in this instance */,81,83,[0],0,[0],0,[0],0,0,0,0,size(),org.logstash.ackedqueue.io.LongVector,size/0,False,81,0,4,4,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,1,0,True
172,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV1.java,org.logstash.ackedqueue.io.MmapPageIOV1,"void open(long, int)","@Override
public void open(long minSeqNum, int elementCount) throws IOException {
    mapFile();
    buffer.position(0);
    this.version = buffer.get();
    validateVersion(this.version);
    this.head = 1;
    this.minSeqNum = minSeqNum;
    this.elementCount = elementCount;
    if (this.elementCount > 0) {
        // verify first seqNum to be same as expected minSeqNum
        long seqNum = buffer.getLong();
        if (seqNum != this.minSeqNum) {
            throw new IOException(String.format(""first seqNum=%d is different than minSeqNum=%d"", seqNum, this.minSeqNum));
        }
        // reset back position to first seqNum
        buffer.position(this.head);
        for (int i = 0; i < this.elementCount; i++) {
            // verify that seqNum must be of strict + 1 increasing order
            readNextElement(this.minSeqNum + i, !MmapPageIOV2.VERIFY_CHECKSUM);
        }
    }
}", ,"// verify first seqNum to be same as expected minSeqNum
[[SEP]]// reset back position to first seqNum
[[SEP]]// verify that seqNum must be of strict + 1 increasing order
",// verify first seqNum to be same as expected minSeqNum[[SEP]]// reset back position to first seqNum[[SEP]]// verify that seqNum must be of strict + 1 increasing order,73,99,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,"open(long, int)",org.logstash.ackedqueue.io.MmapPageIOV1,"open/2[long,int]",False,74,1,4,1,3,4,7,19,0,2,2,7,3,1,1,1,0,0,1,4,6,1,2,0,0,0,21,1,0,False
173,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV1.java,org.logstash.ackedqueue.io.MmapPageIOV1,void deactivate(),"@Override
public void deactivate() {
    // close can be called multiple times
    close();
}", ,"// close can be called multiple times
",// close can be called multiple times,150,153,[0],0,[0],0,[0],0,0,0,0,deactivate(),org.logstash.ackedqueue.io.MmapPageIOV1,deactivate/0,False,151,1,1,0,1,1,1,3,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,2,1,0,False
174,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV1.java,org.logstash.ackedqueue.io.MmapPageIOV1,void mapFile(),"// memory map data file to this.buffer and read initial version byte
private void mapFile() throws IOException {
    try (RandomAccessFile raf = new RandomAccessFile(this.file, ""rw"")) {
        if (raf.length() > Integer.MAX_VALUE) {
            throw new IOException(""Page file too large "" + this.file);
        }
        int pageFileCapacity = (int) raf.length();
        // update capacity to actual raf length. this can happen if a page size was changed on a non empty queue directory for example.
        this.capacity = pageFileCapacity;
        if (this.capacity < MmapPageIOV2.MIN_CAPACITY) {
            throw new IOException(String.format(""Page file size is too small to hold elements""));
        }
        this.buffer = raf.getChannel().map(FileChannel.MapMode.READ_ONLY, 0, this.capacity);
    }
    this.buffer.load();
}","// memory map data file to this.buffer and read initial version byte
","// update capacity to actual raf length. this can happen if a page size was changed on a non empty queue directory for example.
",// memory map data file to this.buffer and read initial version byte[[SEP]]// update capacity to actual raf length. this can happen if a page size was changed on a non empty queue directory for example.,240,257,[0],0,[0],0,"[0, 0]",0,0,0,0,mapFile(),org.logstash.ackedqueue.io.MmapPageIOV1,mapFile/0,False,240,0,1,1,0,3,5,14,0,2,0,5,0,0,0,0,1,0,3,1,4,1,2,0,0,0,18,2,0,False
175,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV1.java,org.logstash.ackedqueue.io.MmapPageIOV1,"void readNextElement(long, boolean)","// read and validate next element at page head
// @param verifyChecksum if true the actual element data will be read + checksumed and compared to written checksum
private void readNextElement(long expectedSeqNum, boolean verifyChecksum) throws MmapPageIOV2.PageIOInvalidElementException {
    // if there is no room for the seqNum and length bytes stop here
    if (this.head + MmapPageIOV2.SEQNUM_SIZE + MmapPageIOV2.LENGTH_SIZE > capacity) {
        throw new MmapPageIOV2.PageIOInvalidElementException(""cannot read seqNum and length bytes past buffer capacity"");
    }
    int elementOffset = this.head;
    int newHead = this.head;
    long seqNum = buffer.getLong();
    newHead += MmapPageIOV2.SEQNUM_SIZE;
    if (seqNum != expectedSeqNum) {
        throw new MmapPageIOV2.PageIOInvalidElementException(String.format(""Element seqNum %d is expected to be %d"", seqNum, expectedSeqNum));
    }
    int length = buffer.getInt();
    newHead += MmapPageIOV2.LENGTH_SIZE;
    // length must be > 0
    if (length <= 0) {
        throw new MmapPageIOV2.PageIOInvalidElementException(""Element invalid length"");
    }
    // if there is no room for the proposed data length and checksum just stop here
    if (newHead + length + MmapPageIOV2.CHECKSUM_SIZE > capacity) {
        throw new MmapPageIOV2.PageIOInvalidElementException(""cannot read element payload and checksum past buffer capacity"");
    }
    if (verifyChecksum) {
        // read data and compute checksum;
        this.checkSummer.reset();
        final int prevLimit = buffer.limit();
        buffer.limit(buffer.position() + length);
        this.checkSummer.update(buffer);
        buffer.limit(prevLimit);
        int checksum = buffer.getInt();
        int computedChecksum = (int) this.checkSummer.getValue();
        if (computedChecksum != checksum) {
            throw new MmapPageIOV2.PageIOInvalidElementException(""Element invalid checksum"");
        }
    }
    // at this point we recovered a valid element
    this.offsetMap.add(elementOffset);
    this.head = newHead + length + MmapPageIOV2.CHECKSUM_SIZE;
    buffer.position(this.head);
}","// @param verifyChecksum if true the actual element data will be read + checksumed and compared to written checksum
","// if there is no room for the seqNum and length bytes stop here
[[SEP]]// length must be > 0
[[SEP]]// if there is no room for the proposed data length and checksum just stop here
[[SEP]]// read data and compute checksum;
[[SEP]]// at this point we recovered a valid element
",// read and validate next element at page head// @param verifyChecksum if true the actual element data will be read + checksumed and compared to written checksum[[SEP]]// if there is no room for the seqNum and length bytes stop here[[SEP]]// length must be > 0[[SEP]]// if there is no room for the proposed data length and checksum just stop here[[SEP]]// read data and compute checksum;[[SEP]]// at this point we recovered a valid element,261,313,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0]",0,0,0,0,"readNextElement(long, boolean)",org.logstash.ackedqueue.io.MmapPageIOV1,"readNextElement/2[long,boolean]",False,261,2,3,1,2,7,11,35,0,7,2,11,0,0,0,2,0,0,5,1,10,4,2,0,0,0,28,2,0,False
176,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV1.java,org.logstash.ackedqueue.io.MmapPageIOV1,void validateVersion(byte),"// we don't have different versions yet so simply check if the version is VERSION_ONE for basic integrity check
// and if an unexpected version byte is read throw PageIOInvalidVersionException
private static void validateVersion(byte version) throws MmapPageIOV2.PageIOInvalidVersionException {
    if (version != VERSION_ONE) {
        throw new MmapPageIOV2.PageIOInvalidVersionException(String.format(""Expected page version=%d but found version=%d"", VERSION_ONE, version));
    }
}","// and if an unexpected version byte is read throw PageIOInvalidVersionException
", ,// we don't have different versions yet so simply check if the version is VERSION_ONE for basic integrity check// and if an unexpected version byte is read throw PageIOInvalidVersionException,317,323,[0],0,[0],0,[0],0,0,0,0,validateVersion(byte),org.logstash.ackedqueue.io.MmapPageIOV1,validateVersion/1[byte],False,318,1,2,1,1,2,1,5,0,0,1,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,14,10,0,False
177,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV2.java,org.logstash.ackedqueue.io.MmapPageIOV2,"void open(long, int)","@Override
public void open(long minSeqNum, int elementCount) throws IOException {
    mapFile();
    buffer.position(0);
    this.version = buffer.get();
    validateVersion(this.version);
    this.head = 1;
    this.minSeqNum = minSeqNum;
    this.elementCount = elementCount;
    if (this.elementCount > 0) {
        // verify first seqNum to be same as expected minSeqNum
        long seqNum = buffer.getLong();
        if (seqNum != this.minSeqNum) {
            throw new IOException(String.format(""first seqNum=%d is different than minSeqNum=%d"", seqNum, this.minSeqNum));
        }
        // reset back position to first seqNum
        buffer.position(this.head);
        for (int i = 0; i < this.elementCount; i++) {
            // verify that seqNum must be of strict + 1 increasing order
            readNextElement(this.minSeqNum + i, !VERIFY_CHECKSUM);
        }
    }
}", ,"// verify first seqNum to be same as expected minSeqNum
[[SEP]]// reset back position to first seqNum
[[SEP]]// verify that seqNum must be of strict + 1 increasing order
",// verify first seqNum to be same as expected minSeqNum[[SEP]]// reset back position to first seqNum[[SEP]]// verify that seqNum must be of strict + 1 increasing order,83,109,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,"open(long, int)",org.logstash.ackedqueue.io.MmapPageIOV2,"open/2[long,int]",False,84,1,4,1,3,4,7,19,0,2,2,7,3,1,1,1,0,0,1,4,6,1,2,0,0,0,33,1,0,False
178,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV2.java,org.logstash.ackedqueue.io.MmapPageIOV2,void recover(),"// recover will overwrite/update/set this object minSeqNum, capacity and elementCount attributes
// to reflect what it recovered from the page
@Override
public void recover() throws IOException {
    mapFile();
    buffer.position(0);
    this.version = buffer.get();
    validateVersion(this.version);
    this.head = 1;
    // force minSeqNum to actual first element seqNum
    this.minSeqNum = buffer.getLong();
    // reset back position to first seqNum
    buffer.position(this.head);
    // reset elementCount to 0 and increment to octal number of valid elements found
    this.elementCount = 0;
    for (int i = 0; ; i++) {
        try {
            // verify that seqNum must be of strict + 1 increasing order
            readNextElement(this.minSeqNum + i, VERIFY_CHECKSUM);
            this.elementCount += 1;
        } catch (MmapPageIOV2.PageIOInvalidElementException e) {
            // simply stop at first invalid element
            LOGGER.debug(""PageIO recovery for '{}' element index:{}, readNextElement exception: {}"", file, i, e.getMessage());
            break;
        }
    }
    // if we were not able to read any element just reset minSeqNum to zero
    if (this.elementCount <= 0) {
        this.minSeqNum = 0;
    }
}","// to reflect what it recovered from the page
","// force minSeqNum to actual first element seqNum
[[SEP]]// reset back position to first seqNum
[[SEP]]// reset elementCount to 0 and increment to octal number of valid elements found
[[SEP]]// verify that seqNum must be of strict + 1 increasing order
[[SEP]]// simply stop at first invalid element
[[SEP]]// if we were not able to read any element just reset minSeqNum to zero
","// recover will overwrite/update/set this object minSeqNum, capacity and elementCount attributes// to reflect what it recovered from the page[[SEP]]// force minSeqNum to actual first element seqNum[[SEP]]// reset back position to first seqNum[[SEP]]// reset elementCount to 0 and increment to octal number of valid elements found[[SEP]]// verify that seqNum must be of strict + 1 increasing order[[SEP]]// simply stop at first invalid element[[SEP]]// if we were not able to read any element just reset minSeqNum to zero",153,185,[0],0,"[0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0, 0]",0,0,0,0,recover(),org.logstash.ackedqueue.io.MmapPageIOV2,recover/0,False,154,2,4,0,4,4,8,23,0,1,0,8,3,1,1,0,1,0,1,7,7,1,2,0,0,0,15,1,1,False
179,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV2.java,org.logstash.ackedqueue.io.MmapPageIOV2,void deactivate(),"@Override
public void deactivate() {
    // close can be called multiple times
    close();
}", ,"// close can be called multiple times
",// close can be called multiple times,200,203,[0],0,[0],0,[0],0,0,0,0,deactivate(),org.logstash.ackedqueue.io.MmapPageIOV2,deactivate/0,False,201,1,1,0,1,1,1,3,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,2,1,0,False
180,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV2.java,org.logstash.ackedqueue.io.MmapPageIOV2,void activate(),"@Override
public void activate() throws IOException {
    if (this.buffer == null) {
        try (RandomAccessFile raf = new RandomAccessFile(this.file, ""rw"")) {
            this.buffer = raf.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
        }
        this.buffer.load();
    }
    // TODO: do we need to check is the channel is still open? not sure how it could be closed
}", ,"// TODO: do we need to check is the channel is still open? not sure how it could be closed
",// TODO: do we need to check is the channel is still open? not sure how it could be closed,205,214,[0],0,[1],1,[1],1,1,1,1,activate(),org.logstash.ackedqueue.io.MmapPageIOV2,activate/0,False,206,0,0,0,0,2,3,8,0,1,0,3,0,0,0,1,1,0,1,1,2,0,2,0,0,0,9,1,0,False
181,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV2.java,org.logstash.ackedqueue.io.MmapPageIOV2,void mapFile(),"// memory map data file to this.buffer and read initial version byte
private void mapFile() throws IOException {
    try (RandomAccessFile raf = new RandomAccessFile(this.file, ""rw"")) {
        if (raf.length() > Integer.MAX_VALUE) {
            throw new IOException(""Page file too large "" + this.file);
        }
        int pageFileCapacity = (int) raf.length();
        // update capacity to actual raf length. this can happen if a page size was changed on a non empty queue directory for example.
        this.capacity = pageFileCapacity;
        if (this.capacity < MIN_CAPACITY) {
            throw new IOException(""Page file size is too small to hold elements. "" + ""This is potentially a queue corruption problem. Run `pqcheck` and `pqrepair` to repair the queue."");
        }
        this.buffer = raf.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
    }
    this.buffer.load();
}","// memory map data file to this.buffer and read initial version byte
","// update capacity to actual raf length. this can happen if a page size was changed on a non empty queue directory for example.
",// memory map data file to this.buffer and read initial version byte[[SEP]]// update capacity to actual raf length. this can happen if a page size was changed on a non empty queue directory for example.,293,311,[0],0,[0],0,"[0, 0]",0,0,0,0,mapFile(),org.logstash.ackedqueue.io.MmapPageIOV2,mapFile/0,False,293,0,2,2,0,3,4,14,0,2,0,4,0,0,0,0,1,0,4,1,4,2,2,0,0,0,34,2,0,False
182,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV2.java,org.logstash.ackedqueue.io.MmapPageIOV2,"void readNextElement(long, boolean)","// read and validate next element at page head
// @param verifyChecksum if true the actual element data will be read + checksumed and compared to written checksum
private void readNextElement(long expectedSeqNum, boolean verifyChecksum) throws MmapPageIOV2.PageIOInvalidElementException {
    // if there is no room for the seqNum and length bytes stop here
    // TODO: I know this isn't a great exception message but at the time of writing I couldn't come up with anything better :P
    if (this.head + SEQNUM_SIZE + LENGTH_SIZE > capacity) {
        throw new MmapPageIOV2.PageIOInvalidElementException(""cannot read seqNum and length bytes past buffer capacity"");
    }
    int elementOffset = this.head;
    int newHead = this.head;
    long seqNum = buffer.getLong();
    newHead += SEQNUM_SIZE;
    if (seqNum != expectedSeqNum) {
        throw new MmapPageIOV2.PageIOInvalidElementException(String.format(""Element seqNum %d is expected to be %d"", seqNum, expectedSeqNum));
    }
    int length = buffer.getInt();
    newHead += LENGTH_SIZE;
    // length must be > 0
    if (length <= 0) {
        throw new MmapPageIOV2.PageIOInvalidElementException(""Element invalid length"");
    }
    // if there is no room for the proposed data length and checksum just stop here
    if (newHead + length + CHECKSUM_SIZE > capacity) {
        throw new MmapPageIOV2.PageIOInvalidElementException(""cannot read element payload and checksum past buffer capacity"");
    }
    if (verifyChecksum) {
        // read data and compute checksum;
        this.checkSummer.reset();
        final int prevLimit = buffer.limit();
        buffer.limit(buffer.position() + length);
        this.checkSummer.update(buffer);
        buffer.limit(prevLimit);
        int checksum = buffer.getInt();
        int computedChecksum = (int) this.checkSummer.getValue();
        if (computedChecksum != checksum) {
            throw new MmapPageIOV2.PageIOInvalidElementException(""Element invalid checksum"");
        }
    }
    // at this point we recovered a valid element
    this.offsetMap.add(elementOffset);
    this.head = newHead + length + CHECKSUM_SIZE;
    buffer.position(this.head);
}","// @param verifyChecksum if true the actual element data will be read + checksumed and compared to written checksum
","// if there is no room for the seqNum and length bytes stop here
[[SEP]]// TODO: I know this isn't a great exception message but at the time of writing I couldn't come up with anything better :P
[[SEP]]// length must be > 0
[[SEP]]// if there is no room for the proposed data length and checksum just stop here
[[SEP]]// read data and compute checksum;
[[SEP]]// at this point we recovered a valid element
",// read and validate next element at page head// @param verifyChecksum if true the actual element data will be read + checksumed and compared to written checksum[[SEP]]// if there is no room for the seqNum and length bytes stop here// TODO: I know this isn't a great exception message but at the time of writing I couldn't come up with anything better :P[[SEP]]// length must be > 0[[SEP]]// if there is no room for the proposed data length and checksum just stop here[[SEP]]// read data and compute checksum;[[SEP]]// at this point we recovered a valid element,315,367,[0],0,"[0, 1, 0, 0, 0, 0]",1,"[0, 1, 0, 0, 0, 0]",1,1,1,1,"readNextElement(long, boolean)",org.logstash.ackedqueue.io.MmapPageIOV2,"readNextElement/2[long,boolean]",False,315,2,4,2,2,7,11,35,0,7,2,11,0,0,0,2,0,0,5,1,10,4,2,0,0,0,43,2,0,False
183,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV2.java,org.logstash.ackedqueue.io.MmapPageIOV2,"int write(byte[], long, int, int)","private int write(byte[] bytes, long seqNum, int length, int checksum) {
    // since writes always happen at head, we can just append head to the offsetMap
    assert this.offsetMap.size() == this.elementCount : String.format(""offsetMap size=%d != elementCount=%d"", this.offsetMap.size(), this.elementCount);
    int initialHead = this.head;
    buffer.position(this.head);
    buffer.putLong(seqNum);
    buffer.putInt(length);
    buffer.put(bytes);
    buffer.putInt(checksum);
    this.head += persistedByteCount(bytes.length);
    assert this.head == buffer.position() : String.format(""head=%d != buffer position=%d"", this.head, buffer.position());
    if (this.elementCount <= 0) {
        this.minSeqNum = seqNum;
    }
    this.offsetMap.add(initialHead);
    this.elementCount++;
    return initialHead;
}", ,"// since writes always happen at head, we can just append head to the offsetMap
","// since writes always happen at head, we can just append head to the offsetMap",369,392,[0],0,[0],0,[0],0,0,0,0,"write(byte[], long, int, int)",org.logstash.ackedqueue.io.MmapPageIOV2,"write/4[byte[],long,int,int]",False,369,3,4,1,3,4,9,17,1,1,4,9,1,1,0,2,0,0,2,1,3,0,1,0,0,0,16,2,0,False
184,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\MmapPageIOV2.java,org.logstash.ackedqueue.io.MmapPageIOV2,void validateVersion(byte),"// we don't have different versions yet so simply check if the version is VERSION_ONE for basic integrity check
// and if an unexpected version byte is read throw PageIOInvalidVersionException
private static void validateVersion(byte version) throws MmapPageIOV2.PageIOInvalidVersionException {
    if (version != VERSION_TWO) {
        throw new MmapPageIOV2.PageIOInvalidVersionException(String.format(""Expected page version=%d but found version=%d"", VERSION_TWO, version));
    }
}","// and if an unexpected version byte is read throw PageIOInvalidVersionException
", ,// we don't have different versions yet so simply check if the version is VERSION_ONE for basic integrity check// and if an unexpected version byte is read throw PageIOInvalidVersionException,396,402,[0],0,[0],0,[0],0,0,0,0,validateVersion(byte),org.logstash.ackedqueue.io.MmapPageIOV2,validateVersion/1[byte],False,397,1,3,2,1,2,1,5,0,0,1,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,16,10,0,False
185,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,"void open(long, int)","// the concrete class should be constructed with the pageNum, capacity and dirPath attributes
// and open/recover/create must first be called to setup with physical data file
// 
// TODO: we should probably refactor this with a factory to force the creation of a fully
// initialized concrete object with either open/recover/create instead of allowing
// a partially initialized object using the concrete class constructor. Not sure.
// open an existing data container and reconstruct internal state if required
void open(long minSeqNum, int elementCount) throws IOException;","// open an existing data container and reconstruct internal state if required
", ,"// the concrete class should be constructed with the pageNum, capacity and dirPath attributes// and open/recover/create must first be called to setup with physical data file//// TODO: we should probably refactor this with a factory to force the creation of a fully// initialized concrete object with either open/recover/create instead of allowing// a partially initialized object using the concrete class constructor. Not sure.// open an existing data container and reconstruct internal state if required",41,41,[0],0,[0],0,[1],1,1,1,1,"open(long, int)",org.logstash.ackedqueue.io.PageIO,"open/2[long,int]",False,41,0,2,2,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,False
186,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,void recover(),"// optimistically recover an existing data container and reconstruct internal state
// with the actual read/recovered data. this is only useful when reading a head page
// data file since tail pages are read-only.
void recover() throws IOException;","// data file since tail pages are read-only.
", ,// optimistically recover an existing data container and reconstruct internal state// with the actual read/recovered data. this is only useful when reading a head page// data file since tail pages are read-only.,46,46,[0],0,[0],0,[0],0,0,0,0,recover(),org.logstash.ackedqueue.io.PageIO,recover/0,False,46,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,False
187,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,void create(),"// create a new empty data file
void create() throws IOException;","// create a new empty data file
", ,// create a new empty data file,49,49,[0],0,[0],0,[0],0,0,0,0,create(),org.logstash.ackedqueue.io.PageIO,create/0,False,49,0,3,3,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,False
188,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,boolean hasSpace(int),"// verify if the data container has space for the given number of bytes
boolean hasSpace(int bytes);","// verify if the data container has space for the given number of bytes
", ,// verify if the data container has space for the given number of bytes,52,52,[0],0,[0],0,[0],0,0,0,0,hasSpace(int),org.logstash.ackedqueue.io.PageIO,hasSpace/1[int],False,52,0,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,0,False
189,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,"void write(byte[], long)","// write the given bytes to the data container
void write(byte[] bytes, long seqNum) throws IOException;","// write the given bytes to the data container
", ,// write the given bytes to the data container,55,55,[0],0,[0],0,[0],0,0,0,0,"write(byte[], long)",org.logstash.ackedqueue.io.PageIO,"write/2[byte[],long]",False,55,1,1,1,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,False
190,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,"SequencedList<byte[]> read(long, int)","// read up to limit number of items starting at give seqNum
SequencedList<byte[]> read(long seqNum, int limit) throws IOException;","// read up to limit number of items starting at give seqNum
", ,// read up to limit number of items starting at give seqNum,58,58,[0],0,[0],0,[0],0,0,0,0,"read(long, int)",org.logstash.ackedqueue.io.PageIO,"read/2[long,int]",False,58,2,2,2,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,0,0,False
191,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,int getCapacity(),"// @return the data container total capacity in bytes
int getCapacity();","// @return the data container total capacity in bytes
", ,// @return the data container total capacity in bytes,61,61,[0],0,[0],0,[0],0,0,0,0,getCapacity(),org.logstash.ackedqueue.io.PageIO,getCapacity/0,False,61,0,4,4,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,False
192,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,int getHead(),"// @return the current head offset within the page
int getHead();","// @return the current head offset within the page
", ,// @return the current head offset within the page,64,64,[0],0,[0],0,[0],0,0,0,0,getHead(),org.logstash.ackedqueue.io.PageIO,getHead/0,False,64,0,3,3,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,False
193,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,int persistedByteCount(int),"// @return the actual persisted byte count (with overhead) for the given data bytes
int persistedByteCount(int bytes);","// @return the actual persisted byte count (with overhead) for the given data bytes
", ,// @return the actual persisted byte count (with overhead) for the given data bytes,67,67,[0],0,[0],0,[0],0,0,0,0,persistedByteCount(int),org.logstash.ackedqueue.io.PageIO,persistedByteCount/1[int],False,67,0,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,False
194,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,void deactivate(),"// signal that this data page is not active and resources can be released
void deactivate() throws IOException;","// signal that this data page is not active and resources can be released
", ,// signal that this data page is not active and resources can be released,70,70,[0],0,[0],0,[0],0,0,0,0,deactivate(),org.logstash.ackedqueue.io.PageIO,deactivate/0,False,70,0,2,2,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,False
195,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,void activate(),"// signal that this data page is active will be read or written to
// should do nothing if page is aready active
void activate() throws IOException;","// should do nothing if page is aready active
", ,// signal that this data page is active will be read or written to// should do nothing if page is aready active,74,74,[0],0,[0],0,[0],0,0,0,0,activate(),org.logstash.ackedqueue.io.PageIO,activate/0,False,74,0,2,2,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,False
196,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,void ensurePersisted(),"// issue the proper data container ""fsync"" sematic
void ensurePersisted();","// issue the proper data container ""fsync"" sematic
", ,"// issue the proper data container ""fsync"" sematic",77,77,[0],0,[0],0,[0],0,0,0,0,ensurePersisted(),org.logstash.ackedqueue.io.PageIO,ensurePersisted/0,False,77,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,False
197,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,void purge(),"// delete/unlink/remove data file
void purge() throws IOException;","// delete/unlink/remove data file
", ,// delete/unlink/remove data file,80,80,[0],0,[0],0,[0],0,0,0,0,purge(),org.logstash.ackedqueue.io.PageIO,purge/0,False,80,0,3,3,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,False
198,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,int getElementCount(),"// @return the data container elements count
int getElementCount();","// @return the data container elements count
", ,// @return the data container elements count,83,83,[0],0,[0],0,[0],0,0,0,0,getElementCount(),org.logstash.ackedqueue.io.PageIO,getElementCount/0,False,83,0,2,2,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,0,False
199,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,long getMinSeqNum(),"// @return the data container min sequence number
long getMinSeqNum();","// @return the data container min sequence number
", ,// @return the data container min sequence number,86,86,[0],0,[0],0,[0],0,0,0,0,getMinSeqNum(),org.logstash.ackedqueue.io.PageIO,getMinSeqNum/0,False,86,0,2,2,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,False
200,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ackedqueue\io\PageIO.java,org.logstash.ackedqueue.io.PageIO,boolean isCorruptedPage(),"// check if the page size is < minimum size
boolean isCorruptedPage() throws IOException;","// check if the page size is < minimum size
", ,// check if the page size is < minimum size,89,89,[0],0,[0],0,[0],0,0,0,0,isCorruptedPage(),org.logstash.ackedqueue.io.PageIO,isCorruptedPage/0,False,89,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,False
201,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\BufferedTokenizerExt.java,org.logstash.common.BufferedTokenizerExt,"RubyArray extract(ThreadContext, IRubyObject)","/**
 * Extract takes an arbitrary string of input data and returns an array of
 * tokenized entities, provided there were any available to extract.  This
 * makes for easy processing of datagrams using a pattern like:
 *
 * {@code tokenizer.extract(data).map { |entity| Decode(entity) }.each do}
 *
 * @param context ThreadContext
 * @param data    IRubyObject
 * @return Extracted tokens
 */
@JRubyMethod
@SuppressWarnings(""rawtypes"")
public RubyArray extract(final ThreadContext context, IRubyObject data) {
    final RubyArray entities = data.convertToString().split(delimiter, -1);
    if (hasSizeLimit) {
        final int entitiesSize = ((RubyString) entities.first()).size();
        if (inputSize + entitiesSize > sizeLimit) {
            throw new IllegalStateException(""input buffer full"");
        }
        this.inputSize = inputSize + entitiesSize;
    }
    input.append(entities.shift(context));
    if (entities.isEmpty()) {
        return RubyUtil.RUBY.newArray();
    }
    entities.unshift(input.join(context));
    input.clear();
    input.append(entities.pop(context));
    inputSize = ((RubyString) input.first()).size();
    return entities;
}","/**
 * Extract takes an arbitrary string of input data and returns an array of
 * tokenized entities, provided there were any available to extract.  This
 * makes for easy processing of datagrams using a pattern like:
 *
 * {@code tokenizer.extract(data).map { |entity| Decode(entity) }.each do}
 *
 * @param context ThreadContext
 * @param data    IRubyObject
 * @return Extracted tokens
 */
", ,"/** * Extract takes an arbitrary string of input data and returns an array of * tokenized entities, provided there were any available to extract.  This * makes for easy processing of datagrams using a pattern like: * * {@code tokenizer.extract(data).map { |entity| Decode(entity) }.each do} * * @param context ThreadContext * @param data    IRubyObject * @return Extracted tokens */",76,96,[0],0,[0],0,[0],0,0,0,0,"extract(ThreadContext, IRubyObject)",org.logstash.common.BufferedTokenizerExt,"extract/2[org.logstash.common.ThreadContext,org.logstash.common.IRubyObject]",False,78,5,0,0,0,4,12,19,2,2,2,12,0,0,0,0,0,2,2,1,4,2,2,0,0,0,48,1,0,True
202,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\BufferedTokenizerExt.java,org.logstash.common.BufferedTokenizerExt,IRubyObject flush(ThreadContext),"/**
 * Flush the contents of the input buffer, i.e. return the input buffer even though
 * a token has not yet been encountered
 *
 * @param context ThreadContext
 * @return Buffer contents
 */
@JRubyMethod
public IRubyObject flush(final ThreadContext context) {
    final IRubyObject buffer = input.join(context);
    input.clear();
    return buffer;
}","/**
 * Flush the contents of the input buffer, i.e. return the input buffer even though
 * a token has not yet been encountered
 *
 * @param context ThreadContext
 * @return Buffer contents
 */
", ,"/** * Flush the contents of the input buffer, i.e. return the input buffer even though * a token has not yet been encountered * * @param context ThreadContext * @return Buffer contents */",105,110,[0],0,[0],0,[0],0,0,0,0,flush(ThreadContext),org.logstash.common.BufferedTokenizerExt,flush/1[org.logstash.common.ThreadContext],False,106,3,0,0,0,1,2,5,1,1,1,2,0,0,0,0,0,0,0,0,1,0,0,0,0,0,23,1,0,True
203,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\DeadLetterQueueFactory.java,org.logstash.common.DeadLetterQueueFactory,"DeadLetterQueueWriter getWriter(String, String, long, Duration, QueueStorageType)","/**
 * Retrieves an existing {@link DeadLetterQueueWriter} associated with the given id, or
 * opens a new one to be returned. It is the retrievers responsibility to close these newly
 * created writers.
 *
 * @param id The identifier context for this dlq manager
 * @param dlqPath The path to use for the queue's backing data directory. contains sub-directories
 *                for each id
 * @param maxQueueSize Maximum size of the dead letter queue (in bytes). No entries will be written
 *                     that would make the size of this dlq greater than this value
 * @param flushInterval Maximum duration between flushes of dead letter queue files if no data is sent.
 * @param storageType overwriting type in case of queue full: drop_older or drop_newer.
 * @return write manager for the specific id's dead-letter-queue context
 */
public static DeadLetterQueueWriter getWriter(String id, String dlqPath, long maxQueueSize, Duration flushInterval, QueueStorageType storageType) {
    return REGISTRY.computeIfAbsent(id, key -> newWriter(key, dlqPath, maxQueueSize, flushInterval, storageType));
}","/**
 * Retrieves an existing {@link DeadLetterQueueWriter} associated with the given id, or
 * opens a new one to be returned. It is the retrievers responsibility to close these newly
 * created writers.
 *
 * @param id The identifier context for this dlq manager
 * @param dlqPath The path to use for the queue's backing data directory. contains sub-directories
 *                for each id
 * @param maxQueueSize Maximum size of the dead letter queue (in bytes). No entries will be written
 *                     that would make the size of this dlq greater than this value
 * @param flushInterval Maximum duration between flushes of dead letter queue files if no data is sent.
 * @param storageType overwriting type in case of queue full: drop_older or drop_newer.
 * @return write manager for the specific id's dead-letter-queue context
 */
", ,"/** * Retrieves an existing {@link DeadLetterQueueWriter} associated with the given id, or * opens a new one to be returned. It is the retrievers responsibility to close these newly * created writers. * * @param id The identifier context for this dlq manager * @param dlqPath The path to use for the queue's backing data directory. contains sub-directories *                for each id * @param maxQueueSize Maximum size of the dead letter queue (in bytes). No entries will be written *                     that would make the size of this dlq greater than this value * @param flushInterval Maximum duration between flushes of dead letter queue files if no data is sent. * @param storageType overwriting type in case of queue full: drop_older or drop_newer. * @return write manager for the specific id's dead-letter-queue context */",82,84,[0],0,[0],0,[0],0,0,0,0,"getWriter(String, String, long, Duration, QueueStorageType)",org.logstash.common.DeadLetterQueueFactory,"getWriter/5[java.lang.String,java.lang.String,long,java.time.Duration,org.logstash.common.io.QueueStorageType]",False,82,3,4,3,1,1,2,3,1,1,5,2,1,1,0,0,0,0,0,0,0,0,0,0,0,1,76,9,0,True
204,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\DeadLetterQueueFactory.java,org.logstash.common.DeadLetterQueueFactory,"DeadLetterQueueWriter getWriter(String, String, long, Duration, QueueStorageType, Duration)","/**
 * Like {@link #getWriter(String, String, long, Duration, QueueStorageType)} but also setting the age duration
 * of the segments.
 *
 * @param id The identifier context for this dlq manager
 * @param dlqPath The path to use for the queue's backing data directory. contains sub-directories
 *                for each id
 * @param maxQueueSize Maximum size of the dead letter queue (in bytes). No entries will be written
 *                     that would make the size of this dlq greater than this value
 * @param flushInterval Maximum duration between flushes of dead letter queue files if no data is sent.
 * @param storageType overwriting type in case of queue full: drop_older or drop_newer.
 * @param age the period that DLQ events should be considered as valid, before automatic removal.
 * @return write manager for the specific id's dead-letter-queue context
 */
public static DeadLetterQueueWriter getWriter(String id, String dlqPath, long maxQueueSize, Duration flushInterval, QueueStorageType storageType, Duration age) {
    return REGISTRY.computeIfAbsent(id, key -> newWriter(key, dlqPath, maxQueueSize, flushInterval, storageType, age));
}","/**
 * Like {@link #getWriter(String, String, long, Duration, QueueStorageType)} but also setting the age duration
 * of the segments.
 *
 * @param id The identifier context for this dlq manager
 * @param dlqPath The path to use for the queue's backing data directory. contains sub-directories
 *                for each id
 * @param maxQueueSize Maximum size of the dead letter queue (in bytes). No entries will be written
 *                     that would make the size of this dlq greater than this value
 * @param flushInterval Maximum duration between flushes of dead letter queue files if no data is sent.
 * @param storageType overwriting type in case of queue full: drop_older or drop_newer.
 * @param age the period that DLQ events should be considered as valid, before automatic removal.
 * @return write manager for the specific id's dead-letter-queue context
 */
", ,"/** * Like {@link #getWriter(String, String, long, Duration, QueueStorageType)} but also setting the age duration * of the segments. * * @param id The identifier context for this dlq manager * @param dlqPath The path to use for the queue's backing data directory. contains sub-directories *                for each id * @param maxQueueSize Maximum size of the dead letter queue (in bytes). No entries will be written *                     that would make the size of this dlq greater than this value * @param flushInterval Maximum duration between flushes of dead letter queue files if no data is sent. * @param storageType overwriting type in case of queue full: drop_older or drop_newer. * @param age the period that DLQ events should be considered as valid, before automatic removal. * @return write manager for the specific id's dead-letter-queue context */",100,102,[0],0,[0],0,[0],0,0,0,0,"getWriter(String, String, long, Duration, QueueStorageType, Duration)",org.logstash.common.DeadLetterQueueFactory,"getWriter/6[java.lang.String,java.lang.String,long,java.time.Duration,org.logstash.common.io.QueueStorageType,java.time.Duration]",False,100,3,2,1,1,1,2,3,1,1,6,2,1,1,0,0,0,0,0,0,0,0,0,0,0,1,77,9,0,True
205,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\FsUtil.java,org.logstash.common.FsUtil,"boolean hasFreeSpace(Path, long)","/**
 * Checks if the request number of bytes of free disk space are available under the given
 * path.
 * @param path Directory to check
 * @param size Bytes of free space requested
 * @return True iff the free space in the specified path meets or exceeds the requested space
 */
public static boolean hasFreeSpace(final Path path, final long size) {
    final long freeSpace = path.toFile().getFreeSpace();
    if (freeSpace == 0L && IS_WINDOWS) {
        // On Windows, SUBST'ed drives report 0L from getFreeSpace().
        // The API doc says ""The number of unallocated bytes on the partition or 0L if the abstract pathname does not name a partition.""
        // There is no straightforward fix for this and it seems a fix is included in Java 9.
        // One alternative is to launch and parse a DIR command and look at the reported free space.
        // This is a temporary fix to get the CI tests going which relies on SUBST'ed drives to manage long paths.
        logger.warn(""Cannot retrieve free space on "" + path.toString() + "". This is probably a SUBST'ed drive."");
        return true;
    }
    return freeSpace >= size;
}","/**
 * Checks if the request number of bytes of free disk space are available under the given
 * path.
 * @param path Directory to check
 * @param size Bytes of free space requested
 * @return True iff the free space in the specified path meets or exceeds the requested space
 */
","// On Windows, SUBST'ed drives report 0L from getFreeSpace().
[[SEP]]// The API doc says ""The number of unallocated bytes on the partition or 0L if the abstract pathname does not name a partition.""
[[SEP]]// There is no straightforward fix for this and it seems a fix is included in Java 9.
[[SEP]]// One alternative is to launch and parse a DIR command and look at the reported free space.
[[SEP]]// This is a temporary fix to get the CI tests going which relies on SUBST'ed drives to manage long paths.
","/** * Checks if the request number of bytes of free disk space are available under the given * path. * @param path Directory to check * @param size Bytes of free space requested * @return True iff the free space in the specified path meets or exceeds the requested space */[[SEP]]// On Windows, SUBST'ed drives report 0L from getFreeSpace().// The API doc says ""The number of unallocated bytes on the partition or 0L if the abstract pathname does not name a partition.""// There is no straightforward fix for this and it seems a fix is included in Java 9.// One alternative is to launch and parse a DIR command and look at the reported free space.// This is a temporary fix to get the CI tests going which relies on SUBST'ed drives to manage long paths.",45,60,[0],0,"[0, 0, 0, 0, 1]",1,"[0, 1]",1,0,1,0,"hasFreeSpace(Path, long)",org.logstash.common.FsUtil,"hasFreeSpace/2[java.nio.file.Path,long]",False,46,1,4,3,1,4,4,8,2,1,2,4,0,0,0,1,0,0,2,1,1,1,1,0,0,0,47,9,1,True
206,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\LsQueueUtils.java,org.logstash.common.LsQueueUtils,"void addAll(BlockingQueue<RubyEvent>, Collection<RubyEvent>)","/**
 * Adds all {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} in the given collection to the given queue
 * in a blocking manner, only returning once all events have been added to the queue.
 * @param queue Queue to add Events to
 * @param events Events to add to Queue
 * @throws InterruptedException On interrupt during blocking queue add
 */
public static void addAll(final BlockingQueue<RubyEvent> queue, final Collection<RubyEvent> events) throws InterruptedException {
    for (final RubyEvent event : events) {
        queue.put(event);
    }
}","/**
 * Adds all {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} in the given collection to the given queue
 * in a blocking manner, only returning once all events have been added to the queue.
 * @param queue Queue to add Events to
 * @param events Events to add to Queue
 * @throws InterruptedException On interrupt during blocking queue add
 */
", ,"/** * Adds all {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} in the given collection to the given queue * in a blocking manner, only returning once all events have been added to the queue. * @param queue Queue to add Events to * @param events Events to add to Queue * @throws InterruptedException On interrupt during blocking queue add */",45,53,[0],0,[0],0,[0],0,0,0,0,"addAll(BlockingQueue<RubyEvent>, Collection<RubyEvent>)",org.logstash.common.LsQueueUtils,"addAll/2[java.util.concurrent.BlockingQueue<org.logstash.ext.JrubyEventExtLibrary.RubyEvent>,java.util.Collection<org.logstash.ext.JrubyEventExtLibrary.RubyEvent>]",False,49,1,1,1,0,2,1,5,0,0,2,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,30,9,0,True
207,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\LsQueueUtils.java,org.logstash.common.LsQueueUtils,"Collection<RubyEvent> drain(BlockingQueue<RubyEvent>, int, long)","/**
 * <p>Drains {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} from {@link BlockingQueue} with a timeout.</p>
 * <p>The timeout will be reset as soon as a single {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} was
 * drained from the {@link BlockingQueue}. Draining {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}
 * stops as soon as either the required number of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s
 * were pulled from the queue or the timeout value has gone by without an event drained.</p>
 * @param queue Blocking Queue to drain {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s
 * from
 * @param count Number of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s to drain from
 * {@link BlockingQueue}
 * @param nanos Timeout in Nanoseconds
 * @return Collection of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} drained from
 * {@link BlockingQueue}
 * @throws InterruptedException On Interrupt during {@link BlockingQueue#poll()} or
 * {@link BlockingQueue#drainTo(Collection)}
 */
public static Collection<RubyEvent> drain(final BlockingQueue<RubyEvent> queue, final int count, final long nanos) throws InterruptedException {
    int left = count;
    final ArrayList<RubyEvent> collection = new ArrayList<>(4 * count / 3 + 1);
    do {
        final int drained = drain(queue, collection, left, nanos);
        if (drained == 0) {
            break;
        }
        left -= drained;
    } while (left > 0);
    return collection;
}","/**
 * <p>Drains {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} from {@link BlockingQueue} with a timeout.</p>
 * <p>The timeout will be reset as soon as a single {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} was
 * drained from the {@link BlockingQueue}. Draining {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}
 * stops as soon as either the required number of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s
 * were pulled from the queue or the timeout value has gone by without an event drained.</p>
 * @param queue Blocking Queue to drain {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s
 * from
 * @param count Number of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s to drain from
 * {@link BlockingQueue}
 * @param nanos Timeout in Nanoseconds
 * @return Collection of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} drained from
 * {@link BlockingQueue}
 * @throws InterruptedException On Interrupt during {@link BlockingQueue#poll()} or
 * {@link BlockingQueue#drainTo(Collection)}
 */
", ,/** * <p>Drains {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} from {@link BlockingQueue} with a timeout.</p> * <p>The timeout will be reset as soon as a single {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} was * drained from the {@link BlockingQueue}. Draining {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} * stops as soon as either the required number of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s * were pulled from the queue or the timeout value has gone by without an event drained.</p> * @param queue Blocking Queue to drain {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s * from * @param count Number of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s to drain from * {@link BlockingQueue} * @param nanos Timeout in Nanoseconds * @return Collection of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} drained from * {@link BlockingQueue} * @throws InterruptedException On Interrupt during {@link BlockingQueue#poll()} or * {@link BlockingQueue#drainTo(Collection)} */,71,87,[0],0,[0],0,[0],0,0,0,0,"drain(BlockingQueue<RubyEvent>, int, long)",org.logstash.common.LsQueueUtils,"drain/3[java.util.concurrent.BlockingQueue<org.logstash.ext.JrubyEventExtLibrary.RubyEvent>,int,long]",False,76,2,2,1,1,3,1,13,1,3,3,1,1,1,1,1,0,0,0,5,4,3,2,0,0,0,52,9,0,True
208,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\LsQueueUtils.java,org.logstash.common.LsQueueUtils,"int drain(BlockingQueue<RubyEvent>, Collection<RubyEvent>, int, long)","/**
 * Tries to drain a given number of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} from
 * {@link BlockingQueue} with a timeout.
 * @param queue Blocking Queue to drain {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s
 * from
 * @param count Number of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s to drain from
 * {@link BlockingQueue}
 * @param nanos Timeout in Nanoseconds
 * @return Collection of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} drained from
 * {@link BlockingQueue}
 * @throws InterruptedException On Interrupt during {@link BlockingQueue#poll()} or
 * {@link BlockingQueue#drainTo(Collection)}
 */
private static int drain(final BlockingQueue<RubyEvent> queue, final Collection<RubyEvent> collection, final int count, final long nanos) throws InterruptedException {
    int added = 0;
    do {
        added += queue.drainTo(collection, count - added);
        if (added < count) {
            final RubyEvent event = queue.poll(nanos, TimeUnit.NANOSECONDS);
            if (event == null) {
                break;
            }
            collection.add(event);
            added++;
        }
    } while (added < count);
    return added;
}","/**
 * Tries to drain a given number of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} from
 * {@link BlockingQueue} with a timeout.
 * @param queue Blocking Queue to drain {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s
 * from
 * @param count Number of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s to drain from
 * {@link BlockingQueue}
 * @param nanos Timeout in Nanoseconds
 * @return Collection of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} drained from
 * {@link BlockingQueue}
 * @throws InterruptedException On Interrupt during {@link BlockingQueue#poll()} or
 * {@link BlockingQueue#drainTo(Collection)}
 */
", ,/** * Tries to drain a given number of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} from * {@link BlockingQueue} with a timeout. * @param queue Blocking Queue to drain {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s * from * @param count Number of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}s to drain from * {@link BlockingQueue} * @param nanos Timeout in Nanoseconds * @return Collection of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} drained from * {@link BlockingQueue} * @throws InterruptedException On Interrupt during {@link BlockingQueue#poll()} or * {@link BlockingQueue#drainTo(Collection)} */,102,122,[0],0,[0],0,[0],0,0,0,0,"drain(BlockingQueue<RubyEvent>, Collection<RubyEvent>, int, long)",org.logstash.common.LsQueueUtils,"drain/4[java.util.concurrent.BlockingQueue<org.logstash.ext.JrubyEventExtLibrary.RubyEvent>,java.util.Collection<org.logstash.ext.JrubyEventExtLibrary.RubyEvent>,int,long]",False,108,1,1,1,0,4,3,16,1,2,4,3,0,0,1,1,0,0,0,1,3,1,3,0,0,0,32,10,0,True
209,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\NullDeadLetterQueueWriter.java,org.logstash.common.NullDeadLetterQueueWriter,"void writeEntry(Event, Plugin, String)","@Override
public void writeEntry(Event event, Plugin plugin, String reason) throws IOException {
    // no-op
}", ,"// no-op
",// no-op,39,42,[0],0,[0],0,[0],0,0,0,0,"writeEntry(Event, Plugin, String)",org.logstash.common.NullDeadLetterQueueWriter,"writeEntry/3[co.elastic.logstash.api.Event,co.elastic.logstash.api.Plugin,java.lang.String]",False,40,2,0,0,0,1,0,2,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,1,0,False
210,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\SourceWithMetadata.java,org.logstash.common.SourceWithMetadata,Collection<Object> attributes(),"// Fields checked for being not null and non empty String
private Collection<Object> attributes() {
    return Arrays.asList(this.getId(), this.getProtocol(), this.getLine(), this.getColumn());
}","// Fields checked for being not null and non empty String
", ,// Fields checked for being not null and non empty String,122,124,[0],0,[0],0,[0],0,0,0,0,attributes(),org.logstash.common.SourceWithMetadata,attributes/0,False,122,1,5,1,4,1,5,3,1,0,0,5,4,1,0,0,0,0,0,0,0,0,0,0,0,0,1,2,0,False
211,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\SourceWithMetadata.java,org.logstash.common.SourceWithMetadata,Collection<Object> hashableAttributes(),"// Fields used in the hashSource and hashCode methods to ensure uniqueness
private Collection<Object> hashableAttributes() {
    return Arrays.asList(this.getId(), this.getProtocol(), this.getLine(), this.getColumn(), this.getText());
}","// Fields used in the hashSource and hashCode methods to ensure uniqueness
", ,// Fields used in the hashSource and hashCode methods to ensure uniqueness,127,129,[0],0,[0],0,[0],0,0,0,0,hashableAttributes(),org.logstash.common.SourceWithMetadata,hashableAttributes/0,False,127,1,7,2,5,1,6,3,1,0,0,6,5,1,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,False
212,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueReader.java,org.logstash.common.io.DeadLetterQueueReader,Optional<RecordIOReader> openSegmentReader(Path),"/**
 * Opens the segment reader for the given path.
 * Side effect: Will attempt to remove the given segment from the list of active
 *              segments if segment is not found.
 * @param segment Path to segment File
 * @return Optional containing a RecordIOReader if the segment exists
 * @throws IOException if any IO error happens during file management
 */
private Optional<RecordIOReader> openSegmentReader(Path segment) throws IOException {
    if (!Files.exists(segment)) {
        // file was deleted by upstream process and segments list wasn't yet updated
        segments.remove(segment);
        return Optional.empty();
    }
    try {
        return Optional.of(new RecordIOReader(segment));
    } catch (NoSuchFileException ex) {
        logger.debug(""Segment file {} was deleted by DLQ writer during DLQ reader opening"", segment);
        // file was deleted by upstream process and segments list wasn't yet updated
        segments.remove(segment);
        return Optional.empty();
    }
}","/**
 * Opens the segment reader for the given path.
 * Side effect: Will attempt to remove the given segment from the list of active
 *              segments if segment is not found.
 * @param segment Path to segment File
 * @return Optional containing a RecordIOReader if the segment exists
 * @throws IOException if any IO error happens during file management
 */
","// file was deleted by upstream process and segments list wasn't yet updated
[[SEP]]// file was deleted by upstream process and segments list wasn't yet updated
",/** * Opens the segment reader for the given path. * Side effect: Will attempt to remove the given segment from the list of active *              segments if segment is not found. * @param segment Path to segment File * @return Optional containing a RecordIOReader if the segment exists * @throws IOException if any IO error happens during file management */[[SEP]]// file was deleted by upstream process and segments list wasn't yet updated[[SEP]]// file was deleted by upstream process and segments list wasn't yet updated,149,164,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,0,0,openSegmentReader(Path),org.logstash.common.io.DeadLetterQueueReader,openSegmentReader/1[java.nio.file.Path],False,149,2,6,4,2,3,5,14,3,0,1,5,0,0,0,0,1,0,1,0,0,0,1,0,0,0,46,2,1,True
213,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueReader.java,org.logstash.common.io.DeadLetterQueueReader,byte[] pollEntryBytes(),"// package-private for test
byte[] pollEntryBytes() throws IOException, InterruptedException {
    return pollEntryBytes(100);
}","// package-private for test
", ,// package-private for test,213,215,[0],0,[0],0,[0],0,0,0,0,pollEntryBytes(),org.logstash.common.io.DeadLetterQueueReader,pollEntryBytes/0,False,213,2,7,6,1,1,1,3,1,0,0,1,1,4,0,0,0,0,0,1,0,0,0,0,0,0,5,0,0,False
214,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueReader.java,org.logstash.common.io.DeadLetterQueueReader,byte[] pollEntryBytes(long),"private byte[] pollEntryBytes(long timeout) throws IOException, InterruptedException {
    long timeoutRemaining = timeout;
    if (currentReader == null) {
        timeoutRemaining -= pollNewSegments(timeout);
        // If no new segments are found, exit
        if (segments.isEmpty()) {
            logger.debug(""No entries found: no segment files found in dead-letter-queue directory"");
            return null;
        }
        Optional<RecordIOReader> optReader;
        do {
            final Path firstSegment;
            try {
                firstSegment = segments.first();
            } catch (NoSuchElementException ex) {
                // all elements were removed after the empty check
                logger.debug(""No entries found: no segment files found in dead-letter-queue directory"");
                return null;
            }
            optReader = openSegmentReader(firstSegment);
            if (optReader.isPresent()) {
                currentReader = optReader.get();
            }
        } while (!optReader.isPresent());
    }
    byte[] event = currentReader.readEvent();
    if (event == null && currentReader.isEndOfStream()) {
        if (consumedAllSegments()) {
            pollNewSegments(timeoutRemaining);
        } else {
            currentReader.close();
            if (cleanConsumed) {
                lastConsumedReader = currentReader;
            }
            Optional<RecordIOReader> optReader = openNextExistingReader(currentReader.getPath());
            if (!optReader.isPresent()) {
                // segments were all already deleted files, do a poll
                pollNewSegments(timeoutRemaining);
            } else {
                currentReader = optReader.get();
                return pollEntryBytes(timeoutRemaining);
            }
        }
    }
    return event;
}", ,"// If no new segments are found, exit
[[SEP]]// all elements were removed after the empty check
[[SEP]]// segments were all already deleted files, do a poll
","// If no new segments are found, exit[[SEP]]// all elements were removed after the empty check[[SEP]]// segments were all already deleted files, do a poll",217,265,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,pollEntryBytes(long),org.logstash.common.io.DeadLetterQueueReader,pollEntryBytes/1[long],False,217,4,13,3,10,11,14,47,4,5,1,14,5,3,1,2,1,0,2,0,9,0,3,0,0,0,38,2,2,False
215,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueReader.java,org.logstash.common.io.DeadLetterQueueReader,void markForDelete(),"/**
 * Acknowledge last read event, must match every {@code #pollEntry} call.
 */
public void markForDelete() {
    if (!cleanConsumed) {
        // ack-event is useful only when clean consumed is enabled.
        return;
    }
    if (lastConsumedReader == null) {
        // no reader to a consumed segment is present
        return;
    }
    segmentCallback.segmentCompleted();
    Path lastConsumedSegmentPath = lastConsumedReader.getPath();
    // delete also the older segments in case of multiple segments were consumed
    // before the invocation of the mark method.
    try {
        removeSegmentsBefore(lastConsumedSegmentPath);
    } catch (IOException ex) {
        logger.warn(""Problem occurred in cleaning the segments older than {} "", lastConsumedSegmentPath, ex);
    }
    // delete segment file only after current reader is closed.
    // closing happens in pollEntryBytes method when it identifies the reader is at end of stream
    final Optional<Long> deletedEvents = deleteSegment(lastConsumedSegmentPath);
    if (deletedEvents.isPresent()) {
        // update consumed metrics
        consumedEvents.add(deletedEvents.get());
        consumedSegments.increment();
    }
    // publish the metrics to the listener
    segmentCallback.segmentsDeleted(consumedSegments.intValue(), consumedEvents.longValue());
    lastConsumedReader = null;
}","/**
 * Acknowledge last read event, must match every {@code #pollEntry} call.
 */
","// delete also the older segments in case of multiple segments were consumed
[[SEP]]// delete segment file only after current reader is closed.
[[SEP]]// ack-event is useful only when clean consumed is enabled.
[[SEP]]// no reader to a consumed segment is present
[[SEP]]// before the invocation of the mark method.
[[SEP]]// closing happens in pollEntryBytes method when it identifies the reader is at end of stream
[[SEP]]// update consumed metrics
[[SEP]]// publish the metrics to the listener
","/** * Acknowledge last read event, must match every {@code #pollEntry} call. */[[SEP]]// ack-event is useful only when clean consumed is enabled.[[SEP]]// no reader to a consumed segment is present[[SEP]]// delete also the older segments in case of multiple segments were consumed// before the invocation of the mark method.[[SEP]]// delete segment file only after current reader is closed.// closing happens in pollEntryBytes method when it identifies the reader is at end of stream[[SEP]]// update consumed metrics[[SEP]]// publish the metrics to the listener",270,305,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0, 0]",0,0,0,0,markForDelete(),org.logstash.common.io.DeadLetterQueueReader,markForDelete/0,False,270,4,9,3,6,5,12,23,2,2,0,12,2,1,0,1,1,0,1,0,3,0,1,0,0,0,32,1,1,True
216,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueReader.java,org.logstash.common.io.DeadLetterQueueReader,boolean consumedAllSegments(),"private boolean consumedAllSegments() {
    try {
        return currentReader.getPath().equals(segments.last());
    } catch (NoSuchElementException ex) {
        // last segment was removed while processing
        logger.debug(""No last segment found, poll for new segments"");
        return true;
    }
}", ,"// last segment was removed while processing
",// last segment was removed while processing,307,315,[0],0,[0],0,[0],0,0,0,0,consumedAllSegments(),org.logstash.common.io.DeadLetterQueueReader,consumedAllSegments/0,False,307,2,3,1,2,2,4,9,2,0,0,4,0,0,0,0,1,0,1,0,0,0,1,0,0,0,12,2,1,False
217,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueReader.java,org.logstash.common.io.DeadLetterQueueReader,"void setCurrentReaderAndPosition(Path, long)","public void setCurrentReaderAndPosition(Path segmentPath, long position) throws IOException {
    if (cleanConsumed) {
        removeSegmentsBefore(segmentPath);
    }
    // If the provided segment Path exist, then set the reader to start from the supplied position
    Optional<RecordIOReader> optReader = openSegmentReader(segmentPath);
    if (optReader.isPresent()) {
        currentReader = optReader.get();
        currentReader.seekToOffset(position);
        return;
    }
    // Otherwise, set the current reader to be at the beginning of the next
    // segment.
    optReader = openNextExistingReader(segmentPath);
    if (optReader.isPresent()) {
        currentReader = optReader.get();
        return;
    }
    pollNewSegments();
    // give a second try after a re-load of segments from filesystem
    openNextExistingReader(segmentPath).ifPresent(reader -> currentReader = reader);
}", ,"// Otherwise, set the current reader to be at the beginning of the next
[[SEP]]// If the provided segment Path exist, then set the reader to start from the supplied position
[[SEP]]// segment.
[[SEP]]// give a second try after a re-load of segments from filesystem
","// If the provided segment Path exist, then set the reader to start from the supplied position[[SEP]]// Otherwise, set the current reader to be at the beginning of the next// segment.[[SEP]]// give a second try after a re-load of segments from filesystem",332,357,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,"setCurrentReaderAndPosition(Path, long)",org.logstash.common.io.DeadLetterQueueReader,"setCurrentReaderAndPosition/2[java.nio.file.Path,long]",False,332,2,12,7,5,4,8,18,2,2,2,8,4,3,0,0,0,0,0,0,5,0,1,0,0,1,25,1,0,False
218,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueReader.java,org.logstash.common.io.DeadLetterQueueReader,void removeSegmentsBefore(Path),"private void removeSegmentsBefore(Path validSegment) throws IOException {
    final Comparator<Path> fileTimeAndName = ((Comparator<Path>) this::compareByFileTimestamp).thenComparingInt(DeadLetterQueueUtils::extractSegmentId);
    try (final Stream<Path> segmentFiles = listSegmentPaths(queuePath)) {
        LongSummaryStatistics deletionStats = segmentFiles.filter(p -> fileTimeAndName.compare(p, validSegment) < 0).map(this::deleteSegment).map(o -> o.orElse(0L)).mapToLong(Long::longValue).summaryStatistics();
        // update consumed metrics
        consumedSegments.add(deletionStats.getCount());
        consumedEvents.add(deletionStats.getSum());
    }
}", ,"// update consumed metrics
",// update consumed metrics,359,374,[0],0,[0],0,[0],0,0,0,0,removeSegmentsBefore(Path),org.logstash.common.io.DeadLetterQueueReader,removeSegmentsBefore/1[java.nio.file.Path],False,359,1,3,2,1,2,12,8,0,5,1,12,0,0,0,0,1,1,0,2,3,0,1,0,0,2,27,2,0,False
219,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueReader.java,org.logstash.common.io.DeadLetterQueueReader,"int compareByFileTimestamp(Path, Path)","private int compareByFileTimestamp(Path p1, Path p2) {
    FileTime timestamp1;
    // if one of the getLastModifiedTime raise an error, consider them equals
    // and fallback to the other comparator
    try {
        timestamp1 = Files.getLastModifiedTime(p1);
    } catch (IOException ex) {
        logger.warn(""Error reading file's timestamp for {}"", p1, ex);
        return 0;
    }
    FileTime timestamp2;
    try {
        timestamp2 = Files.getLastModifiedTime(p2);
    } catch (IOException ex) {
        logger.warn(""Error reading file's timestamp for {}"", p2, ex);
        return 0;
    }
    return timestamp1.compareTo(timestamp2);
}", ,"// if one of the getLastModifiedTime raise an error, consider them equals
[[SEP]]// and fallback to the other comparator
","// if one of the getLastModifiedTime raise an error, consider them equals// and fallback to the other comparator",376,395,[0],0,"[0, 0]",0,[0],0,0,0,0,"compareByFileTimestamp(Path, Path)",org.logstash.common.io.DeadLetterQueueReader,"compareByFileTimestamp/2[java.nio.file.Path,java.nio.file.Path]",False,376,1,1,0,1,3,3,19,3,2,2,3,0,0,0,0,2,0,2,2,2,0,1,0,0,0,16,2,2,False
220,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueReader.java,org.logstash.common.io.DeadLetterQueueReader,Optional<Long> deleteSegment(Path),"/**
 * Remove the segment from internal tracking data structures and physically delete the corresponding
 * file from filesystem.
 *
 * @return the number events contained in the removed segment, empty if a problem happened during delete.
 */
private Optional<Long> deleteSegment(Path segment) {
    segments.remove(segment);
    try {
        long eventsInSegment = DeadLetterQueueUtils.countEventsInSegment(segment);
        Files.delete(segment);
        logger.debug(""Deleted segment {}"", segment);
        return Optional.of(eventsInSegment);
    } catch (IOException ex) {
        logger.warn(""Problem occurred in cleaning the segment {} after a repositioning"", segment, ex);
        return Optional.empty();
    }
}","/**
 * Remove the segment from internal tracking data structures and physically delete the corresponding
 * file from filesystem.
 *
 * @return the number events contained in the removed segment, empty if a problem happened during delete.
 */
", ,"/** * Remove the segment from internal tracking data structures and physically delete the corresponding * file from filesystem. * * @return the number events contained in the removed segment, empty if a problem happened during delete. */",403,414,[0],0,[0],0,[0],0,0,0,0,deleteSegment(Path),org.logstash.common.io.DeadLetterQueueReader,deleteSegment/1[java.nio.file.Path],False,403,2,4,1,3,2,7,13,2,1,1,7,0,0,0,0,1,0,2,0,1,0,1,0,0,0,33,2,2,True
221,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueUtils.java,org.logstash.common.io.DeadLetterQueueUtils,long countEventsInSegment(Path),"/**
 * Count the number of 'c' and 's' records in segment.
 * An event can't be bigger than the segments so in case of records split across multiple event blocks,
 * the segment has to contain both the start 's' record, all the middle 'm' up to the end 'e' records.
 */
@SuppressWarnings(""fallthrough"")
static long countEventsInSegment(Path segment) throws IOException {
    try (FileChannel channel = FileChannel.open(segment, StandardOpenOption.READ)) {
        // verify minimal segment size
        if (channel.size() < VERSION_SIZE + RECORD_HEADER_SIZE) {
            return 0L;
        }
        // skip the DLQ version byte
        channel.position(1);
        int posInBlock = 0;
        int currentBlockIdx = 0;
        long countedEvents = 0;
        do {
            ByteBuffer headerBuffer = ByteBuffer.allocate(RECORD_HEADER_SIZE);
            long startPosition = channel.position();
            // if record header can't be fully contained in the block, align to the next
            if (posInBlock + RECORD_HEADER_SIZE + 1 > BLOCK_SIZE) {
                channel.position((++currentBlockIdx) * BLOCK_SIZE + VERSION_SIZE);
                posInBlock = 0;
            }
            channel.read(headerBuffer);
            headerBuffer.flip();
            RecordHeader recordHeader = RecordHeader.get(headerBuffer);
            if (recordHeader == null) {
                // continue with next record, skipping this
                logger.error(""Can't decode record header, position {} current post {} current events count {}"", startPosition, channel.position(), countedEvents);
            } else {
                switch(recordHeader.getType()) {
                    case START:
                    case COMPLETE:
                        countedEvents++;
                    case MIDDLE:
                    case END:
                        {
                            channel.position(channel.position() + recordHeader.getSize());
                            posInBlock += RECORD_HEADER_SIZE + recordHeader.getSize();
                        }
                }
            }
        } while (channel.position() < channel.size());
        return countedEvents;
    }
}","/**
 * Count the number of 'c' and 's' records in segment.
 * An event can't be bigger than the segments so in case of records split across multiple event blocks,
 * the segment has to contain both the start 's' record, all the middle 'm' up to the end 'e' records.
 */
","// verify minimal segment size
[[SEP]]// skip the DLQ version byte
[[SEP]]// if record header can't be fully contained in the block, align to the next
[[SEP]]// continue with next record, skipping this
","/** * Count the number of 'c' and 's' records in segment. * An event can't be bigger than the segments so in case of records split across multiple event blocks, * the segment has to contain both the start 's' record, all the middle 'm' up to the end 'e' records. */[[SEP]]// verify minimal segment size[[SEP]]// skip the DLQ version byte[[SEP]]// if record header can't be fully contained in the block, align to the next[[SEP]]// continue with next record, skipping this",59,103,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,countEventsInSegment(Path),org.logstash.common.io.DeadLetterQueueUtils,countEventsInSegment/1[java.nio.file.Path],False,60,2,6,2,4,9,11,40,2,7,1,11,0,0,1,1,1,1,2,7,9,6,4,0,0,0,70,8,1,True
222,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueWriter.java,org.logstash.common.io.DeadLetterQueueWriter,boolean executeStoragePolicy(int),"/**
 * @param eventPayloadSize payload size in bytes.
 * @return boolean true if event write has to be skipped.
 */
private boolean executeStoragePolicy(int eventPayloadSize) {
    if (!exceedMaxQueueSize(eventPayloadSize)) {
        return false;
    }
    // load currentQueueSize from filesystem because there could be a consumer
    // that's already cleaning
    try {
        this.currentQueueSize.set(computeQueueSize());
    } catch (IOException ex) {
        logger.warn(""Unable to determine DLQ size, skipping storage policy check"", ex);
        return false;
    }
    // after reload verify the condition is still valid
    if (!exceedMaxQueueSize(eventPayloadSize)) {
        return false;
    }
    if (storageType == QueueStorageType.DROP_NEWER) {
        lastError = String.format(""Cannot write event to DLQ(path: %s): reached maxQueueSize of %d"", queuePath, maxQueueSize);
        logger.error(lastError);
        droppedEvents.add(1L);
        return true;
    } else {
        try {
            do {
                dropTailSegment();
            } while (exceedMaxQueueSize(eventPayloadSize));
        } catch (IOException ex) {
            logger.error(""Can't remove some DLQ files while removing older segments"", ex);
        }
        return false;
    }
}","/**
 * @param eventPayloadSize payload size in bytes.
 * @return boolean true if event write has to be skipped.
 */
","// load currentQueueSize from filesystem because there could be a consumer
[[SEP]]// that's already cleaning
[[SEP]]// after reload verify the condition is still valid
",/** * @param eventPayloadSize payload size in bytes. * @return boolean true if event write has to be skipped. */[[SEP]]// load currentQueueSize from filesystem because there could be a consumer// that's already cleaning[[SEP]]// after reload verify the condition is still valid,289,323,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,executeStoragePolicy(int),org.logstash.common.io.DeadLetterQueueWriter,executeStoragePolicy/1[int],False,289,2,7,1,6,7,9,33,5,0,1,9,3,2,1,1,2,0,3,1,1,0,3,0,0,0,45,2,3,True
223,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueWriter.java,org.logstash.common.io.DeadLetterQueueWriter,void deleteExpiredSegments(),"private void deleteExpiredSegments() throws IOException {
    // remove all the old segments that verifies the age retention condition
    boolean cleanNextSegment;
    do {
        if (oldestSegmentPath.isPresent()) {
            Path beheadedSegment = oldestSegmentPath.get();
            expiredEvents.add(deleteTailSegment(beheadedSegment, ""age retention policy""));
        }
        updateOldestSegmentReference();
        cleanNextSegment = isOldestSegmentExpired();
    } while (cleanNextSegment);
    this.currentQueueSize.set(computeQueueSize());
}", ,"// remove all the old segments that verifies the age retention condition
",// remove all the old segments that verifies the age retention condition,339,352,[0],0,[0],0,[0],0,0,0,0,deleteExpiredSegments(),org.logstash.common.io.DeadLetterQueueWriter,deleteExpiredSegments/0,False,339,1,5,1,4,3,8,13,0,2,0,8,4,2,1,0,0,0,1,0,2,0,2,0,0,0,20,2,0,False
224,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueWriter.java,org.logstash.common.io.DeadLetterQueueWriter,"long deleteTailSegment(Path, String)","/**
 * Deletes the segment path, if present. Also return the number of events it contains.
 *
 * @param segment
 *      The segment file to delete.
 * @param motivation
 *      Description of delete motivation.
 * @return the number of events contained in the segment or 0 if the segment was already removed.
 * @throws IOException if any other IO related error happens during deletion of the segment.
 */
private long deleteTailSegment(Path segment, String motivation) throws IOException {
    try {
        long eventsInSegment = DeadLetterQueueUtils.countEventsInSegment(segment);
        Files.delete(segment);
        logger.debug(""Removed segment file {} due to {}"", motivation, segment);
        return eventsInSegment;
    } catch (NoSuchFileException nsfex) {
        // the last segment was deleted by another process, maybe the reader that's cleaning consumed segments
        logger.debug(""File not found {}, maybe removed by the reader pipeline"", segment);
        return 0;
    }
}","/**
 * Deletes the segment path, if present. Also return the number of events it contains.
 *
 * @param segment
 *      The segment file to delete.
 * @param motivation
 *      Description of delete motivation.
 * @return the number of events contained in the segment or 0 if the segment was already removed.
 * @throws IOException if any other IO related error happens during deletion of the segment.
 */
","// the last segment was deleted by another process, maybe the reader that's cleaning consumed segments
","/** * Deletes the segment path, if present. Also return the number of events it contains. * * @param segment *      The segment file to delete. * @param motivation *      Description of delete motivation. * @return the number of events contained in the segment or 0 if the segment was already removed. * @throws IOException if any other IO related error happens during deletion of the segment. */[[SEP]]// the last segment was deleted by another process, maybe the reader that's cleaning consumed segments",364,375,[0],0,[0],0,"[0, 0]",0,0,0,0,"deleteTailSegment(Path, String)",org.logstash.common.io.DeadLetterQueueWriter,"deleteTailSegment/2[java.nio.file.Path,java.lang.String]",False,364,2,5,2,3,2,4,12,2,1,2,4,0,0,0,0,1,0,2,1,1,0,1,0,0,0,44,2,2,True
225,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueWriter.java,org.logstash.common.io.DeadLetterQueueWriter,void updateOldestSegmentReference(),"private void updateOldestSegmentReference() throws IOException {
    oldestSegmentPath = listSegmentPaths(this.queuePath).sorted().findFirst();
    if (!oldestSegmentPath.isPresent()) {
        oldestSegmentTimestamp = Optional.empty();
        return;
    }
    // extract the newest timestamp from the oldest segment
    Optional<Timestamp> foundTimestamp = readTimestampOfLastEventInSegment(oldestSegmentPath.get());
    if (!foundTimestamp.isPresent()) {
        // clean also the last segment, because doesn't contain a timestamp (corrupted maybe)
        // or is not present anymore
        oldestSegmentPath = Optional.empty();
    }
    oldestSegmentTimestamp = foundTimestamp;
}", ,"// extract the newest timestamp from the oldest segment
[[SEP]]// clean also the last segment, because doesn't contain a timestamp (corrupted maybe)
[[SEP]]// or is not present anymore
","// extract the newest timestamp from the oldest segment[[SEP]]// clean also the last segment, because doesn't contain a timestamp (corrupted maybe)// or is not present anymore",377,391,[0],0,"[0, 0, 0]",0,"[0, 0]",0,0,0,0,updateOldestSegmentReference(),org.logstash.common.io.DeadLetterQueueWriter,updateOldestSegmentReference/0,False,377,3,5,3,2,3,8,12,1,1,0,8,1,1,0,0,0,0,0,0,5,0,1,0,0,0,18,2,0,False
226,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueWriter.java,org.logstash.common.io.DeadLetterQueueWriter,Optional<Timestamp> readTimestampOfLastEventInSegment(Path),"/**
 * Extract the timestamp from the last DLQEntry it finds in the given segment.
 * Start from the end of the latest block, and going backward try to read the next event from its start.
 */
private static Optional<Timestamp> readTimestampOfLastEventInSegment(Path segmentPath) throws IOException {
    final int lastBlockId = (int) Math.ceil(((Files.size(segmentPath) - VERSION_SIZE) / (double) BLOCK_SIZE)) - 1;
    byte[] eventBytes;
    try (RecordIOReader recordReader = new RecordIOReader(segmentPath)) {
        int blockId = lastBlockId;
        do {
            recordReader.seekToBlock(blockId);
            eventBytes = recordReader.readEvent();
            blockId--;
        } while (// no event present in last block, try with the one before
        eventBytes == null && blockId >= 0);
    } catch (NoSuchFileException nsfex) {
        // the segment file may have been removed by the clean consumed feature on the reader side
        return Optional.empty();
    }
    if (eventBytes == null) {
        logger.warn(""Cannot find a complete event into the segment file [{}], this is a DLQ segment corruption"", segmentPath);
        return Optional.empty();
    }
    return Optional.of(DLQEntry.deserialize(eventBytes).getEntryTime());
}","/**
 * Extract the timestamp from the last DLQEntry it finds in the given segment.
 * Start from the end of the latest block, and going backward try to read the next event from its start.
 */
","// no event present in last block, try with the one before
[[SEP]]// the segment file may have been removed by the clean consumed feature on the reader side
","/** * Extract the timestamp from the last DLQEntry it finds in the given segment. * Start from the end of the latest block, and going backward try to read the next event from its start. */[[SEP]]// no event present in last block, try with the one before[[SEP]]// the segment file may have been removed by the clean consumed feature on the reader side",397,416,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,0,0,readTimestampOfLastEventInSegment(Path),org.logstash.common.io.DeadLetterQueueWriter,readTimestampOfLastEventInSegment/1[java.nio.file.Path],False,397,5,7,1,6,5,9,21,3,4,1,9,0,0,1,2,1,2,1,2,4,3,2,0,0,0,64,10,1,True
227,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueWriter.java,org.logstash.common.io.DeadLetterQueueWriter,void dropTailSegment(),"// package-private for testing
void dropTailSegment() throws IOException {
    // remove oldest segment
    final Optional<Path> oldestSegment = listSegmentPaths(queuePath).min(Comparator.comparingInt(DeadLetterQueueUtils::extractSegmentId));
    if (oldestSegment.isPresent()) {
        final Path beheadedSegment = oldestSegment.get();
        deleteTailSegment(beheadedSegment, ""dead letter queue size exceeded dead_letter_queue.max_bytes size("" + maxQueueSize + "")"");
    } else {
        logger.info(""Queue size {} exceeded, but no complete DLQ segments found"", maxQueueSize);
    }
    this.currentQueueSize.set(computeQueueSize());
}","// package-private for testing
","// remove oldest segment
",// package-private for testing[[SEP]]// remove oldest segment,419,430,[0],0,[0],0,"[0, 0]",0,0,0,0,dropTailSegment(),org.logstash.common.io.DeadLetterQueueWriter,dropTailSegment/0,False,419,3,6,2,4,2,9,11,0,2,0,9,2,1,0,0,0,0,3,0,2,1,1,0,0,0,27,0,1,False
228,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueWriter.java,org.logstash.common.io.DeadLetterQueueWriter,boolean alreadyProcessed(Event),"/**
 * Method to determine whether the event has already been processed by the DLQ - currently this
 * just checks the metadata to see if metadata has been added to the event that indicates that
 * it has already gone through the DLQ.
 * TODO: Add metadata around 'depth' to enable >1 iteration through the DLQ if required.
 * @param event Logstash Event
 * @return boolean indicating whether the event is eligible to be added to the DLQ
 */
private static boolean alreadyProcessed(final Event event) {
    return event.includes(DEAD_LETTER_QUEUE_METADATA_KEY);
}","/**
 * Method to determine whether the event has already been processed by the DLQ - currently this
 * just checks the metadata to see if metadata has been added to the event that indicates that
 * it has already gone through the DLQ.
 * TODO: Add metadata around 'depth' to enable >1 iteration through the DLQ if required.
 * @param event Logstash Event
 * @return boolean indicating whether the event is eligible to be added to the DLQ
 */
", ,/** * Method to determine whether the event has already been processed by the DLQ - currently this * just checks the metadata to see if metadata has been added to the event that indicates that * it has already gone through the DLQ. * TODO: Add metadata around 'depth' to enable >1 iteration through the DLQ if required. * @param event Logstash Event * @return boolean indicating whether the event is eligible to be added to the DLQ */,440,442,[1],1,[0],0,[1],1,1,1,1,alreadyProcessed(Event),org.logstash.common.io.DeadLetterQueueWriter,alreadyProcessed/1[org.logstash.Event],False,440,1,2,1,1,1,1,3,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,45,10,0,True
229,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueWriter.java,org.logstash.common.io.DeadLetterQueueWriter,boolean isCurrentWriterStale(),"/**
 * Determines whether the current writer is stale. It is stale if writes have been performed, but the
 * last time it was written is further in the past than the flush interval.
 * @return
 */
private boolean isCurrentWriterStale() {
    return currentWriter.isStale(flushInterval);
}","/**
 * Determines whether the current writer is stale. It is stale if writes have been performed, but the
 * last time it was written is further in the past than the flush interval.
 * @return
 */
", ,"/** * Determines whether the current writer is stale. It is stale if writes have been performed, but the * last time it was written is further in the past than the flush interval. * @return */",457,459,[0],0,[0],0,[0],0,0,0,0,isCurrentWriterStale(),org.logstash.common.io.DeadLetterQueueWriter,isCurrentWriterStale/0,False,457,1,2,1,1,1,1,3,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,2,0,True
230,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueWriter.java,org.logstash.common.io.DeadLetterQueueWriter,void createFlushScheduler(),"private void createFlushScheduler() {
    flushScheduler = Executors.newScheduledThreadPool(1, r -> {
        Thread t = new Thread(r);
        // Allow this thread to die when the JVM dies
        t.setDaemon(true);
        // Set the name
        t.setName(""dlq-flush-check"");
        return t;
    });
    flushScheduler.scheduleAtFixedRate(this::flushCheck, 1L, 1L, TimeUnit.SECONDS);
}", ,"// Allow this thread to die when the JVM dies
[[SEP]]// Set the name
",// Allow this thread to die when the JVM dies[[SEP]]// Set the name,483,493,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,createFlushScheduler(),org.logstash.common.io.DeadLetterQueueWriter,createFlushScheduler/0,False,483,0,1,1,0,1,4,10,1,2,0,4,0,0,0,0,0,0,1,3,2,0,1,0,0,1,8,2,0,False
231,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueWriter.java,org.logstash.common.io.DeadLetterQueueWriter,void cleanupTempFiles(),"// Clean up existing temp files - files with an extension of .log.tmp. Either delete them if an existing
// segment file with the same base name exists, or rename the
// temp file to the segment file, which can happen when a process ends abnormally
private void cleanupTempFiles() throws IOException {
    listFiles(queuePath, "".log.tmp"").forEach(this::cleanupTempFile);
}","// temp file to the segment file, which can happen when a process ends abnormally
", ,"// Clean up existing temp files - files with an extension of .log.tmp. Either delete them if an existing// segment file with the same base name exists, or rename the// temp file to the segment file, which can happen when a process ends abnormally",531,534,[0],0,[0],0,[0],0,0,0,0,cleanupTempFiles(),org.logstash.common.io.DeadLetterQueueWriter,cleanupTempFiles/0,False,531,1,2,1,1,1,2,3,0,0,0,2,0,0,0,0,0,0,1,0,0,0,0,0,0,0,7,2,0,False
232,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueWriter.java,org.logstash.common.io.DeadLetterQueueWriter,void cleanupTempFile(Path),"// check if there is a corresponding .log file - if yes delete the temp file, if no atomic move the
// temp file to be a new segment file..
private void cleanupTempFile(final Path tempFile) {
    String segmentName = tempFile.getFileName().toString().split(""\\."")[0];
    Path segmentFile = queuePath.resolve(String.format(""%s.log"", segmentName));
    try {
        if (Files.exists(segmentFile)) {
            Files.delete(tempFile);
        } else {
            SegmentStatus segmentStatus = RecordIOReader.getSegmentStatus(tempFile);
            switch(segmentStatus) {
                case VALID:
                    logger.debug(""Moving temp file {} to segment file {}"", tempFile, segmentFile);
                    Files.move(tempFile, segmentFile, StandardCopyOption.ATOMIC_MOVE);
                    break;
                case EMPTY:
                    deleteTemporaryFile(tempFile, segmentName);
                    break;
                case INVALID:
                    Path errorFile = queuePath.resolve(String.format(""%s.err"", segmentName));
                    logger.warn(""Segment file {} is in an error state, saving as {}"", segmentFile, errorFile);
                    Files.move(tempFile, errorFile, StandardCopyOption.ATOMIC_MOVE);
                    break;
                default:
                    throw new IllegalStateException(""Unexpected value: "" + RecordIOReader.getSegmentStatus(tempFile));
            }
        }
    } catch (IOException e) {
        throw new IllegalStateException(""Unable to clean up temp file: "" + tempFile, e);
    }
}","// temp file to be a new segment file..
", ,"// check if there is a corresponding .log file - if yes delete the temp file, if no atomic move the// temp file to be a new segment file..",538,567,[0],0,[0],0,[0],0,0,0,0,cleanupTempFile(Path),org.logstash.common.io.DeadLetterQueueWriter,cleanupTempFile/1[java.nio.file.Path],False,538,4,4,0,4,6,12,31,0,4,1,12,1,2,0,0,1,0,7,1,4,2,3,0,0,0,27,2,2,False
233,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\DeadLetterQueueWriter.java,org.logstash.common.io.DeadLetterQueueWriter,"void deleteTemporaryFile(Path, String)","// Windows can leave files in a ""Delete pending"" state, where the file presents as existing to certain
// methods, and not to others, and actively prevents a new file being created with the same file name,
// throwing AccessDeniedException. This method moves the temporary file to a .del file before
// deletion, enabling a new temp file to be created in its place.
private void deleteTemporaryFile(Path tempFile, String segmentName) throws IOException {
    Path deleteTarget;
    if (isWindows()) {
        Path deletedFile = queuePath.resolve(String.format(""%s.del"", segmentName));
        logger.debug(""Moving temp file {} to {}"", tempFile, deletedFile);
        deleteTarget = deletedFile;
        Files.move(tempFile, deletedFile, StandardCopyOption.ATOMIC_MOVE);
    } else {
        deleteTarget = tempFile;
    }
    Files.delete(deleteTarget);
}","// deletion, enabling a new temp file to be created in its place.
", ,"// Windows can leave files in a ""Delete pending"" state, where the file presents as existing to certain// methods, and not to others, and actively prevents a new file being created with the same file name,// throwing AccessDeniedException. This method moves the temporary file to a .del file before// deletion, enabling a new temp file to be created in its place.",573,584,[0],0,[0],0,[1],1,0,0,1,"deleteTemporaryFile(Path, String)",org.logstash.common.io.DeadLetterQueueWriter,"deleteTemporaryFile/2[java.nio.file.Path,java.lang.String]",False,573,2,3,1,2,2,6,13,0,2,2,6,1,1,0,0,0,0,2,0,3,0,1,0,0,0,16,2,1,False
234,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\RecordIOReader.java,org.logstash.common.io.RecordIOReader,void seekToOffset(long),"public void seekToOffset(long channelOffset) throws IOException {
    currentBlock.rewind();
    // align to block boundary and move from that with relative positioning
    long segmentOffset = channelOffset - VERSION_SIZE;
    long blockIndex = segmentOffset / BLOCK_SIZE;
    long blockStartOffset = blockIndex * BLOCK_SIZE;
    channel.position(blockStartOffset + VERSION_SIZE);
    int readBytes = channel.read(currentBlock);
    currentBlockSizeReadFromChannel = readBytes;
    currentBlock.position((int) segmentOffset % BLOCK_SIZE);
    streamPosition = channelOffset;
}", ,"// align to block boundary and move from that with relative positioning
",// align to block boundary and move from that with relative positioning,104,116,[0],0,[0],0,[0],0,0,0,0,seekToOffset(long),org.logstash.common.io.RecordIOReader,seekToOffset/1[long],False,104,0,2,2,0,1,4,11,0,4,1,4,0,0,0,0,0,0,0,0,6,5,0,0,0,0,32,1,0,False
235,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\RecordIOReader.java,org.logstash.common.io.RecordIOReader,"byte[] seekToNextEventPosition(T, Function<byte[], T>, Comparator<T>)","public <T> byte[] seekToNextEventPosition(T target, Function<byte[], T> keyExtractor, Comparator<T> keyComparator) throws IOException {
    int matchingBlock = UNSET;
    int lowBlock = 0;
    // blocks are 0-based so the highest is the number of blocks - 1
    int highBlock = ((int) (channel.size() - VERSION_SIZE) / BLOCK_SIZE) - 1;
    while (lowBlock < highBlock) {
        int middle = (int) Math.ceil((highBlock + lowBlock) / 2.0);
        seekToBlock(middle);
        byte[] readEvent = readEvent();
        // If the event is null, scan from the low block upwards
        if (readEvent == null) {
            matchingBlock = lowBlock;
            break;
        }
        T found = keyExtractor.apply(readEvent);
        int compare = keyComparator.compare(found, target);
        if (compare > 0) {
            highBlock = middle - 1;
        } else if (compare < 0) {
            lowBlock = middle;
        } else {
            matchingBlock = middle;
            break;
        }
    }
    if (matchingBlock == UNSET) {
        matchingBlock = lowBlock;
    }
    // now sequential scan to event
    seekToBlock(matchingBlock);
    int compare = -1;
    byte[] event = null;
    BufferState restorePoint = null;
    while (compare < 0) {
        // Save the buffer state when reading the next event, to restore to if a matching event is found.
        restorePoint = saveBufferState();
        event = readEvent();
        if (event == null) {
            return null;
        }
        compare = keyComparator.compare(keyExtractor.apply(event), target);
    }
    if (restorePoint != null) {
        restoreFrom(restorePoint);
    }
    return event;
}", ,"// blocks are 0-based so the highest is the number of blocks - 1
[[SEP]]// If the event is null, scan from the low block upwards
[[SEP]]// now sequential scan to event
[[SEP]]// Save the buffer state when reading the next event, to restore to if a matching event is found.
","// blocks are 0-based so the highest is the number of blocks - 1[[SEP]]// If the event is null, scan from the low block upwards[[SEP]]// now sequential scan to event[[SEP]]// Save the buffer state when reading the next event, to restore to if a matching event is found.",118,166,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,"seekToNextEventPosition(T, Function<byte[], T>, Comparator<T>)",org.logstash.common.io.RecordIOReader,"seekToNextEventPosition/3[T,java.util.function.Function<byte[],T>,java.util.Comparator<T>]",False,118,4,4,0,4,9,8,45,2,10,3,8,4,6,2,4,0,3,0,8,18,6,2,0,0,0,39,1,0,False
236,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\RecordIOReader.java,org.logstash.common.io.RecordIOReader,void consumeBlock(boolean),"void consumeBlock(boolean rewind) throws IOException {
    if (!rewind && currentBlockSizeReadFromChannel == BLOCK_SIZE) {
        // already read enough, no need to read more
        return;
    }
    if (rewind) {
        currentBlockSizeReadFromChannel = 0;
        currentBlock.rewind();
    }
    currentBlock.mark();
    try {
        // Move to last written to position
        currentBlock.position(currentBlockSizeReadFromChannel);
        channel.read(currentBlock);
        currentBlockSizeReadFromChannel = currentBlock.position();
    } finally {
        currentBlock.reset();
    }
}", ,"// already read enough, no need to read more
[[SEP]]// Move to last written to position
","// already read enough, no need to read more[[SEP]]// Move to last written to position",172,190,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,consumeBlock(boolean),org.logstash.common.io.RecordIOReader,consumeBlock/1[boolean],False,172,0,3,3,0,4,6,18,1,0,1,6,0,0,0,1,1,0,0,1,2,0,1,0,0,0,19,0,0,False
237,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\RecordIOReader.java,org.logstash.common.io.RecordIOReader,boolean isEndOfStream(),"/**
 * basically, is last block
 * @return true if this is the end of the stream
 */
public boolean isEndOfStream() {
    return currentBlockSizeReadFromChannel < BLOCK_SIZE;
}","/**
 * basically, is last block
 * @return true if this is the end of the stream
 */
", ,"/** * basically, is last block * @return true if this is the end of the stream */",196,198,[0],0,[0],0,[0],0,0,0,0,isEndOfStream(),org.logstash.common.io.RecordIOReader,isEndOfStream/0,False,196,0,3,3,0,2,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,27,1,0,True
238,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\RecordIOReader.java,org.logstash.common.io.RecordIOReader,int seekToStartOfEventInBlock(),"/**
 */
int seekToStartOfEventInBlock() {
    // Already consumed all the bytes in this block.
    if (currentBlock.position() >= currentBlockSizeReadFromChannel) {
        return -1;
    }
    while (true) {
        RecordType type = RecordType.fromByte(currentBlock.array()[currentBlock.arrayOffset() + currentBlock.position()]);
        if (type == null) {
            return -1;
        }
        switch(type) {
            case COMPLETE:
            case START:
                return currentBlock.position();
            case MIDDLE:
                return -1;
            case END:
                // reached END record, move forward to the next record in the block if it's present
                RecordHeader header = RecordHeader.get(currentBlock);
                currentBlock.position(currentBlock.position() + header.getSize());
                // If this is the end of stream, then cannot seek to start of block
                if (this.isEndOfStream()) {
                    return -1;
                }
                break;
        }
    }
}","/**
 */
","// Already consumed all the bytes in this block.
[[SEP]]// reached END record, move forward to the next record in the block if it's present
[[SEP]]// If this is the end of stream, then cannot seek to start of block
","/** */[[SEP]]// Already consumed all the bytes in this block.[[SEP]]// reached END record, move forward to the next record in the block if it's present[[SEP]]// If this is the end of stream, then cannot seek to start of block",203,230,[0],0,"[0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,seekToStartOfEventInBlock(),org.logstash.common.io.RecordIOReader,seekToStartOfEventInBlock/0,False,203,3,7,3,4,9,8,25,5,2,0,8,1,1,1,1,0,0,0,4,2,2,3,0,0,0,19,0,0,True
239,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\RecordIOReader.java,org.logstash.common.io.RecordIOReader,boolean consumeToStartOfEvent(),"/**
 * @return true if ready to read event, false otherwise
 */
boolean consumeToStartOfEvent() throws IOException {
    // read and seek to start of event
    consumeBlock(false);
    while (true) {
        int eventStartPosition = seekToStartOfEventInBlock();
        if (eventStartPosition >= 0) {
            return true;
        }
        if (isEndOfStream()) {
            return false;
        }
        consumeBlock(true);
    }
}","/**
 * @return true if ready to read event, false otherwise
 */
","// read and seek to start of event
","/** * @return true if ready to read event, false otherwise */[[SEP]]// read and seek to start of event",236,249,[0],0,[0],0,"[0, 0]",0,0,0,0,consumeToStartOfEvent(),org.logstash.common.io.RecordIOReader,consumeToStartOfEvent/0,False,236,1,4,1,3,4,3,13,2,1,0,3,3,2,1,0,0,0,0,1,1,0,2,0,0,0,22,0,0,True
240,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\RecordIOReader.java,org.logstash.common.io.RecordIOReader,void maybeRollToNextBlock(),"private void maybeRollToNextBlock() throws IOException {
    // check block position state
    if (currentBlock.remaining() < RECORD_HEADER_SIZE + 1) {
        streamPosition = this.channel.position();
        consumeBlock(true);
    }
}", ,"// check block position state
",// check block position state,251,257,[0],0,[0],0,[0],0,0,0,0,maybeRollToNextBlock(),org.logstash.common.io.RecordIOReader,maybeRollToNextBlock/0,False,251,1,2,1,1,2,3,6,0,0,0,3,1,1,0,0,0,0,0,1,1,1,1,0,0,0,21,2,0,False
241,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\RecordIOReader.java,org.logstash.common.io.RecordIOReader,"void getRecord(ByteBuffer, RecordHeader)","private void getRecord(ByteBuffer buffer, RecordHeader header) throws IOException {
    Checksum computedChecksum = new CRC32();
    computedChecksum.update(currentBlock.array(), currentBlock.position(), header.getSize());
    if ((int) computedChecksum.getValue() != header.getChecksum()) {
        throw new IllegalStateException(""invalid checksum of record"");
    }
    buffer.put(currentBlock.array(), currentBlock.position(), header.getSize());
    currentBlock.position(currentBlock.position() + header.getSize());
    if (currentBlock.remaining() < RECORD_HEADER_SIZE + 1) {
        // if the block buffer doesn't contain enough space for another record
        // update position to last channel position.
        streamPosition = channel.position();
    } else {
        streamPosition += header.getSize();
    }
}", ,"// if the block buffer doesn't contain enough space for another record
[[SEP]]// update position to last channel position.
",// if the block buffer doesn't contain enough space for another record// update position to last channel position.,259,276,[0],0,"[0, 0]",0,[0],0,0,0,0,"getRecord(ByteBuffer, RecordHeader)",org.logstash.common.io.RecordIOReader,"getRecord/2[java.nio.ByteBuffer,org.logstash.common.io.RecordHeader]",False,259,1,3,1,2,3,10,15,0,1,2,10,0,0,0,1,0,0,1,1,3,2,1,0,0,0,26,2,0,False
242,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\RecordIOReader.java,org.logstash.common.io.RecordIOReader,SegmentStatus getSegmentStatus(Path),"/**
 * Verify whether a segment is valid and non-empty.
 * @param path Path to segment
 * @return SegmentStatus EMPTY if the segment is empty, VALID if it is valid, INVALID otherwise
 */
static SegmentStatus getSegmentStatus(Path path) {
    try (RecordIOReader ioReader = new RecordIOReader(path)) {
        boolean moreEvents = true;
        SegmentStatus segmentStatus = SegmentStatus.EMPTY;
        while (moreEvents) {
            // If all events in the segment can be read, then assume that this is a valid segment
            moreEvents = (ioReader.readEvent() != null);
            if (moreEvents)
                segmentStatus = SegmentStatus.VALID;
        }
        return segmentStatus;
    } catch (IOException | IllegalStateException e) {
        logger.warn(""Error reading segment file {}"", path, e);
        return SegmentStatus.INVALID;
    }
}","/**
 * Verify whether a segment is valid and non-empty.
 * @param path Path to segment
 * @return SegmentStatus EMPTY if the segment is empty, VALID if it is valid, INVALID otherwise
 */
","// If all events in the segment can be read, then assume that this is a valid segment
","/** * Verify whether a segment is valid and non-empty. * @param path Path to segment * @return SegmentStatus EMPTY if the segment is empty, VALID if it is valid, INVALID otherwise */[[SEP]]// If all events in the segment can be read, then assume that this is a valid segment",380,394,[0],0,[0],0,"[0, 0]",0,0,0,0,getSegmentStatus(Path),org.logstash.common.io.RecordIOReader,getSegmentStatus/1[java.nio.file.Path],False,380,3,8,5,3,4,2,15,2,3,1,2,1,5,1,1,1,1,1,0,5,0,3,0,0,0,42,8,1,True
243,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\SegmentListener.java,org.logstash.common.io.SegmentListener,void segmentCompleted(),"/**
 * Notifies the listener about the complete consumption of a bunch of segments.
 */
void segmentCompleted();","/**
 * Notifies the listener about the complete consumption of a bunch of segments.
 */
", ,/** * Notifies the listener about the complete consumption of a bunch of segments. */,10,10,[0],0,[0],0,[0],0,0,0,0,segmentCompleted(),org.logstash.common.io.SegmentListener,segmentCompleted/0,False,7,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,0,0,True
244,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\common\io\SegmentListener.java,org.logstash.common.io.SegmentListener,"void segmentsDeleted(int, long)","/**
 * Notifies the listener about the deletion of consumed segments.
 * It reports the number of deleted segments and number of events contained in those segments.
 *
 * @param numberOfSegments the number of deleted segment files.
 *
 * @param numberOfEvents total number of events that were present in the deleted segments.
 */
void segmentsDeleted(int numberOfSegments, long numberOfEvents);","/**
 * Notifies the listener about the deletion of consumed segments.
 * It reports the number of deleted segments and number of events contained in those segments.
 *
 * @param numberOfSegments the number of deleted segment files.
 *
 * @param numberOfEvents total number of events that were present in the deleted segments.
 */
", ,/** * Notifies the listener about the deletion of consumed segments. * It reports the number of deleted segments and number of events contained in those segments. * * @param numberOfSegments the number of deleted segment files. * * @param numberOfEvents total number of events that were present in the deleted segments. */,20,20,[0],0,[0],0,[0],0,0,0,0,"segmentsDeleted(int, long)",org.logstash.common.io.SegmentListener,"segmentsDeleted/2[int,long]",False,12,0,1,1,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,0,0,True
245,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline,CompiledPipeline.CompiledExecution buildExecution(),"/**
 * Perform the actual compilation of the {@link Dataset} representing the
 * underlying pipeline from the Queue to the outputs using the
 * unordered  execution model.
 * @return CompiledPipeline.CompiledExecution the compiled pipeline
 */
public CompiledPipeline.CompiledExecution buildExecution() {
    return buildExecution(false);
}","/**
 * Perform the actual compilation of the {@link Dataset} representing the
 * underlying pipeline from the Queue to the outputs using the
 * unordered  execution model.
 * @return CompiledPipeline.CompiledExecution the compiled pipeline
 */
", ,/** * Perform the actual compilation of the {@link Dataset} representing the * underlying pipeline from the Queue to the outputs using the * unordered  execution model. * @return CompiledPipeline.CompiledExecution the compiled pipeline */,142,144,[0],0,[0],0,[0],0,0,0,0,buildExecution(),org.logstash.config.ir.CompiledPipeline,buildExecution/0,False,142,2,16,15,1,1,1,3,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,20,1,0,True
246,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline,CompiledPipeline.CompiledExecution buildExecution(boolean),"/**
 * Perform the actual compilation of the {@link Dataset} representing the
 * underlying pipeline from the Queue to the outputs using the ordered or
 * unordered  execution model.
 * @param orderedExecution determines whether to build an execution that enforces order or not
 * @return CompiledPipeline.CompiledExecution the compiled pipeline
 */
public CompiledPipeline.CompiledExecution buildExecution(boolean orderedExecution) {
    return orderedExecution ? new CompiledPipeline.CompiledOrderedExecution() : new CompiledPipeline.CompiledUnorderedExecution();
}","/**
 * Perform the actual compilation of the {@link Dataset} representing the
 * underlying pipeline from the Queue to the outputs using the ordered or
 * unordered  execution model.
 * @param orderedExecution determines whether to build an execution that enforces order or not
 * @return CompiledPipeline.CompiledExecution the compiled pipeline
 */
", ,/** * Perform the actual compilation of the {@link Dataset} representing the * underlying pipeline from the Queue to the outputs using the ordered or * unordered  execution model. * @param orderedExecution determines whether to build an execution that enforces order or not * @return CompiledPipeline.CompiledExecution the compiled pipeline */,153,157,[0],0,[0],0,[0],0,0,0,0,buildExecution(boolean),org.logstash.config.ir.CompiledPipeline,buildExecution/1[boolean],False,153,3,4,2,2,2,0,3,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,28,1,0,True
247,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline,"Map<String, AbstractOutputDelegatorExt> setupOutputs(ConfigVariableExpander)","/**
 * Sets up all outputs learned from {@link PipelineIR}.
 */
private Map<String, AbstractOutputDelegatorExt> setupOutputs(ConfigVariableExpander cve) {
    final Collection<PluginVertex> outs = pipelineIR.getOutputPluginVertices();
    final Map<String, AbstractOutputDelegatorExt> res = new HashMap<>(outs.size());
    outs.forEach(v -> {
        final PluginDefinition def = v.getPluginDefinition();
        final SourceWithMetadata source = v.getSourceWithMetadata();
        final Map<String, Object> args = expandArguments(def, cve);
        res.put(v.getId(), pluginFactory.buildOutput(RubyUtil.RUBY.newString(def.getName()), convertArgs(args), source));
    });
    return res;
}","/**
 * Sets up all outputs learned from {@link PipelineIR}.
 */
", ,/** * Sets up all outputs learned from {@link PipelineIR}. */,162,174,[0],0,[0],0,[0],0,0,0,0,setupOutputs(ConfigVariableExpander),org.logstash.config.ir.CompiledPipeline,setupOutputs/1[org.logstash.plugins.ConfigVariableExpander],False,162,9,9,1,8,1,12,12,1,6,1,12,2,4,0,0,0,0,0,0,5,0,1,0,0,1,28,2,0,True
248,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline,"Map<String, AbstractFilterDelegatorExt> setupFilters(ConfigVariableExpander)","/**
 * Sets up all Ruby filters learnt from {@link PipelineIR}.
 */
private Map<String, AbstractFilterDelegatorExt> setupFilters(ConfigVariableExpander cve) {
    final Collection<PluginVertex> filterPlugins = pipelineIR.getFilterPluginVertices();
    final Map<String, AbstractFilterDelegatorExt> res = new HashMap<>(filterPlugins.size(), 1.0F);
    for (final PluginVertex vertex : filterPlugins) {
        final PluginDefinition def = vertex.getPluginDefinition();
        final SourceWithMetadata source = vertex.getSourceWithMetadata();
        final Map<String, Object> args = expandArguments(def, cve);
        res.put(vertex.getId(), pluginFactory.buildFilter(RubyUtil.RUBY.newString(def.getName()), convertArgs(args), source));
    }
    return res;
}","/**
 * Sets up all Ruby filters learnt from {@link PipelineIR}.
 */
", ,/** * Sets up all Ruby filters learnt from {@link PipelineIR}. */,179,192,[0],0,[0],0,[0],0,0,0,0,setupFilters(ConfigVariableExpander),org.logstash.config.ir.CompiledPipeline,setupFilters/1[org.logstash.plugins.ConfigVariableExpander],False,179,9,9,1,8,2,11,11,1,5,1,11,2,4,1,0,0,0,0,1,5,0,1,0,0,0,31,2,0,True
249,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline,Collection<IRubyObject> setupInputs(ConfigVariableExpander),"/**
 * Sets up all Ruby inputs learnt from {@link PipelineIR}.
 */
private Collection<IRubyObject> setupInputs(ConfigVariableExpander cve) {
    final Collection<PluginVertex> vertices = pipelineIR.getInputPluginVertices();
    final Collection<IRubyObject> nodes = new HashSet<>(vertices.size());
    vertices.forEach(v -> {
        final PluginDefinition def = v.getPluginDefinition();
        final SourceWithMetadata source = v.getSourceWithMetadata();
        final Map<String, Object> args = expandArguments(def, cve);
        IRubyObject o = pluginFactory.buildInput(RubyUtil.RUBY.newString(def.getName()), convertArgs(args), source);
        nodes.add(o);
    });
    return nodes;
}","/**
 * Sets up all Ruby inputs learnt from {@link PipelineIR}.
 */
", ,/** * Sets up all Ruby inputs learnt from {@link PipelineIR}. */,197,209,[0],0,[0],0,[0],0,0,0,0,setupInputs(ConfigVariableExpander),org.logstash.config.ir.CompiledPipeline,setupInputs/1[org.logstash.plugins.ConfigVariableExpander],False,197,9,8,1,7,1,11,13,1,7,1,11,2,4,0,0,0,0,0,0,6,0,1,0,0,1,31,2,0,True
250,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline,"Map<String, Object> expandArguments(PluginDefinition, ConfigVariableExpander)","private Map<String, Object> expandArguments(final PluginDefinition pluginDefinition, final ConfigVariableExpander cve) {
    Map<String, Object> arguments = expandConfigVariables(cve, pluginDefinition.getArguments());
    // Intercept codec definitions from LIR
    for (final Map.Entry<String, Object> entry : arguments.entrySet()) {
        final String key = entry.getKey();
        final Object value = entry.getValue();
        if (value instanceof PluginStatement) {
            final PluginStatement codecPluginStatement = (PluginStatement) value;
            final PluginDefinition codecDefinition = codecPluginStatement.getPluginDefinition();
            final SourceWithMetadata codecSource = codecPluginStatement.getSourceWithMetadata();
            final Map<String, Object> codecArguments = expandArguments(codecDefinition, cve);
            IRubyObject codecInstance = pluginFactory.buildCodec(RubyUtil.RUBY.newString(codecDefinition.getName()), Rubyfier.deep(RubyUtil.RUBY, codecArguments), codecSource);
            arguments.put(key, codecInstance);
        }
    }
    return arguments;
}", ,"// Intercept codec definitions from LIR
",// Intercept codec definitions from LIR,223,243,[0],0,[0],0,[0],0,0,0,0,"expandArguments(PluginDefinition, ConfigVariableExpander)",org.logstash.config.ir.CompiledPipeline,"expandArguments/2[org.logstash.config.ir.PluginDefinition,org.logstash.plugins.ConfigVariableExpander]",False,223,9,12,4,8,3,13,16,1,8,2,13,2,3,1,0,0,0,0,0,8,0,2,0,0,0,22,2,0,False
251,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline,"Object expandConfigVariable(ConfigVariableExpander, Object, boolean)","@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public static Object expandConfigVariable(ConfigVariableExpander cve, Object valueToExpand, boolean keepSecrets) {
    if (valueToExpand instanceof List) {
        List list = (List) valueToExpand;
        List<Object> expandedObjects = new ArrayList<>();
        for (Object o : list) {
            expandedObjects.add(cve.expand(o, keepSecrets));
        }
        return expandedObjects;
    }
    if (valueToExpand instanceof Map) {
        // hidden recursion here expandConfigVariables -> expandConfigVariable
        return expandConfigVariables(cve, (Map<String, Object>) valueToExpand, keepSecrets);
    }
    if (valueToExpand instanceof String) {
        return cve.expand(valueToExpand, keepSecrets);
    }
    return valueToExpand;
}", ,"// hidden recursion here expandConfigVariables -> expandConfigVariable
",// hidden recursion here expandConfigVariables -> expandConfigVariable,258,276,[0],0,[0],0,[0],0,0,0,0,"expandConfigVariable(ConfigVariableExpander, Object, boolean)",org.logstash.config.ir.CompiledPipeline,"expandConfigVariable/3[org.logstash.plugins.ConfigVariableExpander,java.lang.Object,boolean]",False,259,2,4,2,2,5,3,17,4,2,3,3,1,1,1,0,0,0,2,0,2,0,2,0,0,0,17,9,0,False
252,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline,boolean isFilter(Vertex),"/**
 * Checks if a certain {@link Vertex} represents a {@link AbstractFilterDelegatorExt}.
 * @param vertex Vertex to check
 * @return True iff {@link Vertex} represents a {@link AbstractFilterDelegatorExt}
 */
private boolean isFilter(final Vertex vertex) {
    return filters.containsKey(vertex.getId());
}","/**
 * Checks if a certain {@link Vertex} represents a {@link AbstractFilterDelegatorExt}.
 * @param vertex Vertex to check
 * @return True iff {@link Vertex} represents a {@link AbstractFilterDelegatorExt}
 */
", ,/** * Checks if a certain {@link Vertex} represents a {@link AbstractFilterDelegatorExt}. * @param vertex Vertex to check * @return True iff {@link Vertex} represents a {@link AbstractFilterDelegatorExt} */,287,289,[0],0,[0],0,[0],0,0,0,0,isFilter(Vertex),org.logstash.config.ir.CompiledPipeline,isFilter/1[org.logstash.config.ir.graph.Vertex],False,287,1,3,2,1,1,2,3,1,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,2,0,True
253,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline,boolean isOutput(Vertex),"/**
 * Checks if a certain {@link Vertex} represents an output.
 * @param vertex Vertex to check
 * @return True iff {@link Vertex} represents an output
 */
private boolean isOutput(final Vertex vertex) {
    return outputs.containsKey(vertex.getId());
}","/**
 * Checks if a certain {@link Vertex} represents an output.
 * @param vertex Vertex to check
 * @return True iff {@link Vertex} represents an output
 */
", ,/** * Checks if a certain {@link Vertex} represents an output. * @param vertex Vertex to check * @return True iff {@link Vertex} represents an output */,296,298,[0],0,[0],0,[0],0,0,0,0,isOutput(Vertex),org.logstash.config.ir.CompiledPipeline,isOutput/1[org.logstash.config.ir.graph.Vertex],False,296,1,3,2,1,1,2,3,1,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,2,0,True
254,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline.CompiledOrderedExecution,"int compute(Collection<RubyEvent>, boolean, boolean)","@Override
public int compute(final Collection<RubyEvent> batch, final boolean flush, final boolean shutdown) {
    if (!batch.isEmpty()) {
        @SuppressWarnings({ ""unchecked"" })
        final RubyArray<RubyEvent> outputBatch = RubyUtil.RUBY.newArray();
        @SuppressWarnings({ ""unchecked"" })
        final RubyArray<RubyEvent> filterBatch = RubyUtil.RUBY.newArray(1);
        // send batch one-by-one as single-element batches down the filters
        for (final RubyEvent e : batch) {
            filterBatch.set(0, e);
            _compute(filterBatch, outputBatch, flush, shutdown);
        }
        compiledOutputs.compute(outputBatch, flush, shutdown);
        return outputBatch.size();
    } else if (flush || shutdown) {
        @SuppressWarnings({ ""unchecked"" })
        final RubyArray<RubyEvent> outputBatch = RubyUtil.RUBY.newArray();
        _compute(EMPTY_ARRAY, outputBatch, flush, shutdown);
        compiledOutputs.compute(outputBatch, flush, shutdown);
        return outputBatch.size();
    }
    return 0;
}", ,"// send batch one-by-one as single-element batches down the filters
",// send batch one-by-one as single-element batches down the filters,309,328,[0],0,[0],0,[0],0,0,0,0,"compute(Collection<RubyEvent>, boolean, boolean)",org.logstash.config.ir.CompiledPipeline$CompiledOrderedExecution,"compute/3[java.util.Collection<org.logstash.ext.JrubyEventExtLibrary.RubyEvent>,boolean,boolean]",False,310,4,3,1,2,5,6,19,3,3,3,6,1,1,1,0,0,0,3,3,3,0,2,0,0,0,11,1,0,False
255,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline.CompiledUnorderedExecution,"int compute(Collection<RubyEvent>, boolean, boolean)","@Override
public int compute(final Collection<RubyEvent> batch, final boolean flush, final boolean shutdown) {
    // we know for now this comes from batch.collection() which returns a LinkedHashSet
    final Collection<RubyEvent> result = compiledFilters.compute(RubyArray.newArray(RubyUtil.RUBY, batch), flush, shutdown);
    @SuppressWarnings({ ""unchecked"" })
    final RubyArray<RubyEvent> outputBatch = RubyUtil.RUBY.newArray(result.size());
    copyNonCancelledEvents(result, outputBatch);
    compiledFilters.clear();
    compiledOutputs.compute(outputBatch, flush, shutdown);
    return outputBatch.size();
}", ,"// we know for now this comes from batch.collection() which returns a LinkedHashSet
",// we know for now this comes from batch.collection() which returns a LinkedHashSet,344,353,[0],0,[0],0,[0],0,0,0,0,"compute(Collection<RubyEvent>, boolean, boolean)",org.logstash.config.ir.CompiledPipeline$CompiledUnorderedExecution,"compute/3[java.util.Collection<org.logstash.ext.JrubyEventExtLibrary.RubyEvent>,boolean,boolean]",False,345,4,4,1,3,1,6,8,1,2,3,6,0,0,0,0,0,0,1,0,2,0,0,0,0,0,9,1,0,False
256,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline.CompiledExecution,"int compute(QueueBatch, boolean, boolean)","/**
 * @return the number of events that was processed, could be less o greater than batch.size(), depending if
 *  the pipeline drops or clones events during the filter stage.
 */
public abstract int compute(final QueueBatch batch, final boolean flush, final boolean shutdown);","/**
 * @return the number of events that was processed, could be less o greater than batch.size(), depending if
 *  the pipeline drops or clones events during the filter stage.
 */
", ,"/** * @return the number of events that was processed, could be less o greater than batch.size(), depending if *  the pipeline drops or clones events during the filter stage. */",386,386,[0],0,[0],0,[0],0,0,0,0,"compute(QueueBatch, boolean, boolean)",org.logstash.config.ir.CompiledPipeline$CompiledExecution,"compute/3[org.logstash.execution.QueueBatch,boolean,boolean]",False,382,1,14,14,0,1,0,1,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,24,1025,0,True
257,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline.CompiledExecution,Dataset compileFilters(),"/**
 * Instantiates the graph of compiled filter section {@link Dataset}.
 * @return Compiled {@link Dataset} representing the filter section of the pipeline.
 */
private Dataset compileFilters() {
    final Vertex separator = pipelineIR.getGraph().vertices().filter(v -> v instanceof SeparatorVertex).findFirst().orElseThrow(() -> new IllegalStateException(""Missing Filter End Vertex""));
    return DatasetCompiler.terminalFilterDataset(flatten(Collections.emptyList(), separator));
}","/**
 * Instantiates the graph of compiled filter section {@link Dataset}.
 * @return Compiled {@link Dataset} representing the filter section of the pipeline.
 */
", ,/** * Instantiates the graph of compiled filter section {@link Dataset}. * @return Compiled {@link Dataset} representing the filter section of the pipeline. */,394,401,[0],0,[0],0,[0],0,0,0,0,compileFilters(),org.logstash.config.ir.CompiledPipeline$CompiledExecution,compileFilters/0,False,394,7,5,1,4,1,8,4,1,2,0,8,1,3,0,0,0,0,1,0,1,0,0,0,0,2,22,2,0,True
258,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline.CompiledExecution,Dataset compileOutputs(),"/**
 * Instantiates the graph of compiled output section {@link Dataset}.
 * @return Compiled {@link Dataset} representing the output section of the pipeline.
 */
private Dataset compileOutputs() {
    final Collection<Vertex> outputNodes = pipelineIR.getGraph().allLeaves().filter(CompiledPipeline.this::isOutput).collect(Collectors.toList());
    if (outputNodes.isEmpty()) {
        return Dataset.IDENTITY;
    } else {
        return DatasetCompiler.terminalOutputDataset(outputNodes.stream().map(leaf -> outputDataset(leaf, flatten(Collections.emptyList(), leaf))).collect(Collectors.toList()));
    }
}","/**
 * Instantiates the graph of compiled output section {@link Dataset}.
 * @return Compiled {@link Dataset} representing the output section of the pipeline.
 */
", ,/** * Instantiates the graph of compiled output section {@link Dataset}. * @return Compiled {@link Dataset} representing the output section of the pipeline. */,407,418,[0],0,[0],0,[0],0,0,0,0,compileOutputs(),org.logstash.config.ir.CompiledPipeline$CompiledExecution,compileOutputs/0,False,407,6,6,1,5,2,13,9,2,2,0,13,2,4,0,0,0,0,0,0,1,0,1,0,0,1,14,2,0,True
259,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline.CompiledExecution,"Dataset filterDataset(Vertex, Collection<Dataset>)","/**
 * Build a {@link Dataset} representing the {@link RubyEvent}s after
 * the application of the given filter.
 * @param vertex Vertex of the filter to create this {@link Dataset} for
 * @param datasets All the datasets that have children passing into this filter
 * @return Filter {@link Dataset}
 */
private Dataset filterDataset(final Vertex vertex, final Collection<Dataset> datasets) {
    final String vertexId = vertex.getId();
    if (!plugins.containsKey(vertexId)) {
        final ComputeStepSyntaxElement<Dataset> prepared = DatasetCompiler.filterDataset(flatten(datasets, vertex), filters.get(vertexId));
        LOGGER.debug(""Compiled filter\n {} \n into \n {}"", vertex, prepared);
        plugins.put(vertexId, prepared.instantiate());
    }
    return plugins.get(vertexId);
}","/**
 * Build a {@link Dataset} representing the {@link RubyEvent}s after
 * the application of the given filter.
 * @param vertex Vertex of the filter to create this {@link Dataset} for
 * @param datasets All the datasets that have children passing into this filter
 * @return Filter {@link Dataset}
 */
", ,/** * Build a {@link Dataset} representing the {@link RubyEvent}s after * the application of the given filter. * @param vertex Vertex of the filter to create this {@link Dataset} for * @param datasets All the datasets that have children passing into this filter * @return Filter {@link Dataset} */,427,442,[0],0,[0],0,[0],0,0,0,0,"filterDataset(Vertex, Collection<Dataset>)",org.logstash.config.ir.CompiledPipeline$CompiledExecution,"filterDataset/2[org.logstash.config.ir.graph.Vertex,java.util.Collection<org.logstash.config.ir.compiler.Dataset>]",False,427,6,6,1,5,2,9,9,1,2,2,9,1,3,0,0,0,0,1,0,2,0,1,0,0,0,28,2,1,True
260,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline.CompiledExecution,"Dataset outputDataset(Vertex, Collection<Dataset>)","/**
 * Build a {@link Dataset} representing the {@link RubyEvent}s after
 * the application of the given output.
 * @param vertex Vertex of the output to create this {@link Dataset} for
 * @param datasets All the datasets that have children passing into this output
 * @return Output {@link Dataset}
 */
private Dataset outputDataset(final Vertex vertex, final Collection<Dataset> datasets) {
    final String vertexId = vertex.getId();
    if (!plugins.containsKey(vertexId)) {
        final ComputeStepSyntaxElement<Dataset> prepared = DatasetCompiler.outputDataset(flatten(datasets, vertex), outputs.get(vertexId), outputs.size() == 1);
        LOGGER.debug(""Compiled output\n {} \n into \n {}"", vertex, prepared);
        plugins.put(vertexId, prepared.instantiate());
    }
    return plugins.get(vertexId);
}","/**
 * Build a {@link Dataset} representing the {@link RubyEvent}s after
 * the application of the given output.
 * @param vertex Vertex of the output to create this {@link Dataset} for
 * @param datasets All the datasets that have children passing into this output
 * @return Output {@link Dataset}
 */
", ,/** * Build a {@link Dataset} representing the {@link RubyEvent}s after * the application of the given output. * @param vertex Vertex of the output to create this {@link Dataset} for * @param datasets All the datasets that have children passing into this output * @return Output {@link Dataset} */,451,467,[0],0,[0],0,[0],0,0,0,0,"outputDataset(Vertex, Collection<Dataset>)",org.logstash.config.ir.CompiledPipeline$CompiledExecution,"outputDataset/2[org.logstash.config.ir.graph.Vertex,java.util.Collection<org.logstash.config.ir.compiler.Dataset>]",False,451,6,7,2,5,2,10,9,1,2,2,10,1,3,0,1,0,0,1,1,2,0,1,0,0,0,28,2,1,True
261,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline.CompiledExecution,"SplitDataset split(Collection<Dataset>, EventCondition, Vertex)","/**
 * Split the given {@link Dataset}s and return the dataset half of their elements that contains
 * the {@link RubyEvent} that fulfil the given {@link EventCondition}.
 * @param datasets Datasets that are the parents of the datasets to split
 * @param condition Condition that must be fulfilled
 * @param vertex Vertex id to cache the resulting {@link Dataset} under
 * @return The half of the datasets contents that fulfils the condition
 */
private SplitDataset split(final Collection<Dataset> datasets, final EventCondition condition, final Vertex vertex) {
    final String vertexId = vertex.getId();
    SplitDataset conditional = iffs.get(vertexId);
    if (conditional == null) {
        final Collection<Dataset> dependencies = flatten(datasets, vertex);
        conditional = iffs.get(vertexId);
        // Check that compiling the dependencies did not already instantiate the conditional
        // by requiring its else branch.
        if (conditional == null) {
            final ComputeStepSyntaxElement<SplitDataset> prepared = DatasetCompiler.splitDataset(dependencies, condition);
            LOGGER.debug(""Compiled conditional\n {} \n into \n {}"", vertex, prepared);
            conditional = prepared.instantiate();
            iffs.put(vertexId, conditional);
        }
    }
    return conditional;
}","/**
 * Split the given {@link Dataset}s and return the dataset half of their elements that contains
 * the {@link RubyEvent} that fulfil the given {@link EventCondition}.
 * @param datasets Datasets that are the parents of the datasets to split
 * @param condition Condition that must be fulfilled
 * @param vertex Vertex id to cache the resulting {@link Dataset} under
 * @return The half of the datasets contents that fulfils the condition
 */
","// Check that compiling the dependencies did not already instantiate the conditional
[[SEP]]// by requiring its else branch.
",/** * Split the given {@link Dataset}s and return the dataset half of their elements that contains * the {@link RubyEvent} that fulfil the given {@link EventCondition}. * @param datasets Datasets that are the parents of the datasets to split * @param condition Condition that must be fulfilled * @param vertex Vertex id to cache the resulting {@link Dataset} under * @return The half of the datasets contents that fulfils the condition */[[SEP]]// Check that compiling the dependencies did not already instantiate the conditional// by requiring its else branch.,477,500,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,"split(Collection<Dataset>, EventCondition, Vertex)",org.logstash.config.ir.CompiledPipeline$CompiledExecution,"split/3[java.util.Collection<org.logstash.config.ir.compiler.Dataset>,org.logstash.config.ir.compiler.EventCondition,org.logstash.config.ir.graph.Vertex]",False,481,8,6,1,5,3,7,15,1,4,3,7,1,3,0,2,0,0,1,0,6,0,2,0,0,0,43,2,1,True
262,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline.CompiledExecution,"Collection<Dataset> flatten(Collection<Dataset>, Vertex)","/**
 * Compiles the next level of the execution from the given {@link Vertex} or simply return
 * the given {@link Dataset} at the previous level if the starting {@link Vertex} cannot
 * be expanded any further (i.e. doesn't have any more incoming vertices that are either
 * a {code filter} or and {code if} statement).
 * @param datasets Nodes from the last already compiled level
 * @param start Vertex to compile children for
 * @return Datasets originating from given {@link Vertex}
 */
private Collection<Dataset> flatten(final Collection<Dataset> datasets, final Vertex start) {
    final Collection<Dataset> result = compileDependencies(start, datasets, start.incomingVertices().filter(v -> isFilter(v) || isOutput(v) || v instanceof IfVertex));
    return result.isEmpty() ? datasets : result;
}","/**
 * Compiles the next level of the execution from the given {@link Vertex} or simply return
 * the given {@link Dataset} at the previous level if the starting {@link Vertex} cannot
 * be expanded any further (i.e. doesn't have any more incoming vertices that are either
 * a {code filter} or and {code if} statement).
 * @param datasets Nodes from the last already compiled level
 * @param start Vertex to compile children for
 * @return Datasets originating from given {@link Vertex}
 */
", ,/** * Compiles the next level of the execution from the given {@link Vertex} or simply return * the given {@link Dataset} at the previous level if the starting {@link Vertex} cannot * be expanded any further (i.e. doesn't have any more incoming vertices that are either * a {code filter} or and {code if} statement). * @param datasets Nodes from the last already compiled level * @param start Vertex to compile children for * @return Datasets originating from given {@link Vertex} */,511,521,[0],0,[0],0,[0],0,0,0,0,"flatten(Collection<Dataset>, Vertex)",org.logstash.config.ir.CompiledPipeline$CompiledExecution,"flatten/2[java.util.Collection<org.logstash.config.ir.compiler.Dataset>,org.logstash.config.ir.graph.Vertex]",False,514,5,9,5,4,2,6,4,1,2,2,6,1,2,0,0,0,0,0,0,1,0,0,0,0,1,51,2,0,True
263,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\CompiledPipeline.java,org.logstash.config.ir.CompiledPipeline.CompiledExecution,"Collection<Dataset> compileDependencies(Vertex, Collection<Dataset>, Stream<Vertex>)","/**
 * Compiles all child vertices for a given vertex.
 * @param datasets Datasets from previous stage
 * @param start Start Vertex that got expanded
 * @param dependencies Dependencies of {@code start}
 * @return Datasets compiled from vertex children
 */
private Collection<Dataset> compileDependencies(final Vertex start, final Collection<Dataset> datasets, final Stream<Vertex> dependencies) {
    return dependencies.map(dependency -> {
        if (isFilter(dependency)) {
            return filterDataset(dependency, datasets);
        } else if (isOutput(dependency)) {
            return outputDataset(dependency, datasets);
        } else {
            // We know that it's an if vertex since the input children are either
            // output, filter or if in type.
            final IfVertex ifvert = (IfVertex) dependency;
            final SplitDataset ifDataset = split(datasets, conditionalCompiler.buildCondition(ifvert.getBooleanExpression()), dependency);
            // It is important that we double check that we are actually dealing with the
            // positive/left branch of the if condition
            if (ifvert.outgoingBooleanEdgesByType(true).anyMatch(edge -> Objects.equals(edge.getTo(), start))) {
                return ifDataset;
            } else {
                return ifDataset.right();
            }
        }
    }).collect(Collectors.toList());
}","/**
 * Compiles all child vertices for a given vertex.
 * @param datasets Datasets from previous stage
 * @param start Start Vertex that got expanded
 * @param dependencies Dependencies of {@code start}
 * @return Datasets compiled from vertex children
 */
","// We know that it's an if vertex since the input children are either
[[SEP]]// It is important that we double check that we are actually dealing with the
[[SEP]]// output, filter or if in type.
[[SEP]]// positive/left branch of the if condition
","/** * Compiles all child vertices for a given vertex. * @param datasets Datasets from previous stage * @param start Start Vertex that got expanded * @param dependencies Dependencies of {@code start} * @return Datasets compiled from vertex children */[[SEP]]// We know that it's an if vertex since the input children are either// output, filter or if in type.[[SEP]]// It is important that we double check that we are actually dealing with the// positive/left branch of the if condition",530,562,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,"compileDependencies(Vertex, Collection<Dataset>, Stream<Vertex>)",org.logstash.config.ir.CompiledPipeline$CompiledExecution,"compileDependencies/3[org.logstash.config.ir.graph.Vertex,java.util.Collection<org.logstash.config.ir.compiler.Dataset>,java.util.stream.Stream<org.logstash.config.ir.graph.Vertex>]",False,534,8,11,1,10,4,15,21,5,4,3,15,3,4,0,0,0,0,0,0,2,0,3,0,0,2,38,2,0,True
264,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\ConfigCompiler.java,org.logstash.config.ir.ConfigCompiler,"PipelineIR configToPipelineIR(List<SourceWithMetadata>, boolean, ConfigVariableExpander)","/**
 * @param sourcesWithMetadata Logstash Config partitioned
 * @param supportEscapes The value of the setting {@code config.support_escapes}
 * @param cve Config variable expander. Substitute variable with value in secret store, env, default config value
 * @return Compiled {@link PipelineIR}
 * @throws InvalidIRException if the the configuration contains errors
 */
@SuppressWarnings(""unchecked"")
public static PipelineIR configToPipelineIR(final List<SourceWithMetadata> sourcesWithMetadata, final boolean supportEscapes, ConfigVariableExpander cve) throws InvalidIRException {
    return compileSources(sourcesWithMetadata, supportEscapes, cve);
}","/**
 * @param sourcesWithMetadata Logstash Config partitioned
 * @param supportEscapes The value of the setting {@code config.support_escapes}
 * @param cve Config variable expander. Substitute variable with value in secret store, env, default config value
 * @return Compiled {@link PipelineIR}
 * @throws InvalidIRException if the the configuration contains errors
 */
", ,"/** * @param sourcesWithMetadata Logstash Config partitioned * @param supportEscapes The value of the setting {@code config.support_escapes} * @param cve Config variable expander. Substitute variable with value in secret store, env, default config value * @return Compiled {@link PipelineIR} * @throws InvalidIRException if the the configuration contains errors */",56,60,[0],0,[0],0,[0],0,0,0,0,"configToPipelineIR(List<SourceWithMetadata>, boolean, ConfigVariableExpander)",org.logstash.config.ir.ConfigCompiler,"configToPipelineIR/3[java.util.List<org.logstash.common.SourceWithMetadata>,boolean,org.logstash.plugins.ConfigVariableExpander]",False,58,4,19,18,1,1,1,3,1,0,3,1,1,4,0,0,0,0,1,0,0,0,0,0,0,0,32,9,0,True
265,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\ConfigCompiler.java,org.logstash.config.ir.ConfigCompiler,"Map<PluginDefinition.Type, Statement> compileImperative(SourceWithMetadata, boolean)","private static Map<PluginDefinition.Type, Statement> compileImperative(SourceWithMetadata sourceWithMetadata, boolean supportEscapes) {
    final IRubyObject compiler = RubyUtil.RUBY.executeScript(""require 'logstash/compiler'\nLogStash::Compiler"", """");
    // invoke Ruby interpreter to execute LSCL treetop
    final IRubyObject code = compiler.callMethod(RubyUtil.RUBY.getCurrentContext(), ""compile_imperative"", new IRubyObject[] { JavaUtil.convertJavaToRuby(RubyUtil.RUBY, sourceWithMetadata), RubyUtil.RUBY.newBoolean(supportEscapes) });
    RubyHash hash = (RubyHash) code;
    Map<PluginDefinition.Type, Statement> result = new HashMap<>();
    result.put(PluginDefinition.Type.INPUT, readStatementFromRubyHash(hash, ""input""));
    result.put(PluginDefinition.Type.FILTER, readStatementFromRubyHash(hash, ""filter""));
    result.put(PluginDefinition.Type.OUTPUT, readStatementFromRubyHash(hash, ""output""));
    return result;
}", ,"// invoke Ruby interpreter to execute LSCL treetop
",// invoke Ruby interpreter to execute LSCL treetop,87,105,[0],0,[0],0,[0],0,0,0,0,"compileImperative(SourceWithMetadata, boolean)",org.logstash.config.ir.ConfigCompiler,"compileImperative/2[org.logstash.common.SourceWithMetadata,boolean]",False,88,6,2,1,1,1,7,10,1,4,2,7,1,1,0,0,0,0,6,0,4,0,0,0,0,0,15,10,0,False
266,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\DSL.java,org.logstash.config.ir.DSL,ValueExpression eValue(long),"public static ValueExpression eValue(long value) {
    try {
        return eValue(null, value);
    } catch (InvalidIRException e) {
        // Can't happen with an int
        e.printStackTrace();
        return null;
    }
}", ,"// Can't happen with an int
",// Can't happen with an int,83,90,[0],0,[0],0,[0],0,0,0,0,eValue(long),org.logstash.config.ir.DSL,eValue/1[long],False,83,2,7,6,1,2,2,9,2,0,1,2,1,1,0,0,1,0,0,0,0,0,1,0,0,0,9,9,0,False
267,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\DSL.java,org.logstash.config.ir.DSL,ValueExpression eValue(double),"public static ValueExpression eValue(double value) {
    try {
        return eValue(null, value);
    } catch (InvalidIRException e) {
        // Can't happen with an int
        e.printStackTrace();
        return null;
    }
}", ,"// Can't happen with an int
",// Can't happen with an int,92,99,[0],0,[0],0,[0],0,0,0,0,eValue(double),org.logstash.config.ir.DSL,eValue/1[double],False,92,2,1,0,1,2,2,9,2,0,1,2,1,1,0,0,1,0,0,0,0,0,1,0,0,0,9,9,0,False
268,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\DSL.java,org.logstash.config.ir.DSL,NoopStatement noop(),"public static NoopStatement noop() {
    try {
        SourceWithMetadata meta = new SourceWithMetadata(""internal"", ""noop"", 0, 0, UUID.randomUUID().toString());
    } catch (IncompleteSourceWithMetadataException e) {
        // Should never happen
        throw new RuntimeException(""Noop could not instantiate metadata, this should never happen"");
    }
    return new NoopStatement(null);
}", ,"// Should never happen
",// Should never happen,243,251,[0],0,[0],0,[0],0,0,0,0,noop(),org.logstash.config.ir.DSL,noop/0,False,243,2,4,2,2,2,2,9,1,1,0,2,0,0,0,0,1,0,3,2,1,0,1,0,0,0,17,9,0,False
269,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\PipelineConfig.java,org.logstash.config.ir.PipelineConfig,String configString(),"public String configString() {
    if (this.configString == null) {
        synchronized (this) {
            if (this.configString == null) {
                final StringBuilder compositeConfig = new StringBuilder();
                for (SourceWithMetadata confPart : confParts) {
                    // If our composite config ends without a trailing newline,
                    // append one before appending the next config part
                    if (compositeConfig.lastIndexOf(NEWLINE) < compositeConfig.length() - 1) {
                        compositeConfig.append(NEWLINE);
                    }
                    compositeConfig.append(confPart.getText());
                }
                this.configString = compositeConfig.toString();
            }
        }
    }
    return this.configString;
}", ,"// If our composite config ends without a trailing newline,
[[SEP]]// append one before appending the next config part
","// If our composite config ends without a trailing newline,// append one before appending the next config part",113,131,[0],0,"[0, 0]",0,[0],0,0,0,0,configString(),org.logstash.config.ir.PipelineConfig,configString/0,False,113,1,4,3,1,5,5,17,1,1,0,5,0,0,1,2,0,0,0,1,2,1,5,0,0,0,17,1,0,False
270,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\PipelineConfig.java,org.logstash.config.ir.PipelineConfig,List<LineToSource> sourceReferences(),"private List<LineToSource> sourceReferences() {
    if (this.sourceRefs == null) {
        int offset = 0;
        List<LineToSource> sourceRefs = new ArrayList<>();
        for (SourceWithMetadata configPart : confParts) {
            // line numbers starts from 1 in text files
            int startLine = configPart.getLine() + offset + 1;
            int endLine = configPart.getLinesCount() + offset;
            LineToSource sourceSegment = new LineToSource(startLine, endLine, configPart);
            sourceRefs.add(sourceSegment);
            offset += configPart.getLinesCount();
        }
        this.sourceRefs = Collections.unmodifiableList(sourceRefs);
    }
    return this.sourceRefs;
}", ,"// line numbers starts from 1 in text files
",// line numbers starts from 1 in text files,188,204,[0],0,[0],0,[0],0,0,0,0,sourceReferences(),org.logstash.config.ir.PipelineConfig,sourceReferences/0,False,188,2,4,1,3,3,4,15,1,5,0,4,0,0,1,1,0,0,0,2,7,2,2,0,0,0,17,2,0,False
271,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\PipelineIR.java,org.logstash.config.ir.PipelineIR,Stream<PluginVertex> pluginVertices(PluginDefinition.Type),"// Return plugin vertices by type
public Stream<PluginVertex> pluginVertices(PluginDefinition.Type type) {
    return pluginVertices().filter(v -> v.getPluginDefinition().getType().equals(type));
}","// Return plugin vertices by type
", ,// Return plugin vertices by type,111,114,[0],0,[0],0,[0],0,0,0,0,pluginVertices(Type),org.logstash.config.ir.PipelineIR,pluginVertices/1[org.logstash.config.ir.PluginDefinition.Type],False,111,4,4,1,3,1,5,3,1,1,1,5,1,1,0,0,0,0,0,0,0,0,0,0,0,1,4,1,0,False
272,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\PipelineIR.java,org.logstash.config.ir.PipelineIR,List<PluginVertex> getPluginVertices(PluginDefinition.Type),"// Return plugin vertices by type
public List<PluginVertex> getPluginVertices(PluginDefinition.Type type) {
    return pluginVertices(type).collect(Collectors.toList());
}","// Return plugin vertices by type
", ,// Return plugin vertices by type,117,119,[0],0,[0],0,[0],0,0,0,0,getPluginVertices(Type),org.logstash.config.ir.PipelineIR,getPluginVertices/1[org.logstash.config.ir.PluginDefinition.Type],False,117,3,4,3,1,1,3,3,1,0,1,3,1,2,0,0,0,0,0,0,0,0,0,0,0,0,5,1,0,False
273,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\PluginDefinition.java,org.logstash.config.ir.PluginDefinition,boolean sourceComponentEquals(SourceComponent),"@Override
public boolean sourceComponentEquals(SourceComponent o) {
    if (o == null)
        return false;
    if (o instanceof PluginDefinition) {
        PluginDefinition oPluginDefinition = (PluginDefinition) o;
        Set<String> allArgs = new HashSet<>();
        allArgs.addAll(getArguments().keySet());
        allArgs.addAll(oPluginDefinition.getArguments().keySet());
        // Compare all arguments except the unique id
        boolean argsMatch = allArgs.stream().filter(k -> !k.equals(""id"")).allMatch(k -> Objects.equals(getArguments().get(k), oPluginDefinition.getArguments().get(k)));
        return argsMatch && type.equals(oPluginDefinition.type) && name.equals(oPluginDefinition.name);
    }
    return false;
}", ,"// Compare all arguments except the unique id
",// Compare all arguments except the unique id,94,113,[0],0,[0],0,[0],0,0,0,0,sourceComponentEquals(SourceComponent),org.logstash.config.ir.PluginDefinition,sourceComponentEquals/1[org.logstash.config.ir.SourceComponent],False,95,2,2,1,1,3,10,12,3,5,1,10,1,1,0,1,0,0,1,0,3,0,1,0,0,2,16,1,0,False
274,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\ClassFields.java,org.logstash.config.ir.compiler.ClassFields,ValueSyntaxElement add(Object),"/**
 * Adds a field holding the given {@link Object}.
 * @param obj Object to add field for
 * @return The field's syntax element that can be used in method bodies
 */
public ValueSyntaxElement add(final Object obj) {
    return addField(FieldDefinition.fromValue(definitions.size(), obj));
}","/**
 * Adds a field holding the given {@link Object}.
 * @param obj Object to add field for
 * @return The field's syntax element that can be used in method bodies
 */
", ,/** * Adds a field holding the given {@link Object}. * @param obj Object to add field for * @return The field's syntax element that can be used in method bodies */,42,44,[0],0,[0],0,[0],0,0,0,0,add(Object),org.logstash.config.ir.compiler.ClassFields,add/1[java.lang.Object],False,42,3,7,5,2,1,3,3,1,0,1,3,1,1,0,0,0,0,0,0,0,0,0,0,0,0,24,1,0,True
275,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\ClassFields.java,org.logstash.config.ir.compiler.ClassFields,ValueSyntaxElement add(Class<?>),"/**
 * Adds a mutable field of the given type, that doesn't have a default value and is not
 * initialized by a constructor assignment.
 * Renders as e.g. {@code private boolean field7}
 * @param type Type of the mutable field.
 * @return The field's syntax element that can be used in method bodies
 */
public ValueSyntaxElement add(final Class<?> type) {
    return addField(FieldDefinition.mutableUnassigned(definitions.size(), type));
}","/**
 * Adds a mutable field of the given type, that doesn't have a default value and is not
 * initialized by a constructor assignment.
 * Renders as e.g. {@code private boolean field7}
 * @param type Type of the mutable field.
 * @return The field's syntax element that can be used in method bodies
 */
", ,"/** * Adds a mutable field of the given type, that doesn't have a default value and is not * initialized by a constructor assignment. * Renders as e.g. {@code private boolean field7} * @param type Type of the mutable field. * @return The field's syntax element that can be used in method bodies */",53,55,[0],0,[0],0,[0],0,0,0,0,add(Class<?>),org.logstash.config.ir.compiler.ClassFields,add/1[java.lang.Class<?>],False,53,3,7,5,2,1,3,3,1,0,1,3,1,1,0,0,0,0,0,0,0,0,0,0,0,0,35,1,0,True
276,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\ClassFields.java,org.logstash.config.ir.compiler.ClassFields,void addAfterInit(Closure),"/**
 * Add a {@link Closure} that should be executed in the constructor after field assignments
 * have been executed.
 * @param closure Closure to run after field assignments
 */
public void addAfterInit(final Closure closure) {
    afterInit.add(closure);
}","/**
 * Add a {@link Closure} that should be executed in the constructor after field assignments
 * have been executed.
 * @param closure Closure to run after field assignments
 */
", ,/** * Add a {@link Closure} that should be executed in the constructor after field assignments * have been executed. * @param closure Closure to run after field assignments */,62,64,[0],0,[0],0,[0],0,0,0,0,addAfterInit(Closure),org.logstash.config.ir.compiler.ClassFields,addAfterInit/1[org.logstash.config.ir.compiler.Closure],False,62,1,1,1,0,1,1,3,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,21,1,0,True
277,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\ClassFields.java,org.logstash.config.ir.compiler.ClassFields,Closure afterInit(),"/**
 * Returns a closure of actions that should be run in the constructor after all field
 * assignments have been executed.
 * @return Closure that should be executed after field assignments are done
 */
public Closure afterInit() {
    return Closure.wrap(afterInit.toArray(new Closure[0]));
}","/**
 * Returns a closure of actions that should be run in the constructor after all field
 * assignments have been executed.
 * @return Closure that should be executed after field assignments are done
 */
", ,/** * Returns a closure of actions that should be run in the constructor after all field * assignments have been executed. * @return Closure that should be executed after field assignments are done */,71,73,[0],0,[0],0,[0],0,0,0,0,afterInit(),org.logstash.config.ir.compiler.ClassFields,afterInit/0,False,71,1,2,1,1,1,2,3,1,0,0,2,0,0,0,0,0,0,0,1,0,0,0,0,0,0,23,1,0,True
278,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\ClassFields.java,org.logstash.config.ir.compiler.ClassFields,FieldDeclarationGroup ctorAssigned(),"/**
 * Returns the subset of fields that are assigned in the constructor.
 * @return Subset of fields to be assigned by the constructor
 */
public FieldDeclarationGroup ctorAssigned() {
    return new FieldDeclarationGroup(definitions.stream().filter(field -> field.getCtorArgument() != null).collect(Collectors.toList()));
}","/**
 * Returns the subset of fields that are assigned in the constructor.
 * @return Subset of fields to be assigned by the constructor
 */
", ,/** * Returns the subset of fields that are assigned in the constructor. * @return Subset of fields to be assigned by the constructor */,79,84,[0],0,[0],0,[0],0,0,0,0,ctorAssigned(),org.logstash.config.ir.compiler.ClassFields,ctorAssigned/0,False,79,2,4,2,2,2,5,3,1,1,0,5,0,0,0,1,0,0,0,0,0,0,0,0,0,1,21,1,0,True
279,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\ClassFields.java,org.logstash.config.ir.compiler.ClassFields,FieldDeclarationGroup inlineAssigned(),"/**
 * Returns the subset of fields that are not assigned in the constructor.
 * They are either mutable without a default value or assigned inline in the class body.
 * @return Subset of fields not assigned by the constructor
 */
public FieldDeclarationGroup inlineAssigned() {
    return new FieldDeclarationGroup(definitions.stream().filter(field -> field.getCtorArgument() == null).collect(Collectors.toList()));
}","/**
 * Returns the subset of fields that are not assigned in the constructor.
 * They are either mutable without a default value or assigned inline in the class body.
 * @return Subset of fields not assigned by the constructor
 */
", ,/** * Returns the subset of fields that are not assigned in the constructor. * They are either mutable without a default value or assigned inline in the class body. * @return Subset of fields not assigned by the constructor */,91,96,[0],0,[0],0,[0],0,0,0,0,inlineAssigned(),org.logstash.config.ir.compiler.ClassFields,inlineAssigned/0,False,91,2,3,1,2,2,5,3,1,1,0,5,0,0,0,1,0,0,0,0,0,0,0,0,0,1,27,1,0,True
280,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\CommonActions.java,org.logstash.config.ir.compiler.CommonActions,"Map<String, Object> addField(Map<String, Object>, Map<String, Object>)","/**
 * Implements the {@code add_field} option for Logstash inputs.
 *
 * @param event       Event on which to add the fields.
 * @param fieldsToAdd The fields to be added to the event.
 * @return Updated event.
 */
static Map<String, Object> addField(Map<String, Object> event, Map<String, Object> fieldsToAdd) {
    Event tempEvent = new org.logstash.Event(event);
    addField(tempEvent, fieldsToAdd);
    return tempEvent.getData();
}","/**
 * Implements the {@code add_field} option for Logstash inputs.
 *
 * @param event       Event on which to add the fields.
 * @param fieldsToAdd The fields to be added to the event.
 * @return Updated event.
 */
", ,/** * Implements the {@code add_field} option for Logstash inputs. * * @param event       Event on which to add the fields. * @param fieldsToAdd The fields to be added to the event. * @return Updated event. */,89,93,[0],0,[0],0,[0],0,0,0,0,"addField(Map<String, Object>, Map<String, Object>)",org.logstash.config.ir.compiler.CommonActions,"addField/2[java.util.Map<java.lang.String,java.lang.Object>,java.util.Map<java.lang.String,java.lang.Object>]",False,89,3,4,1,3,1,2,5,1,1,2,2,1,1,0,0,0,0,0,0,1,0,0,0,0,0,20,8,0,True
281,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\CommonActions.java,org.logstash.config.ir.compiler.CommonActions,"void addField(Event, Map<String, Object>)","/**
 * Implements the {@code add_field} option for Logstash filters.
 *
 * @param evt         Event on which to add the fields.
 * @param fieldsToAdd The fields to be added to the event.
 */
@SuppressWarnings(""unchecked"")
static void addField(Event evt, Map<String, Object> fieldsToAdd) {
    try {
        for (Map.Entry<String, Object> entry : fieldsToAdd.entrySet()) {
            String keyToSet = StringInterpolation.evaluate(evt, entry.getKey());
            Object val = evt.getField(keyToSet);
            Object valueToSet = entry.getValue();
            valueToSet = valueToSet instanceof String ? StringInterpolation.evaluate(evt, (String) entry.getValue()) : entry.getValue();
            if (val == null) {
                evt.setField(keyToSet, valueToSet);
            } else {
                if (val instanceof List) {
                    ((List) val).add(valueToSet);
                    evt.setField(keyToSet, val);
                } else {
                    @SuppressWarnings(""rawtypes"")
                    RubyArray list = RubyArray.newArray(RubyUtil.RUBY, 2);
                    list.add(val);
                    list.add(valueToSet);
                    evt.setField(keyToSet, list);
                }
            }
        }
    } catch (IOException ex) {
        throw new IllegalStateException(ex);
    }
}","/**
 * Implements the {@code add_field} option for Logstash filters.
 *
 * @param evt         Event on which to add the fields.
 * @param fieldsToAdd The fields to be added to the event.
 */
", ,/** * Implements the {@code add_field} option for Logstash filters. * * @param evt         Event on which to add the fields. * @param fieldsToAdd The fields to be added to the event. */,101,129,[0],0,[0],0,[0],0,0,0,0,"addField(Event, Map<String, Object>)",org.logstash.config.ir.compiler.CommonActions,"addField/2[co.elastic.logstash.api.Event,java.util.Map<java.lang.String,java.lang.Object>]",False,102,3,6,3,3,6,9,28,0,4,2,9,0,0,1,1,1,1,2,1,5,0,4,0,0,0,35,8,0,True
282,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\CommonActions.java,org.logstash.config.ir.compiler.CommonActions,"Map<String, Object> addTag(Map<String, Object>, List<Object>)","/**
 * Implements the {@code tags} option for Logstash inputs.
 *
 * @param e    Event on which to add the tags.
 * @param tags The tags to be added to the event.
 * @return Updated event.
 */
static Map<String, Object> addTag(Map<String, Object> e, List<Object> tags) {
    Event tempEvent = new org.logstash.Event(e);
    addTag(tempEvent, tags);
    return tempEvent.getData();
}","/**
 * Implements the {@code tags} option for Logstash inputs.
 *
 * @param e    Event on which to add the tags.
 * @param tags The tags to be added to the event.
 * @return Updated event.
 */
", ,/** * Implements the {@code tags} option for Logstash inputs. * * @param e    Event on which to add the tags. * @param tags The tags to be added to the event. * @return Updated event. */,138,142,[0],0,[0],0,[0],0,0,0,0,"addTag(Map<String, Object>, List<Object>)",org.logstash.config.ir.compiler.CommonActions,"addTag/2[java.util.Map<java.lang.String,java.lang.Object>,java.util.List<java.lang.Object>]",False,138,3,4,1,3,1,2,5,1,1,2,2,1,1,0,0,0,0,0,0,1,0,0,0,0,0,17,8,0,True
283,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\CommonActions.java,org.logstash.config.ir.compiler.CommonActions,"void addTag(Event, List<Object>)","/**
 * Implements the {@code add_tag} option for Logstash filters.
 *
 * @param evt  Event on which to add the tags.
 * @param tags The tags to be added to the event.
 */
static void addTag(Event evt, List<Object> tags) {
    try {
        for (Object o : tags) {
            String tagToAdd = StringInterpolation.evaluate(evt, o.toString());
            evt.tag(tagToAdd);
        }
    } catch (IOException ex) {
        throw new IllegalStateException(ex);
    }
}","/**
 * Implements the {@code add_tag} option for Logstash filters.
 *
 * @param evt  Event on which to add the tags.
 * @param tags The tags to be added to the event.
 */
", ,/** * Implements the {@code add_tag} option for Logstash filters. * * @param evt  Event on which to add the tags. * @param tags The tags to be added to the event. */,150,159,[0],0,[0],0,[0],0,0,0,0,"addTag(Event, List<Object>)",org.logstash.config.ir.compiler.CommonActions,"addTag/2[co.elastic.logstash.api.Event,java.util.List<java.lang.Object>]",False,150,2,5,3,2,3,3,11,0,1,2,3,0,0,1,0,1,0,0,0,1,0,2,0,0,0,26,8,0,True
284,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\CommonActions.java,org.logstash.config.ir.compiler.CommonActions,"Map<String, Object> addType(Map<String, Object>, String)","/**
 * Implements the {@code type} option for Logstash inputs.
 *
 * @param event Event on which to set the type.
 * @param type  The type to set on the event.
 * @return Updated event.
 */
static Map<String, Object> addType(Map<String, Object> event, String type) {
    event.putIfAbsent(""type"", type);
    return event;
}","/**
 * Implements the {@code type} option for Logstash inputs.
 *
 * @param event Event on which to set the type.
 * @param type  The type to set on the event.
 * @return Updated event.
 */
", ,/** * Implements the {@code type} option for Logstash inputs. * * @param event Event on which to set the type. * @param type  The type to set on the event. * @return Updated event. */,168,171,[0],0,[0],0,[0],0,0,0,0,"addType(Map<String, Object>, String)",org.logstash.config.ir.compiler.CommonActions,"addType/2[java.util.Map<java.lang.String,java.lang.Object>,java.lang.String]",False,168,0,2,2,0,1,1,4,1,0,2,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,15,8,0,True
285,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\CommonActions.java,org.logstash.config.ir.compiler.CommonActions,"void removeField(Event, List<String>)","/**
 * Implements the {@code remove_field} option for Logstash filters.
 *
 * @param evt            Event from which to remove the fields.
 * @param fieldsToRemove The fields to remove from the event.
 */
static void removeField(Event evt, List<String> fieldsToRemove) {
    try {
        for (String s : fieldsToRemove) {
            String fieldToRemove = StringInterpolation.evaluate(evt, s);
            evt.remove(fieldToRemove);
        }
    } catch (IOException ex) {
        throw new IllegalStateException(ex);
    }
}","/**
 * Implements the {@code remove_field} option for Logstash filters.
 *
 * @param evt            Event from which to remove the fields.
 * @param fieldsToRemove The fields to remove from the event.
 */
", ,/** * Implements the {@code remove_field} option for Logstash filters. * * @param evt            Event from which to remove the fields. * @param fieldsToRemove The fields to remove from the event. */,179,188,[0],0,[0],0,[0],0,0,0,0,"removeField(Event, List<String>)",org.logstash.config.ir.compiler.CommonActions,"removeField/2[co.elastic.logstash.api.Event,java.util.List<java.lang.String>]",False,179,2,5,3,2,3,2,11,0,1,2,2,0,0,1,0,1,0,0,0,1,0,2,0,0,0,23,8,0,True
286,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\CommonActions.java,org.logstash.config.ir.compiler.CommonActions,"void removeTag(Event, List<String>)","/**
 * Implements the {@code remove_tag} option for Logstash filters.
 *
 * @param evt          Event from which to remove the tags.
 * @param tagsToRemove The tags to remove from the event.
 */
@SuppressWarnings({ ""unchecked"", ""rawtypes"" })
static void removeTag(Event evt, List<String> tagsToRemove) {
    Object o = evt.getField(""tags"");
    if (o instanceof List) {
        List tags = (List) o;
        if (tags.size() > 0) {
            try {
                for (String s : tagsToRemove) {
                    String tagToRemove = StringInterpolation.evaluate(evt, s);
                    tags.remove(tagToRemove);
                }
                evt.setField(""tags"", tags);
            } catch (IOException ex) {
                throw new IllegalStateException(ex);
            }
        }
    }
}","/**
 * Implements the {@code remove_tag} option for Logstash filters.
 *
 * @param evt          Event from which to remove the tags.
 * @param tagsToRemove The tags to remove from the event.
 */
", ,/** * Implements the {@code remove_tag} option for Logstash filters. * * @param evt          Event from which to remove the tags. * @param tagsToRemove The tags to remove from the event. */,196,213,[0],0,[0],0,[0],0,0,0,0,"removeTag(Event, List<String>)",org.logstash.config.ir.compiler.CommonActions,"removeTag/2[co.elastic.logstash.api.Event,java.util.List<java.lang.String>]",False,197,2,5,2,3,5,5,18,0,3,2,5,0,0,1,0,1,0,4,1,3,0,4,0,0,0,26,8,0,True
287,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\ComputeStepSyntaxElement.java,org.logstash.config.ir.compiler.ComputeStepSyntaxElement,void cleanClassCache(),"/*
     * Used in a test to clean start, with class loaders wiped out into Janino compiler and cleared the cached classes.
    * */
@VisibleForTesting
public static void cleanClassCache() {
    synchronized (COMPILER) {
        CLASS_CACHE.clear();
    }
}","/*
     * Used in a test to clean start, with class loaders wiped out into Janino compiler and cleared the cached classes.
    * */
", ,"/*     * Used in a test to clean start, with class loaders wiped out into Janino compiler and cleared the cached classes.    * */",100,105,[0],0,[0],0,[0],0,0,0,0,cleanClassCache(),org.logstash.config.ir.compiler.ComputeStepSyntaxElement,cleanClassCache/0,False,101,1,2,2,0,1,1,5,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,11,9,0,False
288,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\ComputeStepSyntaxElement.java,org.logstash.config.ir.compiler.ComputeStepSyntaxElement,Class<? extends Dataset> compile(),"@SuppressWarnings(""unchecked"")
private /*
     * Returns a {@link Class<? extends Dataset>} for this {@link ComputeStepSyntaxElement}, reusing an existing
     * equivalent implementation from the global class cache when one is available, or otherwise compiling one.
     *
     * This method _is_ thread-safe, and uses the locking semantics of {@link ConcurrentHashMap#computeIfAbsent}.
     * To do so, it relies on {@link #hashCode()} and {@link #equals(Object)}.
     */
Class<? extends Dataset> compile() {
    return CLASS_CACHE.computeIfAbsent(this, (__) -> {
        try {
            final ISimpleCompiler compiler = COMPILER.get();
            final String name = String.format(""CompiledDataset%d"", DATASET_CLASS_INDEX.incrementAndGet());
            final String code = CLASS_NAME_PLACEHOLDER_REGEX.matcher(normalizedSource).replaceAll(name);
            if (SOURCE_DIR != null) {
                final Path sourceFile = SOURCE_DIR.resolve(String.format(""%s.java"", name));
                Files.write(sourceFile, code.getBytes(StandardCharsets.UTF_8));
                compiler.cookFile(sourceFile.toFile());
            } else {
                compiler.cook(code);
            }
            return (Class<T>) compiler.getClassLoader().loadClass(String.format(""org.logstash.generated.%s"", name));
        } catch (final CompileException | ClassNotFoundException | IOException ex) {
            throw new IllegalStateException(ex);
        }
    });
}", ,"/*
     * Returns a {@link Class<? extends Dataset>} for this {@link ComputeStepSyntaxElement}, reusing an existing
     * equivalent implementation from the global class cache when one is available, or otherwise compiling one.
     *
     * This method _is_ thread-safe, and uses the locking semantics of {@link ConcurrentHashMap#computeIfAbsent}.
     * To do so, it relies on {@link #hashCode()} and {@link #equals(Object)}.
     */
","/*     * Returns a {@link Class<? extends Dataset>} for this {@link ComputeStepSyntaxElement}, reusing an existing     * equivalent implementation from the global class cache when one is available, or otherwise compiling one.     *     * This method _is_ thread-safe, and uses the locking semantics of {@link ConcurrentHashMap#computeIfAbsent}.     * To do so, it relies on {@link #hashCode()} and {@link #equals(Object)}.     */",131,159,[0],0,[0],0,[0],0,0,0,0,compile(),org.logstash.config.ir.compiler.ComputeStepSyntaxElement,compile/0,False,139,2,0,0,0,3,14,22,2,5,0,14,0,0,0,1,1,0,4,0,4,0,3,0,0,1,31,2,0,False
289,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\ComputeStepSyntaxElement.java,org.logstash.config.ir.compiler.ComputeStepSyntaxElement,"Map<String, Object> ctorArguments()","/**
 * @return Array of constructor arguments
 */
private Map<String, Object> ctorArguments() {
    final Map<String, Object> result = new HashMap<>();
    fields.ctorAssigned().getFields().forEach(fieldDefinition -> result.put(fieldDefinition.getName(), fieldDefinition.getCtorArgument()));
    return result;
}","/**
 * @return Array of constructor arguments
 */
", ,/** * @return Array of constructor arguments */,212,219,[0],0,[0],0,[0],0,0,0,0,ctorArguments(),org.logstash.config.ir.compiler.ComputeStepSyntaxElement,ctorArguments/0,False,212,3,4,0,4,1,6,5,1,2,0,6,0,0,0,0,0,0,0,0,1,0,0,0,0,1,9,2,0,True
290,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\ComputeStepSyntaxElement.java,org.logstash.config.ir.compiler.ComputeStepSyntaxElement,String fieldsAndCtor(String),"/**
 * Generates the Java code for defining one field and constructor argument for each given value.
 * constructor for
 * @return Java Source String
 */
private String fieldsAndCtor(final String name) {
    final Closure constructor = new Closure();
    final FieldDeclarationGroup ctorFields = fields.ctorAssigned();
    for (final FieldDefinition field : ctorFields.getFields()) {
        if (field.getCtorArgument() != null) {
            final VariableDefinition fieldVar = field.asVariable();
            constructor.add(SyntaxFactory.assignment(fieldVar.access(), SyntaxFactory.cast(fieldVar.type, CTOR_ARGUMENT.access().call(""get"", SyntaxFactory.value(SyntaxFactory.join(""\"""", field.getName(), ""\""""))))));
        }
    }
    return combine(ctorFields, MethodSyntaxElement.constructor(name, constructor.add(fields.afterInit())));
}","/**
 * Generates the Java code for defining one field and constructor argument for each given value.
 * constructor for
 * @return Java Source String
 */
", ,/** * Generates the Java code for defining one field and constructor argument for each given value. * constructor for * @return Java Source String */,226,254,[0],0,[0],0,[0],0,0,0,0,fieldsAndCtor(String),org.logstash.config.ir.compiler.ComputeStepSyntaxElement,fieldsAndCtor/1[java.lang.String],False,226,9,17,0,17,3,16,11,1,3,1,16,0,0,1,1,0,0,3,0,3,0,2,0,0,0,28,2,0,True
291,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\ComputeStepSyntaxElement.java,org.logstash.config.ir.compiler.ComputeStepSyntaxElement,String combine(SyntaxElement...),"/**
 * Renders the string concatenation of the given {@link SyntaxElement}, delimited by
 * line breaks.
 * @param parts Elements to concatenate
 * @return Java source
 */
private static String combine(final SyntaxElement... parts) {
    return Arrays.stream(parts).map(SyntaxElement::generateCode).collect(Collectors.joining(""\n""));
}","/**
 * Renders the string concatenation of the given {@link SyntaxElement}, delimited by
 * line breaks.
 * @param parts Elements to concatenate
 * @return Java source
 */
", ,"/** * Renders the string concatenation of the given {@link SyntaxElement}, delimited by * line breaks. * @param parts Elements to concatenate * @return Java source */",262,265,[0],0,[0],0,[0],0,0,0,0,combine(SyntaxElement[]),org.logstash.config.ir.compiler.ComputeStepSyntaxElement,combine/1[org.logstash.config.ir.compiler.SyntaxElement[]],False,262,1,0,0,0,1,4,3,1,0,1,4,0,0,0,0,0,0,1,0,0,0,0,0,0,0,18,10,0,True
292,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\Dataset.java,org.logstash.config.ir.compiler.Dataset,"Collection<JrubyEventExtLibrary.RubyEvent> compute(RubyArray, boolean, boolean)","/**
 * Compute the actual contents of the backing {@link RubyArray} and cache them.
 * Repeated invocations will be effectively free.
 * @param batch Input {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} received at the root
 * of the execution
 * @param flush True if flushing flushable nodes while traversing the execution
 * @param shutdown True if this is the last call to this instance's compute method because
 * the pipeline it belongs to is shut down
 * @return Computed {@link RubyArray} of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}
 */
Collection<JrubyEventExtLibrary.RubyEvent> compute(@SuppressWarnings({ ""rawtypes"" }) RubyArray batch, boolean flush, boolean shutdown);","/**
 * Compute the actual contents of the backing {@link RubyArray} and cache them.
 * Repeated invocations will be effectively free.
 * @param batch Input {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} received at the root
 * of the execution
 * @param flush True if flushing flushable nodes while traversing the execution
 * @param shutdown True if this is the last call to this instance's compute method because
 * the pipeline it belongs to is shut down
 * @return Computed {@link RubyArray} of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent}
 */
", ,/** * Compute the actual contents of the backing {@link RubyArray} and cache them. * Repeated invocations will be effectively free. * @param batch Input {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} received at the root * of the execution * @param flush True if flushing flushable nodes while traversing the execution * @param shutdown True if this is the last call to this instance's compute method because * the pipeline it belongs to is shut down * @return Computed {@link RubyArray} of {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} */,67,68,[0],0,[0],0,[0],0,0,0,0,"compute(RubyArray, boolean, boolean)",org.logstash.config.ir.compiler.Dataset,"compute/3[org.logstash.config.ir.compiler.RubyArray,boolean,boolean]",False,57,2,6,6,0,1,0,1,0,0,3,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,40,0,0,True
293,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\Dataset.java,org.logstash.config.ir.compiler.Dataset,void clear(),"/**
 * Removes all data from the instance and all of its parents, making the instance ready for
 * use with a new set of input data.
 */
void clear();","/**
 * Removes all data from the instance and all of its parents, making the instance ready for
 * use with a new set of input data.
 */
", ,"/** * Removes all data from the instance and all of its parents, making the instance ready for * use with a new set of input data. */",74,74,[0],0,[0],0,[0],0,0,0,0,clear(),org.logstash.config.ir.compiler.Dataset,clear/0,False,70,0,3,3,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,17,0,0,True
294,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\DatasetCompiler.java,org.logstash.config.ir.compiler.DatasetCompiler,"ComputeStepSyntaxElement<Dataset> filterDataset(Collection<Dataset>, AbstractFilterDelegatorExt)","/**
 * Compiles a {@link Dataset} representing a filter plugin without flush behaviour.
 * @param parents Parent {@link Dataset} to aggregate for this filter
 * @param plugin Filter Plugin
 * @return Dataset representing the filter plugin
 */
public static ComputeStepSyntaxElement<Dataset> filterDataset(final Collection<Dataset> parents, final AbstractFilterDelegatorExt plugin) {
    final ClassFields fields = new ClassFields();
    final ValueSyntaxElement outputBuffer = fields.add(new ArrayList<>());
    final Closure clear = Closure.wrap();
    final Closure compute;
    if (parents.isEmpty()) {
        compute = filterBody(outputBuffer, BATCH_ARG, fields, plugin);
    } else {
        final Collection<ValueSyntaxElement> parentFields = parents.stream().map(fields::add).collect(Collectors.toList());
        @SuppressWarnings(""rawtypes"")
        final RubyArray inputBuffer = RubyUtil.RUBY.newArray();
        clear.add(clearSyntax(parentFields));
        final ValueSyntaxElement inputBufferField = fields.add(inputBuffer);
        compute = withInputBuffering(filterBody(outputBuffer, inputBufferField, fields, plugin), parentFields, inputBufferField);
    }
    return prepare(withOutputBuffering(compute, clear, outputBuffer, fields));
}","/**
 * Compiles a {@link Dataset} representing a filter plugin without flush behaviour.
 * @param parents Parent {@link Dataset} to aggregate for this filter
 * @param plugin Filter Plugin
 * @return Dataset representing the filter plugin
 */
", ,/** * Compiles a {@link Dataset} representing a filter plugin without flush behaviour. * @param parents Parent {@link Dataset} to aggregate for this filter * @param plugin Filter Plugin * @return Dataset representing the filter plugin */,109,134,[0],0,[0],0,[0],0,0,0,0,"filterDataset(Collection<Dataset>, AbstractFilterDelegatorExt)",org.logstash.config.ir.compiler.DatasetCompiler,"filterDataset/2[java.util.Collection<org.logstash.config.ir.compiler.Dataset>,org.logstash.config.ir.compiler.AbstractFilterDelegatorExt]",False,112,8,11,1,10,2,15,17,1,7,2,15,5,5,0,0,0,0,1,0,8,0,1,0,0,0,40,9,0,True
295,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\DatasetCompiler.java,org.logstash.config.ir.compiler.DatasetCompiler,Dataset terminalFilterDataset(Collection<Dataset>),"/**
 * <p>Builds a terminal {@link Dataset} for the filters from the given parent {@link Dataset}s.</p>
 * <p>If the given set of parent {@link Dataset} is empty the sum is defined as the
 * trivial dataset that does not invoke any computation whatsoever.</p>
 * {@link Dataset#compute(RubyArray, boolean, boolean)} is always
 * {@link Collections#emptyList()}.
 * @param parents Parent {@link Dataset} to sum
 * @return Dataset representing the sum of given parent {@link Dataset}
 */
public static Dataset terminalFilterDataset(final Collection<Dataset> parents) {
    if (parents.isEmpty()) {
        return Dataset.IDENTITY;
    }
    final int count = parents.size();
    if (count == 1) {
        // No need for a terminal dataset here, if there is only a single parent node we can
        // call it directly.
        return parents.iterator().next();
    }
    final ClassFields fields = new ClassFields();
    final Collection<ValueSyntaxElement> parentFields = parents.stream().map(fields::add).collect(Collectors.toList());
    @SuppressWarnings(""rawtypes"")
    final RubyArray inputBuffer = RubyUtil.RUBY.newArray();
    final ValueSyntaxElement inputBufferField = fields.add(inputBuffer);
    final ValueSyntaxElement outputBufferField = fields.add(new ArrayList<>());
    final Closure clear = Closure.wrap().add(clearSyntax(parentFields));
    final Closure compute = withInputBuffering(Closure.wrap(// pass thru filter
    buffer(outputBufferField, inputBufferField)), parentFields, inputBufferField);
    return prepare(withOutputBuffering(compute, clear, outputBufferField, fields)).instantiate();
}","/**
 * <p>Builds a terminal {@link Dataset} for the filters from the given parent {@link Dataset}s.</p>
 * <p>If the given set of parent {@link Dataset} is empty the sum is defined as the
 * trivial dataset that does not invoke any computation whatsoever.</p>
 * {@link Dataset#compute(RubyArray, boolean, boolean)} is always
 * {@link Collections#emptyList()}.
 * @param parents Parent {@link Dataset} to sum
 * @return Dataset representing the sum of given parent {@link Dataset}
 */
","// No need for a terminal dataset here, if there is only a single parent node we can
[[SEP]]// call it directly.
[[SEP]]// pass thru filter
","/** * <p>Builds a terminal {@link Dataset} for the filters from the given parent {@link Dataset}s.</p> * <p>If the given set of parent {@link Dataset} is empty the sum is defined as the * trivial dataset that does not invoke any computation whatsoever.</p> * {@link Dataset#compute(RubyArray, boolean, boolean)} is always * {@link Collections#emptyList()}. * @param parents Parent {@link Dataset} to sum * @return Dataset representing the sum of given parent {@link Dataset} */[[SEP]]// No need for a terminal dataset here, if there is only a single parent node we can// call it directly.[[SEP]]// pass thru filter",145,176,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,terminalFilterDataset(Collection<Dataset>),org.logstash.config.ir.compiler.DatasetCompiler,terminalFilterDataset/1[java.util.Collection<org.logstash.config.ir.compiler.Dataset>],False,145,7,12,1,11,3,19,17,3,8,1,19,5,3,0,1,0,0,1,1,8,0,1,0,0,0,54,9,0,True
296,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\DatasetCompiler.java,org.logstash.config.ir.compiler.DatasetCompiler,Dataset terminalOutputDataset(Collection<Dataset>),"/**
 * <p>Builds a terminal {@link Dataset} for the outputs from the given parent {@link Dataset}s.</p>
 * <p>If the given set of parent {@link Dataset} is empty the sum is defined as the
 * trivial dataset that does not invoke any computation whatsoever.</p>
 * {@link Dataset#compute(RubyArray, boolean, boolean)} is always
 * {@link Collections#emptyList()}.
 * @param parents Parent {@link Dataset} to sum and terminate
 * @return Dataset representing the sum of given parent {@link Dataset}
 */
public static Dataset terminalOutputDataset(final Collection<Dataset> parents) {
    if (parents.isEmpty()) {
        throw new IllegalArgumentException(""Cannot create terminal output dataset for an empty number of parent datasets"");
    }
    final int count = parents.size();
    if (count == 1) {
        // No need for a terminal dataset here, if there is only a single parent node we can
        // call it directly.
        return parents.iterator().next();
    }
    final ClassFields fields = new ClassFields();
    final Collection<ValueSyntaxElement> parentFields = parents.stream().map(fields::add).collect(Collectors.toList());
    final Closure compute = Closure.wrap(parentFields.stream().map(DatasetCompiler::computeDataset).toArray(MethodLevelSyntaxElement[]::new)).add(clearSyntax(parentFields));
    return compileOutput(compute, Closure.EMPTY, fields).instantiate();
}","/**
 * <p>Builds a terminal {@link Dataset} for the outputs from the given parent {@link Dataset}s.</p>
 * <p>If the given set of parent {@link Dataset} is empty the sum is defined as the
 * trivial dataset that does not invoke any computation whatsoever.</p>
 * {@link Dataset#compute(RubyArray, boolean, boolean)} is always
 * {@link Collections#emptyList()}.
 * @param parents Parent {@link Dataset} to sum and terminate
 * @return Dataset representing the sum of given parent {@link Dataset}
 */
","// No need for a terminal dataset here, if there is only a single parent node we can
[[SEP]]// call it directly.
","/** * <p>Builds a terminal {@link Dataset} for the outputs from the given parent {@link Dataset}s.</p> * <p>If the given set of parent {@link Dataset} is empty the sum is defined as the * trivial dataset that does not invoke any computation whatsoever.</p> * {@link Dataset#compute(RubyArray, boolean, boolean)} is always * {@link Collections#emptyList()}. * @param parents Parent {@link Dataset} to sum and terminate * @return Dataset representing the sum of given parent {@link Dataset} */[[SEP]]// No need for a terminal dataset here, if there is only a single parent node we can// call it directly.",187,213,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,terminalOutputDataset(Collection<Dataset>),org.logstash.config.ir.compiler.DatasetCompiler,terminalOutputDataset/1[java.util.Collection<org.logstash.config.ir.compiler.Dataset>],False,187,6,7,1,6,3,16,13,2,4,1,16,2,2,0,1,0,0,1,1,4,0,1,0,0,0,50,9,0,True
297,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\DatasetCompiler.java,org.logstash.config.ir.compiler.DatasetCompiler,"ComputeStepSyntaxElement<Dataset> outputDataset(Collection<Dataset>, AbstractOutputDelegatorExt, boolean)","/**
 * Compiles the {@link Dataset} representing an output plugin.
 * Note: The efficiency of the generated code rests on invoking the Ruby method
 * {@code multi_receive} in the cheapest possible way.
 * This is achieved by:
 * 1. Caching the method's {@link org.jruby.runtime.CallSite} into an instance
 * variable.
 * 2. Calling the low level CallSite invocation
 * {@link DynamicMethod#call(org.jruby.runtime.ThreadContext, IRubyObject, org.jruby.RubyModule, String, IRubyObject[], Block)}
 * using an {@code IRubyObject[]} field that is repopulated with the current Event array on
 * every call to {@code compute}.
 * @param parents Parent Datasets
 * @param output Output Plugin (of Ruby type OutputDelegator)
 * @param terminal Set to true if this output is the only output in the pipeline
 * @return Output Dataset
 */
public static ComputeStepSyntaxElement<Dataset> outputDataset(final Collection<Dataset> parents, final AbstractOutputDelegatorExt output, final boolean terminal) {
    final ClassFields fields = new ClassFields();
    final Closure clearSyntax;
    final Closure computeSyntax;
    final ValueSyntaxElement outputField = fields.add(output);
    if (parents.isEmpty()) {
        clearSyntax = Closure.EMPTY;
        computeSyntax = Closure.wrap(setPluginIdForLog4j(outputField), invokeOutput(outputField, BATCH_ARG), unsetPluginIdForLog4j());
    } else {
        final Collection<ValueSyntaxElement> parentFields = parents.stream().map(fields::add).collect(Collectors.toList());
        @SuppressWarnings(""rawtypes"")
        final RubyArray buffer = RubyUtil.RUBY.newArray();
        final Closure inlineClear;
        if (terminal) {
            clearSyntax = Closure.EMPTY;
            inlineClear = clearSyntax(parentFields);
        } else {
            inlineClear = Closure.EMPTY;
            clearSyntax = clearSyntax(parentFields);
        }
        final ValueSyntaxElement inputBuffer = fields.add(buffer);
        computeSyntax = withInputBuffering(Closure.wrap(setPluginIdForLog4j(outputField), invokeOutput(outputField, inputBuffer), inlineClear, unsetPluginIdForLog4j()), parentFields, inputBuffer);
    }
    return compileOutput(computeSyntax, clearSyntax, fields);
}","/**
 * Compiles the {@link Dataset} representing an output plugin.
 * Note: The efficiency of the generated code rests on invoking the Ruby method
 * {@code multi_receive} in the cheapest possible way.
 * This is achieved by:
 * 1. Caching the method's {@link org.jruby.runtime.CallSite} into an instance
 * variable.
 * 2. Calling the low level CallSite invocation
 * {@link DynamicMethod#call(org.jruby.runtime.ThreadContext, IRubyObject, org.jruby.RubyModule, String, IRubyObject[], Block)}
 * using an {@code IRubyObject[]} field that is repopulated with the current Event array on
 * every call to {@code compute}.
 * @param parents Parent Datasets
 * @param output Output Plugin (of Ruby type OutputDelegator)
 * @param terminal Set to true if this output is the only output in the pipeline
 * @return Output Dataset
 */
", ,"/** * Compiles the {@link Dataset} representing an output plugin. * Note: The efficiency of the generated code rests on invoking the Ruby method * {@code multi_receive} in the cheapest possible way. * This is achieved by: * 1. Caching the method's {@link org.jruby.runtime.CallSite} into an instance * variable. * 2. Calling the low level CallSite invocation * {@link DynamicMethod#call(org.jruby.runtime.ThreadContext, IRubyObject, org.jruby.RubyModule, String, IRubyObject[], Block)} * using an {@code IRubyObject[]} field that is repopulated with the current Event array on * every call to {@code compute}. * @param parents Parent Datasets * @param output Output Plugin (of Ruby type OutputDelegator) * @param terminal Set to true if this output is the only output in the pipeline * @return Output Dataset */",231,271,[0],0,[0],0,[0],0,0,0,0,"outputDataset(Collection<Dataset>, AbstractOutputDelegatorExt, boolean)",org.logstash.config.ir.compiler.DatasetCompiler,"outputDataset/3[java.util.Collection<org.logstash.config.ir.compiler.Dataset>,org.logstash.config.ir.compiler.AbstractOutputDelegatorExt,boolean]",False,235,8,12,2,10,3,15,26,1,8,3,15,6,3,0,0,0,0,1,0,12,0,2,0,0,0,80,9,0,True
298,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\DatasetCompiler.java,org.logstash.config.ir.compiler.DatasetCompiler,ComputeStepSyntaxElement<Dataset> prepare(DatasetCompiler.ComputeAndClear),"/**
 * Compiles and subsequently instantiates a {@link Dataset} from given code snippets and
 * constructor arguments.
 * This method must be {@code synchronized} to avoid compiling duplicate classes.
 * @param compute Method definitions for {@code compute} and {@code clear}
 * @return Dataset Instance
 */
private static ComputeStepSyntaxElement<Dataset> prepare(final DatasetCompiler.ComputeAndClear compute) {
    return ComputeStepSyntaxElement.create(Arrays.asList(compute.compute(), compute.clear()), compute.fields(), Dataset.class);
}","/**
 * Compiles and subsequently instantiates a {@link Dataset} from given code snippets and
 * constructor arguments.
 * This method must be {@code synchronized} to avoid compiling duplicate classes.
 * @param compute Method definitions for {@code compute} and {@code clear}
 * @return Dataset Instance
 */
", ,/** * Compiles and subsequently instantiates a {@link Dataset} from given code snippets and * constructor arguments. * This method must be {@code synchronized} to avoid compiling duplicate classes. * @param compute Method definitions for {@code compute} and {@code clear} * @return Dataset Instance */,320,324,[0],0,[0],0,[0],0,0,0,0,prepare(ComputeAndClear),org.logstash.config.ir.compiler.DatasetCompiler,prepare/1[org.logstash.config.ir.compiler.DatasetCompiler.ComputeAndClear],False,320,3,7,3,4,1,5,3,1,0,1,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,25,10,0,True
299,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\DatasetCompiler.java,org.logstash.config.ir.compiler.DatasetCompiler,"Closure withInputBuffering(Closure, Collection<ValueSyntaxElement>, ValueSyntaxElement)","/**
 * Generates code that buffers all events that aren't cancelled from a given set of parent
 * {@link Dataset} to a given collection, executes the given closure and then clears the
 * collection used for buffering.
 * @param compute Closure to execute
 * @param parents Parents to buffer results for
 * @param inputBuffer Buffer to store results in
 * @return Closure wrapped by buffering parent results and clearing them
 */
private static Closure withInputBuffering(final Closure compute, final Collection<ValueSyntaxElement> parents, final ValueSyntaxElement inputBuffer) {
    return Closure.wrap(parents.stream().map(par -> SyntaxFactory.value(""org.logstash.config.ir.compiler.Utils"").call(""copyNonCancelledEvents"", computeDataset(par), inputBuffer)).toArray(MethodLevelSyntaxElement[]::new)).add(compute).add(clear(inputBuffer));
}","/**
 * Generates code that buffers all events that aren't cancelled from a given set of parent
 * {@link Dataset} to a given collection, executes the given closure and then clears the
 * collection used for buffering.
 * @param compute Closure to execute
 * @param parents Parents to buffer results for
 * @param inputBuffer Buffer to store results in
 * @return Closure wrapped by buffering parent results and clearing them
 */
", ,"/** * Generates code that buffers all events that aren't cancelled from a given set of parent * {@link Dataset} to a given collection, executes the given closure and then clears the * collection used for buffering. * @param compute Closure to execute * @param parents Parents to buffer results for * @param inputBuffer Buffer to store results in * @return Closure wrapped by buffering parent results and clearing them */",335,342,[0],0,[0],0,[0],0,0,0,0,"withInputBuffering(Closure, Collection<ValueSyntaxElement>, ValueSyntaxElement)",org.logstash.config.ir.compiler.DatasetCompiler,"withInputBuffering/3[org.logstash.config.ir.compiler.Closure,java.util.Collection<org.logstash.config.ir.compiler.ValueSyntaxElement>,org.logstash.config.ir.compiler.ValueSyntaxElement]",False,336,4,11,4,7,1,10,3,1,1,3,10,2,1,0,0,0,0,2,0,0,0,0,0,0,1,47,10,0,True
300,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\DatasetCompiler.java,org.logstash.config.ir.compiler.DatasetCompiler,"DatasetCompiler.ComputeAndClear withOutputBuffering(Closure, Closure, ValueSyntaxElement, ClassFields)","/**
 * Generates compute and clear actions with logic for setting a boolean {@code done}
 * flag and caching the result of the computation in the {@code compute} closure.
 * Wraps {@code clear} closure with condition to only execute the clear if the {@code done}
 * flag is set to {@code true}. Also adds clearing the output buffer used for caching the
 * {@code compute} result to the {@code clear} closure.
 * @param compute Compute closure to execute
 * @param clear Clear closure to execute
 * @param outputBuffer Output buffer used for caching {@code compute} result
 * @param fields Class fields
 * @return ComputeAndClear with adjusted methods and {@code done} flag added to fields
 */
private static DatasetCompiler.ComputeAndClear withOutputBuffering(final Closure compute, final Closure clear, final ValueSyntaxElement outputBuffer, final ClassFields fields) {
    final SyntaxFactory.MethodCallReturnValue done = new SyntaxFactory.MethodCallReturnValue(SyntaxFactory.value(""this""), ""isDone"");
    return computeAndClear(Closure.wrap(SyntaxFactory.ifCondition(done, Closure.wrap(SyntaxFactory.ret(outputBuffer)))).add(compute).add(new SyntaxFactory.MethodCallReturnValue(SyntaxFactory.value(""this""), ""setDone"")).add(SyntaxFactory.ret(outputBuffer)), Closure.wrap(SyntaxFactory.ifCondition(done, Closure.wrap(clear.add(clear(outputBuffer)), new SyntaxFactory.MethodCallReturnValue(SyntaxFactory.value(""this""), ""clearDone"")))), fields);
}","/**
 * Generates compute and clear actions with logic for setting a boolean {@code done}
 * flag and caching the result of the computation in the {@code compute} closure.
 * Wraps {@code clear} closure with condition to only execute the clear if the {@code done}
 * flag is set to {@code true}. Also adds clearing the output buffer used for caching the
 * {@code compute} result to the {@code clear} closure.
 * @param compute Compute closure to execute
 * @param clear Clear closure to execute
 * @param outputBuffer Output buffer used for caching {@code compute} result
 * @param fields Class fields
 * @return ComputeAndClear with adjusted methods and {@code done} flag added to fields
 */
", ,/** * Generates compute and clear actions with logic for setting a boolean {@code done} * flag and caching the result of the computation in the {@code compute} closure. * Wraps {@code clear} closure with condition to only execute the clear if the {@code done} * flag is set to {@code true}. Also adds clearing the output buffer used for caching the * {@code compute} result to the {@code clear} closure. * @param compute Compute closure to execute * @param clear Clear closure to execute * @param outputBuffer Output buffer used for caching {@code compute} result * @param fields Class fields * @return ComputeAndClear with adjusted methods and {@code done} flag added to fields */,356,374,[0],0,[0],0,[0],0,0,0,0,"withOutputBuffering(Closure, Closure, ValueSyntaxElement, ClassFields)",org.logstash.config.ir.compiler.DatasetCompiler,"withOutputBuffering/4[org.logstash.config.ir.compiler.Closure,org.logstash.config.ir.compiler.Closure,org.logstash.config.ir.compiler.ValueSyntaxElement,org.logstash.config.ir.compiler.ClassFields]",False,357,7,12,3,9,1,8,4,1,1,4,8,2,1,0,0,0,0,6,0,1,0,0,0,0,0,47,10,0,True
301,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\EventCondition.java,org.logstash.config.ir.compiler.EventCondition,boolean fulfilled(JrubyEventExtLibrary.RubyEvent),"/**
 * Checks if {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} fulfils the condition.
 * @param event org.logstash.ext.RubyEvent to check
 * @return True iff event fulfils condition
 */
boolean fulfilled(JrubyEventExtLibrary.RubyEvent event);","/**
 * Checks if {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} fulfils the condition.
 * @param event org.logstash.ext.RubyEvent to check
 * @return True iff event fulfils condition
 */
", ,/** * Checks if {@link org.logstash.ext.JrubyEventExtLibrary.RubyEvent} fulfils the condition. * @param event org.logstash.ext.RubyEvent to check * @return True iff event fulfils condition */,74,74,[0],0,[0],0,[0],0,0,0,0,fulfilled(RubyEvent),org.logstash.config.ir.compiler.EventCondition,fulfilled/1[org.logstash.ext.JrubyEventExtLibrary.RubyEvent],False,69,1,3,3,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,0,0,True
302,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\EventCondition.java,org.logstash.config.ir.compiler.EventCondition.Compiler,EventCondition buildCondition(BooleanExpression),"/**
 * Compiles a {@link BooleanExpression} into an {@link EventCondition}.
 * All compilation is globally {@code synchronized} on {@link EventCondition.Compiler#cache}
 * to minimize code size by avoiding compiling logically equivalent expressions in more than
 * one instance.
 *
 * @param expression BooleanExpress to compile
 * @return Compiled {@link EventCondition}
 */
public EventCondition buildCondition(final BooleanExpression expression) {
    synchronized (cache) {
        final String cachekey = expression.toRubyString();
        final EventCondition cached = cache.get(cachekey);
        if (cached != null) {
            return cached;
        }
        final EventCondition condition;
        if (expression instanceof Eq) {
            condition = eq((Eq) expression);
        } else if (expression instanceof RegexEq) {
            condition = regex((RegexEq) expression);
        } else if (expression instanceof In) {
            condition = in((In) expression);
        } else if (expression instanceof Or || expression instanceof And) {
            condition = booleanCondition((BinaryBooleanExpression) expression);
        } else if (expression instanceof Truthy) {
            condition = truthy((Truthy) expression);
        } else if (expression instanceof Not) {
            condition = not((Not) expression);
        } else if (expression instanceof Gt || expression instanceof Gte || expression instanceof Lt || expression instanceof Lte) {
            condition = comparison((BinaryBooleanExpression) expression);
        } else if (expression instanceof Neq) {
            condition = not(eq((BinaryBooleanExpression) expression));
        } else {
            throw new EventCondition.Compiler.UnexpectedTypeException(expression);
        }
        cache.put(cachekey, condition);
        return condition;
    }
}","/**
 * Compiles a {@link BooleanExpression} into an {@link EventCondition}.
 * All compilation is globally {@code synchronized} on {@link EventCondition.Compiler#cache}
 * to minimize code size by avoiding compiling logically equivalent expressions in more than
 * one instance.
 *
 * @param expression BooleanExpress to compile
 * @return Compiled {@link EventCondition}
 */
", ,/** * Compiles a {@link BooleanExpression} into an {@link EventCondition}. * All compilation is globally {@code synchronized} on {@link EventCondition.Compiler#cache} * to minimize code size by avoiding compiling logically equivalent expressions in more than * one instance. * * @param expression BooleanExpress to compile * @return Compiled {@link EventCondition} */,110,141,[0],0,[0],0,[0],0,0,0,0,buildCondition(BooleanExpression),org.logstash.config.ir.compiler.EventCondition$Compiler,buildCondition/1[org.logstash.config.ir.expression.BooleanExpression],False,110,18,13,3,10,14,11,39,2,3,1,11,8,14,0,1,0,0,0,0,10,0,2,0,0,0,56,1,0,True
303,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\EventCondition.java,org.logstash.config.ir.compiler.EventCondition.Compiler,boolean vAndE(BinaryBooleanExpression),"/**
 * Checks if a {@link BinaryBooleanExpression} consists of a {@link ValueExpression} on the
 * left and a {@link EventValueExpression} on the right.
 *
 * @param expression Expression to check type for
 * @return True if the left branch of the {@link BinaryBooleanExpression} is a
 * {@link ValueExpression} and its right side is a {@link EventValueExpression}.
 */
private static boolean vAndE(final BinaryBooleanExpression expression) {
    return expression.getLeft() instanceof ValueExpression && expression.getRight() instanceof EventValueExpression;
}","/**
 * Checks if a {@link BinaryBooleanExpression} consists of a {@link ValueExpression} on the
 * left and a {@link EventValueExpression} on the right.
 *
 * @param expression Expression to check type for
 * @return True if the left branch of the {@link BinaryBooleanExpression} is a
 * {@link ValueExpression} and its right side is a {@link EventValueExpression}.
 */
", ,/** * Checks if a {@link BinaryBooleanExpression} consists of a {@link ValueExpression} on the * left and a {@link EventValueExpression} on the right. * * @param expression Expression to check type for * @return True if the left branch of the {@link BinaryBooleanExpression} is a * {@link ValueExpression} and its right side is a {@link EventValueExpression}. */,194,197,[0],0,[0],0,[0],0,0,0,0,vAndE(BinaryBooleanExpression),org.logstash.config.ir.compiler.EventCondition$Compiler,vAndE/1[org.logstash.config.ir.expression.BinaryBooleanExpression],False,194,4,5,3,2,1,2,3,1,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,27,10,0,True
304,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\EventCondition.java,org.logstash.config.ir.compiler.EventCondition.Compiler,"EventCondition in(ValueExpression, ValueExpression)","/**
 * Compiles a constant (due to both of its sides being constant {@link ValueExpression})
 * conditional.
 *
 * @param left  Constant left side {@link ValueExpression}
 * @param right Constant right side {@link ValueExpression}
 * @return Constant {@link EventCondition}
 */
private static EventCondition in(final ValueExpression left, final ValueExpression right) {
    final Object found = right.get();
    final Object other = left.get();
    final boolean res;
    if (found instanceof ConvertedList && other instanceof RubyString) {
        res = ((ConvertedList) found).stream().anyMatch(item -> item.toString().equals(other.toString()));
    } else if (found instanceof RubyString && other instanceof RubyString) {
        res = found.toString().contains(other.toString());
    } else if (found instanceof RubyString && other instanceof ConvertedList) {
        res = ((ConvertedList) other).stream().anyMatch(item -> item.toString().equals(found.toString()));
    } else {
        res = found != null && found.equals(other);
    }
    return constant(res);
}","/**
 * Compiles a constant (due to both of its sides being constant {@link ValueExpression})
 * conditional.
 *
 * @param left  Constant left side {@link ValueExpression}
 * @param right Constant right side {@link ValueExpression}
 * @return Constant {@link EventCondition}
 */
", ,/** * Compiles a constant (due to both of its sides being constant {@link ValueExpression}) * conditional. * * @param left  Constant left side {@link ValueExpression} * @param right Constant right side {@link ValueExpression} * @return Constant {@link EventCondition} */,341,357,[0],0,[0],0,[0],0,0,0,0,"in(ValueExpression, ValueExpression)",org.logstash.config.ir.compiler.EventCondition$Compiler,"in/2[org.logstash.config.ir.expression.ValueExpression,org.logstash.config.ir.expression.ValueExpression]",False,341,5,3,1,2,7,8,18,1,5,2,8,1,1,0,1,0,2,0,0,6,0,1,0,0,2,29,10,0,True
305,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\EventCondition.java,org.logstash.config.ir.compiler.EventCondition.Compiler,"boolean contains(ConvertedList, Object)","/**
 * Contains function using Ruby equivalent comparison logic.
 *
 * @param list  List to find value in
 * @param value Value to find in list
 * @return True iff value is in list
 */
private static boolean contains(final ConvertedList list, final Object value) {
    for (final Object element : list) {
        if (value.equals(element)) {
            return true;
        }
    }
    return false;
}","/**
 * Contains function using Ruby equivalent comparison logic.
 *
 * @param list  List to find value in
 * @param value Value to find in list
 * @return True iff value is in list
 */
", ,/** * Contains function using Ruby equivalent comparison logic. * * @param list  List to find value in * @param value Value to find in list * @return True iff value is in list */,468,475,[0],0,[0],0,[0],0,0,0,0,"contains(ConvertedList, Object)",org.logstash.config.ir.compiler.EventCondition$Compiler,"contains/2[org.logstash.ConvertedList,java.lang.Object]",False,468,1,3,3,0,3,1,8,2,0,2,1,0,0,1,0,0,0,0,0,0,0,2,0,0,0,22,10,0,True
306,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\EventCondition.java,org.logstash.config.ir.compiler.EventCondition.Compiler,"boolean matches(RubyString, RubyRegexp)","private static boolean matches(RubyString str, RubyRegexp regexp) {
    // match? returns true/false
    return regexp.match_p(RubyUtil.RUBY.getCurrentContext(), str).isTrue();
}", ,"// match? returns true/false
",// match? returns true/false,515,517,[0],0,[0],0,[0],0,0,0,0,"matches(RubyString, RubyRegexp)",org.logstash.config.ir.compiler.EventCondition$Compiler,"matches/2[org.logstash.config.ir.compiler.RubyString,org.logstash.config.ir.compiler.RubyRegexp]",False,515,2,2,2,0,1,3,3,1,0,2,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,10,0,False
307,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\FieldDefinition.java,org.logstash.config.ir.compiler.FieldDefinition,"FieldDefinition fromValue(int, Object)","/**
 * Create an immutable field with given value and at given index.
 * @param index Index for naming
 * @param value Object value of the field
 * @return Field definition
 */
public static FieldDefinition fromValue(final int index, final Object value) {
    return new FieldDefinition(variableDefinition(value.getClass(), index), false, null, value);
}","/**
 * Create an immutable field with given value and at given index.
 * @param index Index for naming
 * @param value Object value of the field
 * @return Field definition
 */
", ,/** * Create an immutable field with given value and at given index. * @param index Index for naming * @param value Object value of the field * @return Field definition */,43,47,[0],0,[0],0,[0],0,0,0,0,"fromValue(int, Object)",org.logstash.config.ir.compiler.FieldDefinition,"fromValue/2[int,java.lang.Object]",False,43,1,3,1,2,1,2,3,1,0,2,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,21,9,0,True
308,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\FieldDefinition.java,org.logstash.config.ir.compiler.FieldDefinition,"FieldDefinition mutableUnassigned(int, Class<?>)","/**
 * Creates a mutable field with given type and without an assigned value.
 * @param index Index for naming
 * @param type Type of the field
 * @return Field definition
 */
public static FieldDefinition mutableUnassigned(final int index, final Class<?> type) {
    return new FieldDefinition(variableDefinition(type, index), true, null, null);
}","/**
 * Creates a mutable field with given type and without an assigned value.
 * @param index Index for naming
 * @param type Type of the field
 * @return Field definition
 */
", ,/** * Creates a mutable field with given type and without an assigned value. * @param index Index for naming * @param type Type of the field * @return Field definition */,55,59,[0],0,[0],0,[0],0,0,0,0,"mutableUnassigned(int, Class<?>)",org.logstash.config.ir.compiler.FieldDefinition,"mutableUnassigned/2[int,java.lang.Class<?>]",False,55,1,3,1,2,1,1,3,1,0,2,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,22,9,0,True
309,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\FieldDefinition.java,org.logstash.config.ir.compiler.FieldDefinition,VariableDefinition asVariable(),"/**
 * Gets the {@link VariableDefinition} of the field.
 * @return Variable Definition
 */
public VariableDefinition asVariable() {
    return def;
}","/**
 * Gets the {@link VariableDefinition} of the field.
 * @return Variable Definition
 */
", ,/** * Gets the {@link VariableDefinition} of the field. * @return Variable Definition */,73,75,[0],0,[0],0,[0],0,0,0,0,asVariable(),org.logstash.config.ir.compiler.FieldDefinition,asVariable/0,False,73,1,2,2,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,1,0,True
310,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\FieldDefinition.java,org.logstash.config.ir.compiler.FieldDefinition,Object getCtorArgument(),"/**
 * Gets the value that is assigned to the field in the constructor if one is set
 * or {@code null} if none is set.
 * @return Constructor argument to be assigned to the field
 */
public Object getCtorArgument() {
    return ctorArgument;
}","/**
 * Gets the value that is assigned to the field in the constructor if one is set
 * or {@code null} if none is set.
 * @return Constructor argument to be assigned to the field
 */
", ,/** * Gets the value that is assigned to the field in the constructor if one is set * or {@code null} if none is set. * @return Constructor argument to be assigned to the field */,82,84,[0],0,[0],0,[0],0,0,0,0,getCtorArgument(),org.logstash.config.ir.compiler.FieldDefinition,getCtorArgument/0,False,82,0,4,4,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,23,1,0,True
311,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\JavaInputDelegatorExt.java,org.logstash.config.ir.compiler.JavaInputDelegatorExt,IRubyObject doStop(ThreadContext),"@JRubyMethod(name = ""do_stop"")
public IRubyObject doStop(final ThreadContext context) {
    try {
        input.stop();
        input.awaitStop();
    } catch (InterruptedException ex) {
        // do nothing
    }
    return this;
}", ,"// do nothing
",// do nothing,133,142,[0],0,[0],0,[0],0,0,0,0,doStop(ThreadContext),org.logstash.config.ir.compiler.JavaInputDelegatorExt,doStop/1[org.logstash.config.ir.compiler.ThreadContext],False,134,5,2,0,2,2,2,9,1,0,1,2,0,0,0,0,1,0,1,0,0,0,1,0,0,0,12,1,0,False
312,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\MethodSyntaxElement.java,org.logstash.config.ir.compiler.MethodSyntaxElement,"MethodSyntaxElement constructor(String, Closure)","/**
 * Builds a constructor from the given method body and arguments.
 * @param classname Name of the Class
 * @param body Constructor Method Body
 * @return Method Syntax
 */
static MethodSyntaxElement constructor(final String classname, final Closure body) {
    return new MethodSyntaxElement.MethodSyntaxElementImpl(classname, """", body, Collections.singletonList(ComputeStepSyntaxElement.CTOR_ARGUMENT));
}","/**
 * Builds a constructor from the given method body and arguments.
 * @param classname Name of the Class
 * @param body Constructor Method Body
 * @return Method Syntax
 */
", ,/** * Builds a constructor from the given method body and arguments. * @param classname Name of the Class * @param body Constructor Method Body * @return Method Syntax */,41,46,[0],0,[0],0,[0],0,0,0,0,"constructor(String, Closure)",org.logstash.config.ir.compiler.MethodSyntaxElement,"constructor/2[java.lang.String,org.logstash.config.ir.compiler.Closure]",False,41,3,2,1,1,1,1,3,1,0,2,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,19,8,0,True
313,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\MethodSyntaxElement.java,org.logstash.config.ir.compiler.MethodSyntaxElement,MethodSyntaxElement clear(Closure),"/**
 * Builds an implementation of {@link Dataset#clear()} from the given method body.
 * @param body Method Body
 * @return Method Syntax
 */
static MethodSyntaxElement clear(final Closure body) {
    return new MethodSyntaxElement.MethodSyntaxElementImpl(void.class, ""clear"", body);
}","/**
 * Builds an implementation of {@link Dataset#clear()} from the given method body.
 * @param body Method Body
 * @return Method Syntax
 */
", ,/** * Builds an implementation of {@link Dataset#clear()} from the given method body. * @param body Method Body * @return Method Syntax */,53,55,[0],0,[0],0,[0],0,0,0,0,clear(Closure),org.logstash.config.ir.compiler.MethodSyntaxElement,clear/1[org.logstash.config.ir.compiler.Closure],False,53,3,2,1,1,1,0,3,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,15,8,0,True
314,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\MethodSyntaxElement.java,org.logstash.config.ir.compiler.MethodSyntaxElement,MethodSyntaxElement compute(Closure),"/**
 * Builds an implementation of {@link Dataset#compute(RubyArray, boolean, boolean)} ()}
 * from the given method body.
 * @param body Method Body
 * @return Method Syntax
 */
static MethodSyntaxElement compute(final Closure body) {
    return new MethodSyntaxElement.MethodSyntaxElementImpl(Collection.class, ""compute"", body, new VariableDefinition(RubyArray.class, DatasetCompiler.BATCH_ARG), new VariableDefinition(boolean.class, DatasetCompiler.FLUSH_ARG), new VariableDefinition(boolean.class, DatasetCompiler.SHUTDOWN_ARG));
}","/**
 * Builds an implementation of {@link Dataset#compute(RubyArray, boolean, boolean)} ()}
 * from the given method body.
 * @param body Method Body
 * @return Method Syntax
 */
", ,"/** * Builds an implementation of {@link Dataset#compute(RubyArray, boolean, boolean)} ()} * from the given method body. * @param body Method Body * @return Method Syntax */",63,70,[0],0,[0],0,[0],0,0,0,0,compute(Closure),org.logstash.config.ir.compiler.MethodSyntaxElement,compute/1[org.logstash.config.ir.compiler.Closure],False,63,5,3,1,2,1,0,3,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,17,8,0,True
315,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\MethodSyntaxElement.java,org.logstash.config.ir.compiler.MethodSyntaxElement,MethodSyntaxElement right(ValueSyntaxElement),"/**
 * Builds an implementation of {@link SplitDataset#right()} given reference to the else branch's
 * event collection.
 * @param elseData Else Branch's Event Collection Syntax Element
 * @return Method Syntax
 */
static MethodSyntaxElement right(final ValueSyntaxElement elseData) {
    return new MethodSyntaxElement.MethodSyntaxElementImpl(Dataset.class, ""right"", Closure.wrap(SyntaxFactory.ret(elseData)));
}","/**
 * Builds an implementation of {@link SplitDataset#right()} given reference to the else branch's
 * event collection.
 * @param elseData Else Branch's Event Collection Syntax Element
 * @return Method Syntax
 */
", ,/** * Builds an implementation of {@link SplitDataset#right()} given reference to the else branch's * event collection. * @param elseData Else Branch's Event Collection Syntax Element * @return Method Syntax */,78,82,[0],0,[0],0,[0],0,0,0,0,right(ValueSyntaxElement),org.logstash.config.ir.compiler.MethodSyntaxElement,right/1[org.logstash.config.ir.compiler.ValueSyntaxElement],False,78,6,4,1,3,1,2,3,1,0,1,2,0,0,0,0,0,0,1,0,0,0,0,0,0,0,18,8,0,True
316,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\OutputStrategyExt.java,org.logstash.config.ir.compiler.OutputStrategyExt.SimpleAbstractOutputStrategyExt,"IRubyObject initialize(ThreadContext, IRubyObject[])","@JRubyMethod(required = 4)
public IRubyObject initialize(final ThreadContext context, final IRubyObject[] args) {
    final RubyClass outputClass = (RubyClass) args[0];
    final IRubyObject metric = args[1];
    final ExecutionContextExt executionContext = (ExecutionContextExt) args[2];
    final RubyHash pluginArgs = (RubyHash) args[3];
    // TODO: fixup mocks
    // Calling ""new"" here manually to allow mocking the ctor in RSpec Tests
    output = ContextualizerExt.initializePlugin(context, executionContext, outputClass, pluginArgs);
    initOutputCallsite(outputClass);
    output.callMethod(context, ""metric="", metric);
    return this;
}", ,"// TODO: fixup mocks
[[SEP]]// Calling ""new"" here manually to allow mocking the ctor in RSpec Tests
","// TODO: fixup mocks// Calling ""new"" here manually to allow mocking the ctor in RSpec Tests",253,266,[0],0,"[1, 0]",1,[1],1,1,1,1,"initialize(ThreadContext, IRubyObject[])",org.logstash.config.ir.compiler.OutputStrategyExt$SimpleAbstractOutputStrategyExt,"initialize/2[org.logstash.config.ir.compiler.ThreadContext,org.logstash.config.ir.compiler.IRubyObject[]]",False,254,9,2,0,2,1,3,10,1,4,2,3,0,0,0,0,0,0,1,5,5,0,0,0,0,0,20,1,0,False
317,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\SplitDataset.java,org.logstash.config.ir.compiler.SplitDataset,Dataset right(),"/**
 * {@link Dataset} representing the else branch of the conditional.
 * @return Else Branch Dataset
 */
Dataset right();","/**
 * {@link Dataset} representing the else branch of the conditional.
 * @return Else Branch Dataset
 */
", ,/** * {@link Dataset} representing the else branch of the conditional. * @return Else Branch Dataset */,32,32,[0],0,[0],0,[0],0,0,0,0,right(),org.logstash.config.ir.compiler.SplitDataset,right/0,False,28,1,2,2,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,0,0,True
318,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\SyntaxElement.java,org.logstash.config.ir.compiler.SyntaxElement,String generateCode(),"/**
 * @return Java code that can be compiled by Janino
 */
String generateCode();","/**
 * @return Java code that can be compiled by Janino
 */
", ,/** * @return Java code that can be compiled by Janino */,31,31,[0],0,[0],0,[0],0,0,0,0,generateCode(),org.logstash.config.ir.compiler.SyntaxElement,generateCode/0,False,28,0,10,10,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,0,0,True
319,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\SyntaxFactory.java,org.logstash.config.ir.compiler.SyntaxFactory,String join(String...),"/**
 * Joins given {@link String}s without delimiter.
 * @param parts Strings to join
 * @return Strings join without delimiter
 */
public static String join(final String... parts) {
    return String.join("""", parts);
}","/**
 * Joins given {@link String}s without delimiter.
 * @param parts Strings to join
 * @return Strings join without delimiter
 */
", ,/** * Joins given {@link String}s without delimiter. * @param parts Strings to join * @return Strings join without delimiter */,38,40,[0],0,[0],0,[0],0,0,0,0,join(String[]),org.logstash.config.ir.compiler.SyntaxFactory,join/1[java.lang.String[]],False,38,0,17,17,0,1,1,3,1,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,9,9,0,True
320,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\Utils.java,org.logstash.config.ir.compiler.Utils,"void copyNonCancelledEvents(Collection<JrubyEventExtLibrary.RubyEvent>, List)","@SuppressWarnings({ ""unchecked"", ""rawtypes"" })
public static // has field1.compute(batchArg, flushArg, shutdownArg) passed as input
void copyNonCancelledEvents(Collection<JrubyEventExtLibrary.RubyEvent> input, List output) {
    for (JrubyEventExtLibrary.RubyEvent e : input) {
        if (!(e.getEvent().isCancelled())) {
            output.add(e);
        }
    }
}", ,"// has field1.compute(batchArg, flushArg, shutdownArg) passed as input
","// has field1.compute(batchArg, flushArg, shutdownArg) passed as input",33,41,[0],0,[0],0,[0],0,0,0,0,"copyNonCancelledEvents(Collection<RubyEvent>, List)",org.logstash.config.ir.compiler.Utils,"copyNonCancelledEvents/2[java.util.Collection<org.logstash.ext.JrubyEventExtLibrary.RubyEvent>,java.util.List]",False,35,2,4,2,2,3,3,7,0,0,2,3,0,0,1,0,0,1,2,0,0,0,2,0,0,0,7,9,0,False
321,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\ValueSyntaxElement.java,org.logstash.config.ir.compiler.ValueSyntaxElement,"ValueSyntaxElement call(String, MethodLevelSyntaxElement...)","/**
 * Call method on instance.
 * @param method Method Name
 * @param args Arguments to pass to Method
 * @return Method Return
 */
default ValueSyntaxElement call(final String method, final MethodLevelSyntaxElement... args) {
    return new SyntaxFactory.MethodCallReturnValue(this, method, args);
}","/**
 * Call method on instance.
 * @param method Method Name
 * @param args Arguments to pass to Method
 * @return Method Return
 */
", ,/** * Call method on instance. * @param method Method Name * @param args Arguments to pass to Method * @return Method Return */,34,36,[0],0,[0],0,[0],0,0,0,0,"call(String, MethodLevelSyntaxElement[])",org.logstash.config.ir.compiler.ValueSyntaxElement,"call/2[java.lang.String,org.logstash.config.ir.compiler.MethodLevelSyntaxElement[]]",False,34,3,13,12,1,1,0,3,1,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,65536,0,True
322,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\VariableDefinition.java,org.logstash.config.ir.compiler.VariableDefinition,ValueSyntaxElement access(),"/**
 * Get a {@link ValueSyntaxElement} for accessing the variable.
 * @return Syntax element allowing access to the variable
 */
public ValueSyntaxElement access() {
    return SyntaxFactory.value(name);
}","/**
 * Get a {@link ValueSyntaxElement} for accessing the variable.
 * @return Syntax element allowing access to the variable
 */
", ,/** * Get a {@link ValueSyntaxElement} for accessing the variable. * @return Syntax element allowing access to the variable */,45,47,[0],0,[0],0,[0],0,0,0,0,access(),org.logstash.config.ir.compiler.VariableDefinition,access/0,False,45,2,4,3,1,1,1,3,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,13,1,0,True
323,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\VariableDefinition.java,org.logstash.config.ir.compiler.VariableDefinition,VariableDefinition rename(String),"/**
 * Create a copy of this instance with a new name but the same type.
 * @param newName New Name
 * @return Variable Definition with Adjusted Name
 */
public VariableDefinition rename(final String newName) {
    return new VariableDefinition(type, newName);
}","/**
 * Create a copy of this instance with a new name but the same type.
 * @param newName New Name
 * @return Variable Definition with Adjusted Name
 */
", ,/** * Create a copy of this instance with a new name but the same type. * @param newName New Name * @return Variable Definition with Adjusted Name */,54,56,[0],0,[0],0,[0],0,0,0,0,rename(String),org.logstash.config.ir.compiler.VariableDefinition,rename/1[java.lang.String],False,54,1,1,0,1,1,0,3,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,17,1,0,True
324,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\compiler\VariableDefinition.java,org.logstash.config.ir.compiler.VariableDefinition,Class<?> safeType(Class<?>),"/**
 * Determines a type that can be used in runtime compilable syntax. Types that are dynamically
 * compiled by Logstash or JRuby are filtered as their static parent types.
 * @param clazz Class to find safe type for
 * @return Safe type that can be used in syntax
 */
private static Class<?> safeType(final Class<?> clazz) {
    final Class<?> safe;
    if (EventCondition.class.isAssignableFrom(clazz)) {
        safe = EventCondition.class;
    } else if (DynamicMethod.class.isAssignableFrom(clazz)) {
        safe = DynamicMethod.class;
    } else if (Dataset.class.isAssignableFrom(clazz)) {
        safe = Dataset.class;
    } else {
        safe = clazz;
    }
    return safe;
}","/**
 * Determines a type that can be used in runtime compilable syntax. Types that are dynamically
 * compiled by Logstash or JRuby are filtered as their static parent types.
 * @param clazz Class to find safe type for
 * @return Safe type that can be used in syntax
 */
", ,/** * Determines a type that can be used in runtime compilable syntax. Types that are dynamically * compiled by Logstash or JRuby are filtered as their static parent types. * @param clazz Class to find safe type for * @return Safe type that can be used in syntax */,69,81,[0],0,[0],0,[0],0,0,0,0,safeType(Class<?>),org.logstash.config.ir.compiler.VariableDefinition,safeType/1[java.lang.Class<?>],False,69,3,1,1,0,4,3,16,1,1,1,3,0,0,0,0,0,0,0,0,4,0,1,0,0,0,31,10,0,True
325,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\expression\ExpressionSubstitution.java,org.logstash.config.ir.expression.ExpressionSubstitution,"Expression substituteBoolExpression(ConfigVariableExpander, Expression)","/**
 * Replace ${VAR:defaultValue} in BinaryBooleanExpression, UnaryBooleanExpression and ValueExpression, excluding RegexValueExpression
 * with the value in the following precedence: secret store, environment variable, default value
 * @param cve The actual pattern matching take place
 * @param expression The Expression to be substituted
 * @return substituted Expression
 */
public static Expression substituteBoolExpression(ConfigVariableExpander cve, Expression expression) {
    try {
        if (expression instanceof BinaryBooleanExpression) {
            BinaryBooleanExpression binaryBoolExp = (BinaryBooleanExpression) expression;
            Expression substitutedLeftExp = substituteBoolExpression(cve, binaryBoolExp.getLeft());
            Expression substitutedRightExp = substituteBoolExpression(cve, binaryBoolExp.getRight());
            if (substitutedLeftExp != binaryBoolExp.getLeft() || substitutedRightExp != binaryBoolExp.getRight()) {
                Constructor<? extends BinaryBooleanExpression> constructor = binaryBoolExp.getClass().getConstructor(SourceWithMetadata.class, Expression.class, Expression.class);
                return constructor.newInstance(binaryBoolExp.getSourceWithMetadata(), substitutedLeftExp, substitutedRightExp);
            }
        } else if (expression instanceof UnaryBooleanExpression) {
            UnaryBooleanExpression unaryBoolExp = (UnaryBooleanExpression) expression;
            Expression substitutedExp = substituteBoolExpression(cve, unaryBoolExp.getExpression());
            if (substitutedExp != unaryBoolExp.getExpression()) {
                Constructor<? extends UnaryBooleanExpression> constructor = unaryBoolExp.getClass().getConstructor(SourceWithMetadata.class, Expression.class);
                return constructor.newInstance(unaryBoolExp.getSourceWithMetadata(), substitutedExp);
            }
        } else if (expression instanceof ValueExpression && !(expression instanceof RegexValueExpression) && (((ValueExpression) expression).get() != null)) {
            Object expanded = CompiledPipeline.expandConfigVariableKeepingSecrets(cve, ((ValueExpression) expression).get());
            return new ValueExpression(expression.getSourceWithMetadata(), expanded);
        }
        return expression;
    } catch (NoSuchMethodException | InstantiationException | IllegalAccessException | InvocationTargetException | InvalidIRException e) {
        throw new IllegalStateException(""Unable to instantiate substituted condition expression"", e);
    }
}","/**
 * Replace ${VAR:defaultValue} in BinaryBooleanExpression, UnaryBooleanExpression and ValueExpression, excluding RegexValueExpression
 * with the value in the following precedence: secret store, environment variable, default value
 * @param cve The actual pattern matching take place
 * @param expression The Expression to be substituted
 * @return substituted Expression
 */
", ,"/** * Replace ${VAR:defaultValue} in BinaryBooleanExpression, UnaryBooleanExpression and ValueExpression, excluding RegexValueExpression * with the value in the following precedence: secret store, environment variable, default value * @param cve The actual pattern matching take place * @param expression The Expression to be substituted * @return substituted Expression */",19,45,[0],0,[0],0,[0],0,0,0,0,"substituteBoolExpression(ConfigVariableExpander, Expression)",org.logstash.config.ir.expression.ExpressionSubstitution,"substituteBoolExpression/2[org.logstash.plugins.ConfigVariableExpander,org.logstash.config.ir.expression.Expression]",False,19,11,15,7,8,10,10,29,4,8,2,10,1,0,0,4,1,4,1,0,8,0,3,0,0,0,57,9,0,True
326,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Edge.java,org.logstash.config.ir.graph.Edge,Stream<Edge> ancestors(),"public Stream<Edge> ancestors() {
    // Without all the distinct calls this can be slow
    return Stream.concat(this.from.incomingEdges(), this.from.incomingEdges().flatMap(Edge::ancestors).distinct()).distinct();
}", ,"// Without all the distinct calls this can be slow
",// Without all the distinct calls this can be slow,82,85,[0],0,[0],0,[0],0,0,0,0,ancestors(),org.logstash.config.ir.graph.Edge,ancestors/0,False,82,2,2,1,1,1,4,3,1,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,False
327,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Edge.java,org.logstash.config.ir.graph.Edge,Stream<Edge> descendants(),"public Stream<Edge> descendants() {
    // Without all the distinct calls this can be slow
    return Stream.concat(this.to.outgoingEdges(), this.to.outgoingEdges().flatMap(Edge::ancestors).distinct()).distinct();
}", ,"// Without all the distinct calls this can be slow
",// Without all the distinct calls this can be slow,87,90,[0],0,[0],0,[0],0,0,0,0,descendants(),org.logstash.config.ir.graph.Edge,descendants/0,False,87,2,2,1,1,1,4,3,1,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,False
328,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,"void addVertex(Vertex, boolean)","private void addVertex(Vertex v, boolean doRefresh) throws InvalidIRException {
    // If this belongs to another graph use a copy
    if (v.getGraph() != null && v.getGraph() != this) {
        throw new InvalidIRException(""Attempted to add vertex already belonging to a graph!"");
    }
    v.setGraph(this);
    this.vertices.add(v);
    if (doRefresh)
        this.refresh();
}", ,"// If this belongs to another graph use a copy
",// If this belongs to another graph use a copy,63,74,[0],0,[0],0,[0],0,0,0,0,"addVertex(Vertex, boolean)",org.logstash.config.ir.graph.Graph,"addVertex/2[org.logstash.config.ir.graph.Vertex,boolean]",False,63,3,7,3,4,4,4,8,0,0,2,4,1,4,0,2,0,0,1,0,0,0,1,0,0,0,16,2,0,False
329,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,Vertex importVertex(Vertex),"// Takes an arbitrary vertex from any graph and brings it into this one.
// It may have to copy it. The actual vertex that gets used is returned
public Vertex importVertex(Vertex v) throws InvalidIRException {
    if (v.getGraph() != this) {
        if (v.getGraph() == null) {
            this.addVertex(v);
            return v;
        } else {
            Vertex copy = v.copy();
            this.addVertex(copy);
            return copy;
        }
    } else {
        return v;
    }
}","// It may have to copy it. The actual vertex that gets used is returned
", ,// Takes an arbitrary vertex from any graph and brings it into this one.// It may have to copy it. The actual vertex that gets used is returned,78,91,[0],0,[0],0,[0],0,0,0,0,importVertex(Vertex),org.logstash.config.ir.graph.Graph,importVertex/1[org.logstash.config.ir.graph.Vertex],False,78,2,4,1,3,3,3,16,3,1,1,3,1,6,0,2,0,0,0,0,1,0,2,0,0,0,9,1,0,False
330,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,Graph copy(),"// Returns a copy of this graph
public Graph copy() throws InvalidIRException {
    return Graph.combine(this).graph;
}","// Returns a copy of this graph
", ,// Returns a copy of this graph,128,130,[0],0,[0],0,[0],0,0,0,0,copy(),org.logstash.config.ir.graph.Graph,copy/0,False,128,1,5,4,1,1,1,3,1,0,0,1,1,2,0,0,0,0,0,0,0,0,0,0,0,0,6,1,0,False
331,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,GraphCombinationResult combine(Graph...),"// Returns a new graph that is the union of all provided graphs.
// If a single graph is passed in this will return a copy of it
public static GraphCombinationResult combine(Graph... graphs) throws InvalidIRException {
    Map<Vertex, Vertex> oldToNewVertices = new LinkedHashMap<>();
    Map<Edge, Edge> oldToNewEdges = new LinkedHashMap<>();
    for (Graph graph : graphs) {
        graph.vertices().forEachOrdered(v -> oldToNewVertices.put(v, v.copy()));
        for (Edge e : graph.getEdges()) {
            Edge copy = e.copy(oldToNewVertices.get(e.getFrom()), oldToNewVertices.get(e.getTo()));
            oldToNewEdges.put(e, copy);
        }
    }
    Graph newGraph = new Graph(oldToNewVertices.values(), oldToNewEdges.values());
    return new GraphCombinationResult(newGraph, oldToNewVertices, oldToNewEdges);
}","// If a single graph is passed in this will return a copy of it
", ,// Returns a new graph that is the union of all provided graphs.// If a single graph is passed in this will return a copy of it,134,149,[0],0,[0],0,[0],0,0,0,0,combine(Graph[]),org.logstash.config.ir.graph.Graph,combine/1[org.logstash.config.ir.graph.Graph[]],False,134,4,13,5,8,3,12,13,1,5,1,12,2,1,2,0,0,0,0,0,4,0,2,0,0,1,20,9,0,False
332,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,Graph chain(Graph),"/*
      Return a copy of this graph with the other graph's nodes to this one by connection this graph's leaves to
      the other graph's root
    */
public Graph chain(Graph otherGraph) throws InvalidIRException {
    if (otherGraph.vertices.isEmpty())
        return this.copy();
    if (this.isEmpty())
        return otherGraph.copy();
    GraphCombinationResult combineResult = Graph.combine(this, otherGraph);
    // Build these lists here since we do mutate the graph in place later
    // This isn't strictly necessary, but makes things less confusing
    Collection<Vertex> fromLeaves = allLeaves().map(combineResult.oldToNewVertices::get).collect(Collectors.toList());
    Collection<Vertex> toRoots = otherGraph.roots().map(combineResult.oldToNewVertices::get).collect(Collectors.toList());
    return combineResult.graph.chain(fromLeaves, toRoots);
}","/*
      Return a copy of this graph with the other graph's nodes to this one by connection this graph's leaves to
      the other graph's root
    */
","// Build these lists here since we do mutate the graph in place later
[[SEP]]// This isn't strictly necessary, but makes things less confusing
","/*      Return a copy of this graph with the other graph's nodes to this one by connection this graph's leaves to      the other graph's root    */[[SEP]]// Build these lists here since we do mutate the graph in place later// This isn't strictly necessary, but makes things less confusing",167,179,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,chain(Graph),org.logstash.config.ir.graph.Graph,chain/1[org.logstash.config.ir.graph.Graph],False,167,3,11,5,6,3,10,8,3,3,1,10,6,12,0,0,0,0,0,0,3,0,1,0,0,0,15,1,0,False
333,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,"Graph chain(Collection<Vertex>, Collection<Vertex>)","// This does *not* return a copy for performance reasons
private Graph chain(Collection<Vertex> fromLeaves, Collection<Vertex> toVertices) throws InvalidIRException {
    for (Vertex leaf : fromLeaves) {
        for (Edge.EdgeFactory unusedEf : leaf.getUnusedOutgoingEdgeFactories()) {
            for (Vertex toVertex : toVertices) {
                this.chainVertices(unusedEf, leaf, toVertex);
            }
        }
    }
    return this;
}","// This does *not* return a copy for performance reasons
", ,// This does *not* return a copy for performance reasons,187,197,[0],0,[0],0,[0],0,0,0,0,"chain(Collection<Vertex>, Collection<Vertex>)",org.logstash.config.ir.graph.Graph,"chain/2[java.util.Collection<org.logstash.config.ir.graph.Vertex>,java.util.Collection<org.logstash.config.ir.graph.Vertex>]",False,187,2,4,2,2,4,2,10,1,0,2,2,1,8,3,0,0,0,0,0,0,0,3,0,0,0,14,2,0,False
334,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,"Collection<Edge> chainVerticesUnsafe(Edge.EdgeFactory, Vertex...)","// Will not validate the graph after running!
// You must invoke validate the graph yourself
// after invoking
public Collection<Edge> chainVerticesUnsafe(Edge.EdgeFactory edgeFactory, Vertex... argVertices) throws InvalidIRException {
    List<Vertex> importedVertices = new ArrayList<>(argVertices.length);
    for (Vertex va : argVertices) {
        importedVertices.add(this.importVertex(va));
    }
    List<Edge> newEdges = new ArrayList<>();
    for (int i = 0; i < importedVertices.size() - 1; i++) {
        Vertex from = importedVertices.get(i);
        Vertex to = importedVertices.get(i + 1);
        this.addVertex(from, false);
        this.addVertex(to, false);
        Edge edge = edgeFactory.make(from, to);
        newEdges.add(edge);
        this.addEdge(edge, false);
    }
    refresh();
    return newEdges;
}","// after invoking
", ,// Will not validate the graph after running!// You must invoke validate the graph yourself// after invoking,217,239,[0],0,[0],0,[0],0,0,0,0,"chainVerticesUnsafe(EdgeFactory, Vertex[])",org.logstash.config.ir.graph.Graph,"chainVerticesUnsafe/2[org.logstash.config.ir.graph.Edge.EdgeFactory,org.logstash.config.ir.graph.Vertex[]]",False,217,4,7,2,5,3,9,18,1,6,2,9,4,6,2,0,0,0,0,3,6,2,1,0,0,0,19,1,0,False
335,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,void refresh(),"// Many of the operations we perform involve modifying one graph by adding vertices/edges
// from another. This method ensures that all the vertices/edges we know about having been pulled into
// this graph. Methods in this class that add or remove externally provided vertices/edges
// should call this method to ensure that the rest of the graph these items depend on are pulled
// in.
public void refresh() throws InvalidIRException {
    this.calculateRanks();
    this.vertices.forEach(Vertex::clearCache);
    this.calculateTopologicalSort();
}","// in.
", ,// Many of the operations we perform involve modifying one graph by adding vertices/edges// from another. This method ensures that all the vertices/edges we know about having been pulled into// this graph. Methods in this class that add or remove externally provided vertices/edges// should call this method to ensure that the rest of the graph these items depend on are pulled// in.,264,268,[0],0,[0],0,[0],0,0,0,0,refresh(),org.logstash.config.ir.graph.Graph,refresh/0,False,264,1,6,4,2,1,3,5,0,0,0,3,2,3,0,0,0,0,0,0,0,0,0,0,0,0,5,1,0,False
336,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,Integer rank(Vertex),"public Integer rank(Vertex vertex) {
    Integer rank = vertexRanks.get(vertex);
    // This should never happen
    if (rank == null)
        throw new RuntimeException(""Attempted to get rank from vertex where it is not yet calculated: "" + this);
    return rank;
}", ,"// This should never happen
",// This should never happen,282,287,[0],0,[0],0,[0],0,0,0,0,rank(Vertex),org.logstash.config.ir.graph.Graph,rank/1[org.logstash.config.ir.graph.Vertex],False,282,1,1,1,0,2,1,5,1,1,1,1,0,0,0,1,0,0,1,0,1,1,1,0,0,0,15,1,0,False
337,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,void validate(),"public void validate() throws InvalidIRException {
    if (this.isEmpty())
        return;
    if (this.getVertices().stream().noneMatch(Vertex::isLeaf)) {
        throw new InvalidIRException(""Graph has no leaf vertices!\n"" + this.toString());
    }
    // Check for duplicate IDs in the config
    List<String> duplicateIdErrorMessages = this.vertices().collect(Collectors.groupingBy(Vertex::getId)).values().stream().filter(group -> group.size() > 1).map(group -> {
        return ""ID: "" + group.stream().findAny().get().getId() + "" "" + group.stream().map(Object::toString).collect(Collectors.joining(""\n""));
    }).collect(Collectors.toList());
    if (!duplicateIdErrorMessages.isEmpty()) {
        String dupeErrors = duplicateIdErrorMessages.stream().collect(Collectors.joining(""\n""));
        throw new InvalidIRException(""Config has duplicate Ids: \n"" + dupeErrors);
    }
}", ,"// Check for duplicate IDs in the config
",// Check for duplicate IDs in the config,289,312,[0],0,[0],0,[0],0,0,0,0,validate(),org.logstash.config.ir.graph.Graph,validate/0,False,289,3,12,6,6,5,23,14,2,4,0,23,4,4,0,0,0,0,6,1,2,3,1,0,0,2,16,1,0,False
338,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,Stream<Vertex> allLeaves(),"// Vertices which are partially leaves in that they support multiple
// outgoing edge types but only have one or fewer attached
public Stream<Vertex> allLeaves() {
    return vertices().filter(Vertex::isPartialLeaf);
}","// outgoing edge types but only have one or fewer attached
", ,// Vertices which are partially leaves in that they support multiple// outgoing edge types but only have one or fewer attached,325,327,[0],0,[0],0,[0],0,0,0,0,allLeaves(),org.logstash.config.ir.graph.Graph,allLeaves/0,False,325,2,4,3,1,1,2,3,1,0,0,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,3,1,0,False
339,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,Collection<Vertex> getAllLeaves(),"// Get all leaves whether partial or not
public Collection<Vertex> getAllLeaves() {
    return allLeaves().collect(Collectors.toList());
}","// Get all leaves whether partial or not
", ,// Get all leaves whether partial or not,330,332,[0],0,[0],0,[0],0,0,0,0,getAllLeaves(),org.logstash.config.ir.graph.Graph,getAllLeaves/0,False,330,2,2,1,1,1,3,3,1,0,0,3,1,2,0,0,0,0,0,0,0,0,0,0,0,0,4,1,0,False
340,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,boolean hasEquivalentEdge(Edge),"// returns true if this graph has a .sourceComponentEquals equivalent edge
public boolean hasEquivalentEdge(Edge otherE) {
    return edges().anyMatch(e -> e.sourceComponentEquals(otherE));
}","// returns true if this graph has a .sourceComponentEquals equivalent edge
", ,// returns true if this graph has a .sourceComponentEquals equivalent edge,408,410,[0],0,[0],0,[0],0,0,0,0,hasEquivalentEdge(Edge),org.logstash.config.ir.graph.Graph,hasEquivalentEdge/1[org.logstash.config.ir.graph.Edge],False,408,2,3,1,2,1,3,3,1,1,1,3,1,1,0,0,0,0,0,0,0,0,0,0,0,1,7,1,0,False
341,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Graph.java,org.logstash.config.ir.graph.Graph,String uniqueHash(),"public String uniqueHash() {
    return Util.digest(this.vertices().filter(// has no metadata
    v -> !(v instanceof QueueVertex) && !(v instanceof SeparatorVertex)).map(Vertex::getSourceWithMetadata).map(SourceWithMetadata::uniqueHash).sorted().collect(Collectors.joining()));
}", ,"// has no metadata
",// has no metadata,433,440,[0],0,[0],0,[0],0,0,0,0,uniqueHash(),org.logstash.config.ir.graph.Graph,uniqueHash/0,False,433,5,7,5,2,1,8,3,1,1,0,8,1,1,0,0,0,2,0,0,0,0,0,0,0,1,6,1,0,False
342,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\IfVertex.java,org.logstash.config.ir.graph.IfVertex,boolean sourceComponentEquals(SourceComponent),"@Override
public boolean sourceComponentEquals(SourceComponent other) {
    if (other == null)
        return false;
    if (other == this)
        return true;
    if (other instanceof IfVertex) {
        IfVertex otherV = (IfVertex) other;
        // We don't check the id because we're comparing functional equality, not
        // identity
        return otherV.booleanExpression.sourceComponentEquals(this.booleanExpression);
    }
    return false;
}", ,"// We don't check the id because we're comparing functional equality, not
[[SEP]]// identity
","// We don't check the id because we're comparing functional equality, not// identity",48,59,[0],0,"[0, 0]",0,[0],0,0,0,0,sourceComponentEquals(SourceComponent),org.logstash.config.ir.graph.IfVertex,sourceComponentEquals/1[org.logstash.config.ir.SourceComponent],False,49,3,1,0,1,4,1,9,4,1,1,1,0,0,0,2,0,0,0,0,1,0,1,0,0,0,11,1,0,False
343,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\IfVertex.java,org.logstash.config.ir.graph.IfVertex,boolean hasEdgeType(boolean),"public boolean hasEdgeType(boolean type) {
    for (Edge e : getOutgoingEdges()) {
        // There should only  be boolean edges here!
        BooleanEdge bEdge = (BooleanEdge) e;
        if (bEdge.getEdgeType() == type)
            return true;
    }
    return false;
}", ,"// There should only  be boolean edges here!
",// There should only  be boolean edges here!,61,67,[0],0,[0],0,[0],0,0,0,0,hasEdgeType(boolean),org.logstash.config.ir.graph.IfVertex,hasEdgeType/1[boolean],False,61,2,4,2,2,3,2,7,2,1,1,2,0,0,1,1,0,0,0,0,1,0,2,0,0,0,12,1,0,False
344,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\IfVertex.java,org.logstash.config.ir.graph.IfVertex,String humanReadableExpression(),"// The easiest readable version of this for a human.
// If the original source is available we use that, otherwise we serialize the expression
public String humanReadableExpression() {
    String sourceText = this.booleanExpression.getSourceWithMetadata() != null ? this.booleanExpression.getSourceWithMetadata().getText() : null;
    if (sourceText != null) {
        return sourceText;
    } else {
        return this.getBooleanExpression().toRubyString();
    }
}","// If the original source is available we use that, otherwise we serialize the expression
", ,"// The easiest readable version of this for a human.// If the original source is available we use that, otherwise we serialize the expression",86,93,[0],0,[0],0,[0],0,0,0,0,humanReadableExpression(),org.logstash.config.ir.graph.IfVertex,humanReadableExpression/0,False,86,4,4,0,4,3,4,9,2,1,0,4,1,1,0,2,0,0,0,0,1,0,1,0,0,0,6,1,0,False
345,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\PluginVertex.java,org.logstash.config.ir.graph.PluginVertex,boolean sourceComponentEquals(SourceComponent),"@Override
public boolean sourceComponentEquals(SourceComponent other) {
    if (other == null)
        return false;
    if (other == this)
        return true;
    if (other instanceof PluginVertex) {
        PluginVertex otherV = (PluginVertex) other;
        // We don't test ID equality because we're testing
        // Semantics, and ids have nothing to do with that
        return otherV.getPluginDefinition().sourceComponentEquals(this.getPluginDefinition());
    }
    return false;
}", ,"// We don't test ID equality because we're testing
[[SEP]]// Semantics, and ids have nothing to do with that
","// We don't test ID equality because we're testing// Semantics, and ids have nothing to do with that",50,61,[0],0,"[0, 0]",0,[0],0,0,0,0,sourceComponentEquals(SourceComponent),org.logstash.config.ir.graph.PluginVertex,sourceComponentEquals/1[org.logstash.config.ir.SourceComponent],False,51,3,2,0,2,4,2,9,4,1,1,2,1,1,0,2,0,0,0,0,1,0,1,0,0,0,11,1,0,False
346,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\QueueVertex.java,org.logstash.config.ir.graph.QueueVertex,QueueVertex copy(),"@Override
public QueueVertex copy() {
    try {
        return new QueueVertex();
    } catch (IncompleteSourceWithMetadataException e) {
        // Never happens
        throw new RuntimeException(e);
    }
}", ,"// Never happens
",// Never happens,43,51,[0],0,[0],0,[0],0,0,0,0,copy(),org.logstash.config.ir.graph.QueueVertex,copy/0,False,44,1,1,0,1,2,0,8,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,10,1,0,False
347,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\QueueVertex.java,org.logstash.config.ir.graph.QueueVertex,SourceWithMetadata getSourceWithMetadata(),"// Special vertices really have no metadata
@Override
public SourceWithMetadata getSourceWithMetadata() {
    return null;
}","// Special vertices really have no metadata
", ,// Special vertices really have no metadata,59,62,[0],0,[0],0,[0],0,0,0,0,getSourceWithMetadata(),org.logstash.config.ir.graph.QueueVertex,getSourceWithMetadata/0,False,60,1,0,0,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,1,0,False
348,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\SeparatorVertex.java,org.logstash.config.ir.graph.SeparatorVertex,SeparatorVertex copy(),"@Override
public SeparatorVertex copy() {
    try {
        return new SeparatorVertex(this.getId());
    } catch (IncompleteSourceWithMetadataException e) {
        // Never happens
        throw new RuntimeException(e);
    }
}", ,"// Never happens
",// Never happens,39,47,[0],0,[0],0,[0],0,0,0,0,copy(),org.logstash.config.ir.graph.SeparatorVertex,copy/0,False,40,2,2,0,2,2,1,8,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,10,1,0,False
349,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\SeparatorVertex.java,org.logstash.config.ir.graph.SeparatorVertex,SourceWithMetadata getSourceWithMetadata(),"// Special vertices really have no metadata
@Override
public SourceWithMetadata getSourceWithMetadata() {
    return null;
}","// Special vertices really have no metadata
", ,// Special vertices really have no metadata,55,58,[0],0,[0],0,[0],0,0,0,0,getSourceWithMetadata(),org.logstash.config.ir.graph.SeparatorVertex,getSourceWithMetadata/0,False,56,1,0,0,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,1,0,False
350,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Vertex.java,org.logstash.config.ir.graph.Vertex,int rank(),"// Rank is the shortest distance to a root for this vertex
public int rank() {
    return this.graph.rank(this);
}","// Rank is the shortest distance to a root for this vertex
", ,// Rank is the shortest distance to a root for this vertex,167,169,[0],0,[0],0,[0],0,0,0,0,rank(),org.logstash.config.ir.graph.Vertex,rank/0,False,167,1,1,0,1,1,1,3,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,False
351,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Vertex.java,org.logstash.config.ir.graph.Vertex,String uniqueHash(),"@Override
public String uniqueHash() {
    if (this.hashCache != null) {
        return this.hashCache;
    }
    if (this.getSourceWithMetadata() != null) {
        return this.getSourceWithMetadata().uniqueHash();
    } else {
        // This should never happen outside of the test suite where we construct pipelines
        // without source metadata
        throw new RuntimeException(""Attempted to compute unique hash on a vertex with no source metadata!"");
    }
}", ,"// This should never happen outside of the test suite where we construct pipelines
[[SEP]]// without source metadata
",// This should never happen outside of the test suite where we construct pipelines// without source metadata,171,184,[0],0,"[0, 0]",0,[0],0,0,0,0,uniqueHash(),org.logstash.config.ir.graph.Vertex,uniqueHash/0,False,172,2,4,2,2,3,2,11,2,0,0,2,1,1,0,2,0,0,1,0,0,0,1,0,0,0,15,1,0,False
352,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Vertex.java,org.logstash.config.ir.graph.Vertex,Collection<Edge.EdgeFactory> getUnusedOutgoingEdgeFactories(),"// Can be overridden in subclasses to define multiple
// expected Edge classes this Vertex can take.
// If any EdgeFactory instances are returned this Vertex is considered
// a partial leaf.
public Collection<Edge.EdgeFactory> getUnusedOutgoingEdgeFactories() {
    if (!this.hasOutgoingEdges()) {
        return Collections.singletonList(PlainEdge.factory);
    }
    return Collections.emptyList();
}","// a partial leaf.
", ,// Can be overridden in subclasses to define multiple// expected Edge classes this Vertex can take.// If any EdgeFactory instances are returned this Vertex is considered// a partial leaf.,195,200,[0],0,[0],0,[0],0,0,0,0,getUnusedOutgoingEdgeFactories(),org.logstash.config.ir.graph.Vertex,getUnusedOutgoingEdgeFactories/0,False,195,2,5,4,1,2,3,6,2,0,0,3,1,3,0,0,0,0,0,0,0,0,1,0,0,0,5,1,0,False
353,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\Vertex.java,org.logstash.config.ir.graph.Vertex,String getId(),"public String getId() {
    if (explicitId != null)
        return explicitId;
    if (generatedId != null)
        return generatedId;
    if (this.getGraph() == null) {
        throw new RuntimeException(""Attempted to get ID from PluginVertex before attaching it to a graph!"");
    }
    // Generating unique hashes for vertices is very slow!
    // We try to avoid this where possible, which means that generally only tests hit the path with hashes, since
    // they have no source metadata. This might also be used in the future by alternate config languages which are
    // willing to take the hit.
    if (this.getSourceWithMetadata() != null) {
        generatedId = Util.digest(this.graph.uniqueHash() + ""|"" + this.getSourceWithMetadata().uniqueHash());
    } else {
        generatedId = this.uniqueHash();
    }
    return generatedId;
}", ,"// Generating unique hashes for vertices is very slow!
[[SEP]]// We try to avoid this where possible, which means that generally only tests hit the path with hashes, since
[[SEP]]// they have no source metadata. This might also be used in the future by alternate config languages which are
[[SEP]]// willing to take the hit.
","// Generating unique hashes for vertices is very slow!// We try to avoid this where possible, which means that generally only tests hit the path with hashes, since// they have no source metadata. This might also be used in the future by alternate config languages which are// willing to take the hit.",214,233,[0],0,"[0, 0, 0, 0]",0,[0],0,0,0,0,getId(),org.logstash.config.ir.graph.Vertex,getId/0,False,214,4,24,18,6,5,6,14,3,0,0,6,3,2,0,4,0,0,2,0,2,1,1,0,0,0,18,1,0,False
354,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\algorithms\BreadthFirst.java,org.logstash.config.ir.graph.algorithms.BreadthFirst,"BfsResult breadthFirst(Collection<Vertex>, boolean, Consumer<Map.Entry<Vertex, Integer>>)","/* This isn't as pretty as the DFS search with its streaminess, but for our current uses we only really
    *  care about using this to get the calculated vertexDistances, so that's fine. */
public static BfsResult breadthFirst(Collection<Vertex> roots, boolean reverse, Consumer<Map.Entry<Vertex, Integer>> consumer) {
    Map<Vertex, Integer> vertexDistances = new HashMap<>();
    Map<Vertex, Vertex> vertexParents = new HashMap<>();
    Deque<Vertex> queue = new ArrayDeque<>(roots);
    roots.forEach(v -> vertexDistances.put(v, 0));
    while (!queue.isEmpty()) {
        Vertex currentVertex = queue.removeFirst();
        Integer currentDistance = vertexDistances.get(currentVertex);
        if (consumer != null) {
            consumer.accept(new AbstractMap.SimpleImmutableEntry<>(currentVertex, currentDistance));
        }
        Stream<Vertex> nextVertices = reverse ? currentVertex.incomingVertices() : currentVertex.outgoingVertices();
        nextVertices.forEach(nextVertex -> {
            if (vertexDistances.get(nextVertex) == null) {
                vertexDistances.put(nextVertex, currentDistance + 1);
                vertexParents.put(nextVertex, currentVertex);
                queue.push(nextVertex);
            }
        });
    }
    return new BfsResult(vertexDistances);
}","/* This isn't as pretty as the DFS search with its streaminess, but for our current uses we only really
    *  care about using this to get the calculated vertexDistances, so that's fine. */
", ,"/* This isn't as pretty as the DFS search with its streaminess, but for our current uses we only really    *  care about using this to get the calculated vertexDistances, so that's fine. */",36,65,[0],0,[0],0,[0],0,0,0,0,"breadthFirst(Collection<Vertex>, boolean, Consumer<Entry<Vertex, Integer>>)",org.logstash.config.ir.graph.algorithms.BreadthFirst,"breadthFirst/3[java.util.Collection<org.logstash.config.ir.graph.Vertex>,boolean,java.util.function.Consumer<java.util.Map.Entry<org.logstash.config.ir.graph.Vertex,java.lang.Integer>>]",False,39,2,5,2,3,5,11,23,1,8,3,11,0,0,1,2,0,0,0,2,6,1,3,0,0,2,19,9,0,False
355,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\graph\algorithms\TopologicalSort.java,org.logstash.config.ir.graph.algorithms.TopologicalSort,List<Vertex> sortVertices(Graph),"// Uses Kahn's algorithm to do a topological sort and detect cycles
public static List<Vertex> sortVertices(Graph g) throws UnexpectedGraphCycleError {
    if (g.getEdges().size() == 0)
        return new ArrayList<>(g.getVertices());
    List<Vertex> sorted = new ArrayList<>(g.getVertices().size());
    Deque<Vertex> pending = new LinkedList<>();
    pending.addAll(g.getRoots());
    Set<Edge> traversedEdges = new HashSet<>();
    while (!pending.isEmpty()) {
        Vertex currentVertex = pending.removeFirst();
        sorted.add(currentVertex);
        currentVertex.getOutgoingEdges().forEach(edge -> {
            traversedEdges.add(edge);
            Vertex toVertex = edge.getTo();
            if (toVertex.getIncomingEdges().stream().allMatch(traversedEdges::contains)) {
                pending.add(toVertex);
            }
        });
    }
    // Check for cycles
    if (g.edges().noneMatch(traversedEdges::contains)) {
        throw new UnexpectedGraphCycleError(g);
    }
    return sorted;
}","// Uses Kahn's algorithm to do a topological sort and detect cycles
","// Check for cycles
",// Uses Kahn's algorithm to do a topological sort and detect cycles[[SEP]]// Check for cycles,39,68,[0],0,[0],0,"[0, 0]",0,0,0,0,sortVertices(Graph),org.logstash.config.ir.graph.algorithms.TopologicalSort,sortVertices/1[org.logstash.config.ir.graph.Graph],False,39,4,10,2,8,5,19,23,2,6,1,19,0,0,1,1,0,0,0,1,5,0,3,0,0,1,15,9,0,False
356,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\config\ir\imperative\IfStatement.java,org.logstash.config.ir.imperative.IfStatement,Graph toGraph(ConfigVariableExpander),"@Override
public Graph toGraph(ConfigVariableExpander cve) throws InvalidIRException {
    Graph trueGraph = getTrueStatement().toGraph(cve);
    Graph falseGraph = getFalseStatement().toGraph(cve);
    // If there is nothing in the true or false sections of this if statement,
    // we can omit the if statement altogether!
    if (trueGraph.isEmpty() && falseGraph.isEmpty()) {
        return new Graph();
    }
    Graph.GraphCombinationResult combination = Graph.combine(trueGraph, falseGraph);
    Graph newGraph = combination.graph;
    Collection<Vertex> trueRoots = trueGraph.roots().map(combination.oldToNewVertices::get).collect(Collectors.toList());
    Collection<Vertex> falseRoots = falseGraph.roots().map(combination.oldToNewVertices::get).collect(Collectors.toList());
    IfVertex ifVertex = new IfVertex(this.getSourceWithMetadata(), (BooleanExpression) ExpressionSubstitution.substituteBoolExpression(cve, this.booleanExpression));
    newGraph.addVertex(ifVertex);
    for (Vertex v : trueRoots) {
        newGraph.chainVerticesUnsafe(BooleanEdge.trueFactory, ifVertex, v);
    }
    for (Vertex v : falseRoots) {
        newGraph.chainVerticesUnsafe(BooleanEdge.falseFactory, ifVertex, v);
    }
    return newGraph;
}", ,"// If there is nothing in the true or false sections of this if statement,
[[SEP]]// we can omit the if statement altogether!
","// If there is nothing in the true or false sections of this if statement,// we can omit the if statement altogether!",102,131,[0],0,"[0, 0]",0,[0],0,0,0,0,toGraph(ConfigVariableExpander),org.logstash.config.ir.imperative.IfStatement,toGraph/1[org.logstash.plugins.ConfigVariableExpander],False,103,10,19,7,12,5,13,20,2,7,1,13,2,1,2,0,0,0,0,0,7,0,1,0,0,0,25,1,0,False
357,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\AbstractPipelineExt.java,org.logstash.execution.AbstractPipelineExt,IRubyObject openQueue(ThreadContext),"/**
 * queue opening needs to happen out of the initialize method because the
 * AbstractPipeline is used for pipeline config validation and the queue
 * should not be opened for this. This should be called only in the actual
 * Pipeline/JavaPipeline initialisation.
 * @param context ThreadContext
 * @return Nil
 */
@JRubyMethod(name = ""open_queue"")
public final IRubyObject openQueue(final ThreadContext context) {
    try {
        queue = QueueFactoryExt.create(context, null, settings);
    } catch (final Exception ex) {
        LOGGER.error(""Logstash failed to create queue."", ex);
        throw new IllegalStateException(ex);
    }
    inputQueueClient = queue.writeClient(context);
    filterQueueClient = queue.readClient();
    filterQueueClient.setEventsMetric(metric.namespace(context, EVENTS_METRIC_NAMESPACE));
    filterQueueClient.setPipelineMetric(metric.namespace(context, RubyArray.newArray(context.runtime, new IRubyObject[] { STATS_KEY, PIPELINES_KEY, pipelineId.convertToString().intern(), EVENTS_KEY })));
    return context.nil;
}","/**
 * queue opening needs to happen out of the initialize method because the
 * AbstractPipeline is used for pipeline config validation and the queue
 * should not be opened for this. This should be called only in the actual
 * Pipeline/JavaPipeline initialisation.
 * @param context ThreadContext
 * @return Nil
 */
", ,/** * queue opening needs to happen out of the initialize method because the * AbstractPipeline is used for pipeline config validation and the queue * should not be opened for this. This should be called only in the actual * Pipeline/JavaPipeline initialisation. * @param context ThreadContext * @return Nil */,197,225,[0],0,[0],0,[0],0,0,0,0,openQueue(ThreadContext),org.logstash.execution.AbstractPipelineExt,openQueue/1[org.logstash.execution.ThreadContext],False,198,8,7,0,7,2,10,14,1,0,1,10,0,0,0,0,1,0,2,0,3,0,1,0,0,0,48,17,1,True
358,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\AbstractPipelineExt.java,org.logstash.execution.AbstractPipelineExt,DeadLetterQueueWriter createDeadLetterQueueWriterFromSettings(ThreadContext),"private DeadLetterQueueWriter createDeadLetterQueueWriterFromSettings(ThreadContext context) {
    final QueueStorageType storageType = QueueStorageType.parse(getSetting(context, ""dead_letter_queue.storage_policy"").asJavaString());
    String dlqPath = getSetting(context, ""path.dead_letter_queue"").asJavaString();
    long dlqMaxBytes = getSetting(context, ""dead_letter_queue.max_bytes"").convertToInteger().getLongValue();
    Duration dlqFlushInterval = Duration.ofMillis(getSetting(context, ""dead_letter_queue.flush_interval"").convertToInteger().getLongValue());
    if (hasSetting(context, ""dead_letter_queue.retain.age"") && !getSetting(context, ""dead_letter_queue.retain.age"").isNil()) {
        // convert to Duration
        final Duration age = parseToDuration(getSetting(context, ""dead_letter_queue.retain.age"").convertToString().toString());
        return DeadLetterQueueFactory.getWriter(pipelineId.asJavaString(), dlqPath, dlqMaxBytes, dlqFlushInterval, storageType, age);
    }
    return DeadLetterQueueFactory.getWriter(pipelineId.asJavaString(), dlqPath, dlqMaxBytes, dlqFlushInterval, storageType);
}", ,"// convert to Duration
",// convert to Duration,285,300,[0],0,[0],0,[0],0,0,0,0,createDeadLetterQueueWriterFromSettings(ThreadContext),org.logstash.execution.AbstractPipelineExt,createDeadLetterQueueWriterFromSettings/1[org.logstash.execution.ThreadContext],False,285,5,7,1,6,3,13,11,2,5,1,13,3,1,0,0,0,0,7,0,5,0,1,0,0,0,26,2,0,False
359,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\AbstractPipelineExt.java,org.logstash.execution.AbstractPipelineExt,Duration parseToDuration(String),"/**
 * Convert time strings like 3d or 4h or 5m to a duration
 */
@VisibleForTesting
static Duration parseToDuration(String timeStr) {
    final Matcher matcher = Pattern.compile(""(?<value>\\d+)\\s*(?<time>[dhms])"").matcher(timeStr);
    if (!matcher.matches()) {
        throw new IllegalArgumentException(""Expected a time specification in the form <number>[d,h,m,s], e.g. 3m, but found ["" + timeStr + ""]"");
    }
    final int value = Integer.parseInt(matcher.group(""value""));
    final String timeSpecifier = matcher.group(""time"");
    final TemporalUnit unit;
    switch(timeSpecifier) {
        case ""d"":
            unit = ChronoUnit.DAYS;
            break;
        case ""h"":
            unit = ChronoUnit.HOURS;
            break;
        case ""m"":
            unit = ChronoUnit.MINUTES;
            break;
        case ""s"":
            unit = ChronoUnit.SECONDS;
            break;
        default:
            throw new IllegalStateException(""Expected a time unit specification from d,h,m,s but found: ["" + timeSpecifier + ""]"");
    }
    return Duration.of(value, unit);
}","/**
 * Convert time strings like 3d or 4h or 5m to a duration
 */
", ,/** * Convert time strings like 3d or 4h or 5m to a duration */,305,322,[0],0,[0],0,[0],0,0,0,0,parseToDuration(String),org.logstash.execution.AbstractPipelineExt,parseToDuration/1[java.lang.String],False,306,1,5,5,0,6,6,26,1,4,1,6,0,0,0,0,0,0,11,0,7,2,1,0,0,0,33,8,0,True
360,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\AbstractPipelineExt.java,org.logstash.execution.AbstractPipelineExt,IRubyObject initializeFlowMetrics(ThreadContext),"// as much as this is sub-par, refactoring makes it harder to read.
@SuppressWarnings(""DuplicatedCode"")
@JRubyMethod(name = ""initialize_flow_metrics"")
public final IRubyObject initializeFlowMetrics(final ThreadContext context) {
    if (metric.collector(context).isNil()) {
        return context.nil;
    }
    final UptimeMetric uptimeMetric = initOrGetUptimeMetric(context, buildNamespace(), UPTIME_IN_MILLIS_KEY);
    final Metric<Number> uptimeInPreciseMillis = uptimeMetric.withUnitsPrecise(MILLISECONDS);
    final Metric<Number> uptimeInPreciseSeconds = uptimeMetric.withUnitsPrecise(SECONDS);
    final RubySymbol[] flowNamespace = buildNamespace(FLOW_KEY);
    final RubySymbol[] eventsNamespace = buildNamespace(EVENTS_KEY);
    final LongCounter eventsInCounter = initOrGetCounterMetric(context, eventsNamespace, IN_KEY);
    final FlowMetric inputThroughput = createFlowMetric(INPUT_THROUGHPUT_KEY, eventsInCounter, uptimeInPreciseSeconds);
    this.flowMetrics.add(inputThroughput);
    storeMetric(context, flowNamespace, inputThroughput);
    final LongCounter eventsFilteredCounter = initOrGetCounterMetric(context, eventsNamespace, FILTERED_KEY);
    final FlowMetric filterThroughput = createFlowMetric(FILTER_THROUGHPUT_KEY, eventsFilteredCounter, uptimeInPreciseSeconds);
    this.flowMetrics.add(filterThroughput);
    storeMetric(context, flowNamespace, filterThroughput);
    final LongCounter eventsOutCounter = initOrGetCounterMetric(context, eventsNamespace, OUT_KEY);
    final FlowMetric outputThroughput = createFlowMetric(OUTPUT_THROUGHPUT_KEY, eventsOutCounter, uptimeInPreciseSeconds);
    this.flowMetrics.add(outputThroughput);
    storeMetric(context, flowNamespace, outputThroughput);
    final LongCounter queuePushWaitInMillis = initOrGetCounterMetric(context, eventsNamespace, PUSH_DURATION_KEY);
    final FlowMetric backpressureFlow = createFlowMetric(QUEUE_BACKPRESSURE_KEY, queuePushWaitInMillis, uptimeInPreciseMillis);
    this.flowMetrics.add(backpressureFlow);
    storeMetric(context, flowNamespace, backpressureFlow);
    final LongCounter durationInMillis = initOrGetCounterMetric(context, eventsNamespace, DURATION_IN_MILLIS_KEY);
    final FlowMetric concurrencyFlow = createFlowMetric(WORKER_CONCURRENCY_KEY, durationInMillis, uptimeInPreciseMillis);
    this.flowMetrics.add(concurrencyFlow);
    storeMetric(context, flowNamespace, concurrencyFlow);
    return context.nil;
}", ,"// as much as this is sub-par, refactoring makes it harder to read.
","// as much as this is sub-par, refactoring makes it harder to read.",407,445,[0],0,[0],0,[0],0,0,0,0,initializeFlowMetrics(ThreadContext),org.logstash.execution.AbstractPipelineExt,initializeFlowMetrics/1[org.logstash.execution.ThreadContext],False,409,10,7,0,7,2,9,31,2,15,1,9,4,3,0,0,0,0,2,0,15,0,1,0,0,0,55,17,0,False
361,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\EventDispatcherExt.java,org.logstash.execution.EventDispatcherExt,"IRubyObject addListener(ThreadContext, IRubyObject)","/**
 * This operation is slow because we use a CopyOnWriteArrayList
 * But the majority of the addition will be done at bootstrap time
 * So add_listener shouldn't be called often at runtime.
 * On the other hand the notification could be called really often.
 * @param context ThreadContext
 * @param listener Listener
 * @return Nil
 */
@JRubyMethod(name = ""add_listener"")
public IRubyObject addListener(final ThreadContext context, final IRubyObject listener) {
    return listeners.add(listener) ? context.tru : context.fals;
}","/**
 * This operation is slow because we use a CopyOnWriteArrayList
 * But the majority of the addition will be done at bootstrap time
 * So add_listener shouldn't be called often at runtime.
 * On the other hand the notification could be called really often.
 * @param context ThreadContext
 * @param listener Listener
 * @return Nil
 */
", ,/** * This operation is slow because we use a CopyOnWriteArrayList * But the majority of the addition will be done at bootstrap time * So add_listener shouldn't be called often at runtime. * On the other hand the notification could be called really often. * @param context ThreadContext * @param listener Listener * @return Nil */,69,72,[1],1,[0],0,[1],1,0,1,0,"addListener(ThreadContext, IRubyObject)",org.logstash.execution.EventDispatcherExt,"addListener/2[org.logstash.execution.ThreadContext,org.logstash.execution.IRubyObject]",False,70,3,0,0,0,2,1,3,1,0,2,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,43,1,0,True
362,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\EventDispatcherExt.java,org.logstash.execution.EventDispatcherExt,"IRubyObject removeListener(ThreadContext, IRubyObject)","/**
 * This operation is slow because we use a `CopyOnWriteArrayList` as the backend, instead of a
 * ConcurrentHashMap, but since we are mostly adding stuff and iterating the `CopyOnWriteArrayList`
 * should provide a better performance.
 * See note on add_listener, this method shouldn't be called really often.
 * @param context ThreadContext
 * @param listener Listener
 * @return True iff listener was actually removed
 */
@JRubyMethod(name = ""remove_listener"")
public IRubyObject removeListener(final ThreadContext context, final IRubyObject listener) {
    return listeners.remove(listener) ? context.tru : context.fals;
}","/**
 * This operation is slow because we use a `CopyOnWriteArrayList` as the backend, instead of a
 * ConcurrentHashMap, but since we are mostly adding stuff and iterating the `CopyOnWriteArrayList`
 * should provide a better performance.
 * See note on add_listener, this method shouldn't be called really often.
 * @param context ThreadContext
 * @param listener Listener
 * @return True iff listener was actually removed
 */
", ,"/** * This operation is slow because we use a `CopyOnWriteArrayList` as the backend, instead of a * ConcurrentHashMap, but since we are mostly adding stuff and iterating the `CopyOnWriteArrayList` * should provide a better performance. * See note on add_listener, this method shouldn't be called really often. * @param context ThreadContext * @param listener Listener * @return True iff listener was actually removed */",83,86,[1],1,[0],0,[1],1,0,1,0,"removeListener(ThreadContext, IRubyObject)",org.logstash.execution.EventDispatcherExt,"removeListener/2[org.logstash.execution.ThreadContext,org.logstash.execution.IRubyObject]",False,84,3,0,0,0,2,1,3,1,0,2,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,45,1,0,True
363,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\MemoryReadBatch.java,org.logstash.execution.MemoryReadBatch,Collection<RubyEvent> events(),"@Override
public Collection<RubyEvent> events() {
    // This does not filter cancelled events because it is
    // only used in the WorkerLoop where there are no cancelled
    // events yet.
    return events;
}", ,"// This does not filter cancelled events because it is
[[SEP]]// only used in the WorkerLoop where there are no cancelled
[[SEP]]// events yet.
",// This does not filter cancelled events because it is// only used in the WorkerLoop where there are no cancelled// events yet.,63,69,[0],0,"[0, 0, 0]",0,[0],0,0,0,0,events(),org.logstash.execution.MemoryReadBatch,events/0,False,64,1,0,0,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,False
364,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\MemoryReadBatch.java,org.logstash.execution.MemoryReadBatch,void close(),"@Override
public void close() {
    // no-op
}", ,"// no-op
",// no-op,76,79,[0],0,[0],0,[0],0,0,0,0,close(),org.logstash.execution.MemoryReadBatch,close/0,False,77,0,0,0,0,1,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,False
365,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\PipelineReporterExt.java,org.logstash.execution.PipelineReporterExt,PipelineReporterExt.SnapshotExt snapshot(ThreadContext),"/**
 * The main way of accessing data from the reporter,,
 * this provides a (more or less) consistent snapshot of what's going on in the
 * pipeline with some extra decoration
 * @param context Thread Context
 * @return Snapshot
 */
@JRubyMethod
public PipelineReporterExt.SnapshotExt snapshot(final ThreadContext context) {
    return new PipelineReporterExt.SnapshotExt(context.runtime, RubyUtil.PIPELINE_REPORTER_SNAPSHOT_CLASS).initialize(toHash(context));
}","/**
 * The main way of accessing data from the reporter,,
 * this provides a (more or less) consistent snapshot of what's going on in the
 * pipeline with some extra decoration
 * @param context Thread Context
 * @return Snapshot
 */
", ,"/** * The main way of accessing data from the reporter,, * this provides a (more or less) consistent snapshot of what's going on in the * pipeline with some extra decoration * @param context Thread Context * @return Snapshot */",117,122,[0],0,[0],0,[0],0,0,0,0,snapshot(ThreadContext),org.logstash.execution.PipelineReporterExt,snapshot/1[org.logstash.execution.ThreadContext],False,118,4,3,0,3,1,2,3,1,0,1,2,1,2,0,0,0,0,0,0,0,0,0,0,0,0,29,1,0,True
366,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\QueueReadClientBase.java,org.logstash.execution.QueueReadClientBase,void closeBatch(QueueBatch),"@Override
public void closeBatch(QueueBatch batch) throws IOException {
    batch.close();
    inflightBatches.remove(Thread.currentThread().getId());
    Long startTime = inflightClocks.remove(Thread.currentThread().getId());
    if (startTime != null && batch.filteredSize() > 0) {
        // stop timer and record metrics iff the batch is non-empty.
        long elapsedTimeMillis = (System.nanoTime() - startTime) / 1_000_000;
        eventMetricTime.increment(elapsedTimeMillis);
        pipelineMetricTime.increment(elapsedTimeMillis);
    }
}", ,"// stop timer and record metrics iff the batch is non-empty.
",// stop timer and record metrics iff the batch is non-empty.,130,141,[0],0,[0],0,[0],0,0,0,0,closeBatch(QueueBatch),org.logstash.execution.QueueReadClientBase,closeBatch/1[org.logstash.execution.QueueBatch],False,131,2,5,2,3,3,8,10,0,2,1,8,0,0,0,1,0,1,0,2,2,2,1,0,0,0,15,1,0,False
367,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\QueueReadClientBase.java,org.logstash.execution.QueueReadClientBase,void rubyCloseBatch(IRubyObject),"/**
 * Closes the specified batch. This JRuby extension method is currently used only in the
 * original pipeline and rspec tests.
 * @param batch specified batch
 * @throws IOException if an IO error occurs
 */
@JRubyMethod(name = ""close_batch"")
public void rubyCloseBatch(final IRubyObject batch) throws IOException {
    closeBatch(extractQueueBatch(batch));
}","/**
 * Closes the specified batch. This JRuby extension method is currently used only in the
 * original pipeline and rspec tests.
 * @param batch specified batch
 * @throws IOException if an IO error occurs
 */
", ,/** * Closes the specified batch. This JRuby extension method is currently used only in the * original pipeline and rspec tests. * @param batch specified batch * @throws IOException if an IO error occurs */,149,152,[0],0,[0],0,[0],0,0,0,0,rubyCloseBatch(IRubyObject),org.logstash.execution.QueueReadClientBase,rubyCloseBatch/1[org.logstash.execution.IRubyObject],False,150,3,2,0,2,1,2,3,0,0,1,2,2,1,0,0,0,0,1,0,0,0,0,0,0,0,32,1,0,True
368,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\QueueReadClientBase.java,org.logstash.execution.QueueReadClientBase,void rubyStartMetrics(IRubyObject),"/**
 * Initializes metric on the specified batch. This JRuby extension method is currently used
 * only in the original pipeline and rspec tests.
 * @param batch specified batch
 */
@JRubyMethod(name = ""start_metrics"")
public void rubyStartMetrics(final IRubyObject batch) {
    startMetrics(extractQueueBatch(batch));
}","/**
 * Initializes metric on the specified batch. This JRuby extension method is currently used
 * only in the original pipeline and rspec tests.
 * @param batch specified batch
 */
", ,/** * Initializes metric on the specified batch. This JRuby extension method is currently used * only in the original pipeline and rspec tests. * @param batch specified batch */,159,162,[0],0,[0],0,[0],0,0,0,0,rubyStartMetrics(IRubyObject),org.logstash.execution.QueueReadClientBase,rubyStartMetrics/1[org.logstash.execution.IRubyObject],False,160,3,2,0,2,1,2,3,0,0,1,2,2,1,0,0,0,0,1,0,0,0,0,0,0,0,30,1,0,True
369,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\QueueReadClientBase.java,org.logstash.execution.QueueReadClientBase,QueueBatch extractQueueBatch(IRubyObject),"/**
 * Extracts QueueBatch from one of two possible IRubyObject classes. Only the Ruby pipeline
 * uses JavaProxy instances, so once that is fully deprecated, this method can be simplified
 * to eliminate the type check.
 * @param batch specified IRubyObject batch
 * @return Extracted queue batch
 */
private static QueueBatch extractQueueBatch(final IRubyObject batch) {
    return JavaUtil.unwrapIfJavaObject(batch);
}","/**
 * Extracts QueueBatch from one of two possible IRubyObject classes. Only the Ruby pipeline
 * uses JavaProxy instances, so once that is fully deprecated, this method can be simplified
 * to eliminate the type check.
 * @param batch specified IRubyObject batch
 * @return Extracted queue batch
 */
", ,"/** * Extracts QueueBatch from one of two possible IRubyObject classes. Only the Ruby pipeline * uses JavaProxy instances, so once that is fully deprecated, this method can be simplified * to eliminate the type check. * @param batch specified IRubyObject batch * @return Extracted queue batch */",171,173,[0],0,[0],0,[0],0,0,0,0,extractQueueBatch(IRubyObject),org.logstash.execution.QueueReadClientBase,extractQueueBatch/1[org.logstash.execution.IRubyObject],False,171,2,2,2,0,1,1,3,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,34,10,0,True
370,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\QueueReadClientBase.java,org.logstash.execution.QueueReadClientBase,void rubyAddFilteredMetrics(IRubyObject),"/**
 * Increments the filter metrics. This JRuby extension method is currently used
 * only in the original pipeline and rspec tests.
 * @param size numeric value by which to increment metric
 */
@JRubyMethod(name = ""add_filtered_metrics"")
public void rubyAddFilteredMetrics(final IRubyObject size) {
    addFilteredMetrics(((RubyNumeric) size).getIntValue());
}","/**
 * Increments the filter metrics. This JRuby extension method is currently used
 * only in the original pipeline and rspec tests.
 * @param size numeric value by which to increment metric
 */
", ,/** * Increments the filter metrics. This JRuby extension method is currently used * only in the original pipeline and rspec tests. * @param size numeric value by which to increment metric */,180,183,[0],0,[0],0,[0],0,0,0,0,rubyAddFilteredMetrics(IRubyObject),org.logstash.execution.QueueReadClientBase,rubyAddFilteredMetrics/1[org.logstash.execution.IRubyObject],False,181,4,1,0,1,1,2,3,0,0,1,2,1,1,0,0,0,1,1,0,0,0,0,0,0,0,34,1,0,True
371,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\QueueReadClientBase.java,org.logstash.execution.QueueReadClientBase,void rubyAddOutputMetrics(IRubyObject),"/**
 * Increments the output metrics. This JRuby extension method is currently used
 * only in the original pipeline and rspec tests.
 * @param size numeric value by which to increment metric
 */
@JRubyMethod(name = ""add_output_metrics"")
public void rubyAddOutputMetrics(final IRubyObject size) {
    addOutputMetrics(((RubyNumeric) size).getIntValue());
}","/**
 * Increments the output metrics. This JRuby extension method is currently used
 * only in the original pipeline and rspec tests.
 * @param size numeric value by which to increment metric
 */
", ,/** * Increments the output metrics. This JRuby extension method is currently used * only in the original pipeline and rspec tests. * @param size numeric value by which to increment metric */,190,193,[0],0,[0],0,[0],0,0,0,0,rubyAddOutputMetrics(IRubyObject),org.logstash.execution.QueueReadClientBase,rubyAddOutputMetrics/1[org.logstash.execution.IRubyObject],False,191,4,1,0,1,1,2,3,0,0,1,2,1,1,0,0,0,1,1,0,0,0,0,0,0,0,34,1,0,True
372,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\WorkerLoop.java,org.logstash.execution.WorkerLoop,void run(),"@Override
public void run() {
    try {
        boolean isShutdown = false;
        do {
            isShutdown = isShutdown || shutdownRequested.get();
            final QueueBatch batch = readClient.readBatch();
            final boolean isFlush = flushRequested.compareAndSet(true, false);
            if (batch.filteredSize() > 0 || isFlush) {
                consumedCounter.add(batch.filteredSize());
                readClient.startMetrics(batch);
                final int outputCount = execution.compute(batch, isFlush, false);
                int filteredCount = batch.filteredSize();
                filteredCounter.add(filteredCount);
                readClient.addOutputMetrics(outputCount);
                readClient.addFilteredMetrics(filteredCount);
                readClient.closeBatch(batch);
                if (isFlush) {
                    flushing.set(false);
                }
            }
        } while (!isShutdown || isDraining());
        // we are shutting down, queue is drained if it was required, now  perform a final flush.
        // for this we need to create a new empty batch to contain the final flushed events
        final QueueBatch batch = readClient.newBatch();
        readClient.startMetrics(batch);
        execution.compute(batch, true, true);
        readClient.closeBatch(batch);
    } catch (final Exception ex) {
        throw new IllegalStateException(ex);
    }
}", ,"// we are shutting down, queue is drained if it was required, now  perform a final flush.
[[SEP]]// for this we need to create a new empty batch to contain the final flushed events
","// we are shutting down, queue is drained if it was required, now  perform a final flush.// for this we need to create a new empty batch to contain the final flushed events",76,107,[0],0,"[0, 0]",0,[0],0,0,0,0,run(),org.logstash.execution.WorkerLoop,run/0,False,77,4,9,0,9,7,13,31,0,6,0,13,1,1,1,0,1,0,0,1,7,0,4,0,0,0,16,1,0,False
373,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\execution\queue\QueueWriter.java,org.logstash.execution.queue.QueueWriter,"void push(Map<String, Object>)","/**
 * Pushes a single event to the Queue, blocking indefinitely if the Queue is not ready for a
 * write. Implementations of this interface must produce events from a deep copy of the supplied
 * map because upstream clients of this interface may reuse map instances between calls to push.
 *
 * @param event Logstash event data
 */
void push(Map<String, Object> event);","/**
 * Pushes a single event to the Queue, blocking indefinitely if the Queue is not ready for a
 * write. Implementations of this interface must produce events from a deep copy of the supplied
 * map because upstream clients of this interface may reuse map instances between calls to push.
 *
 * @param event Logstash event data
 */
", ,"/** * Pushes a single event to the Queue, blocking indefinitely if the Queue is not ready for a * write. Implementations of this interface must produce events from a deep copy of the supplied * map because upstream clients of this interface may reuse map instances between calls to push. * * @param event Logstash event data */",37,37,[0],0,[0],0,[0],0,0,0,0,"push(Map<String, Object>)",org.logstash.execution.queue.QueueWriter,"push/1[java.util.Map<java.lang.String,java.lang.Object>]",False,30,0,2,2,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,33,0,0,True
374,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JRubyWrappedWriteClientExt.java,org.logstash.ext.JRubyWrappedWriteClientExt,"JRubyWrappedWriteClientExt initialize(JRubyAbstractQueueWriteClientExt, String, AbstractMetricExt, IRubyObject)","public JRubyWrappedWriteClientExt initialize(final JRubyAbstractQueueWriteClientExt queueWriteClientExt, final String pipelineId, final AbstractMetricExt metric, final IRubyObject pluginId) {
    this.writeClient = queueWriteClientExt;
    final RubySymbol pipelineIdSym = getRuntime().newSymbol(pipelineId);
    final RubySymbol pluginIdSym = pluginId.asString().intern();
    // Synchronize on the metric since setting up new fields on it is not threadsafe
    synchronized (metric) {
        final AbstractNamespacedMetricExt eventsMetrics = getMetric(metric, STATS_KEY, EVENTS_KEY);
        eventsMetricsCounter = LongCounter.fromRubyBase(eventsMetrics, MetricKeys.IN_KEY);
        eventsMetricsTime = LongCounter.fromRubyBase(eventsMetrics, MetricKeys.PUSH_DURATION_KEY);
        final AbstractNamespacedMetricExt pipelineEventMetrics = getMetric(metric, STATS_KEY, PIPELINES_KEY, pipelineIdSym, EVENTS_KEY);
        pipelineMetricsCounter = LongCounter.fromRubyBase(pipelineEventMetrics, MetricKeys.IN_KEY);
        pipelineMetricsTime = LongCounter.fromRubyBase(pipelineEventMetrics, MetricKeys.PUSH_DURATION_KEY);
        final AbstractNamespacedMetricExt pluginMetrics = getMetric(metric, STATS_KEY, PIPELINES_KEY, pipelineIdSym, PLUGINS_KEY, INPUTS_KEY, pluginIdSym, EVENTS_KEY);
        pluginMetricsCounter = LongCounter.fromRubyBase(pluginMetrics, MetricKeys.OUT_KEY);
        pluginMetricsTime = LongCounter.fromRubyBase(pluginMetrics, MetricKeys.PUSH_DURATION_KEY);
    }
    return this;
}", ,"// Synchronize on the metric since setting up new fields on it is not threadsafe
",// Synchronize on the metric since setting up new fields on it is not threadsafe,75,106,[0],0,[0],0,[0],0,0,0,0,"initialize(JRubyAbstractQueueWriteClientExt, String, AbstractMetricExt, IRubyObject)",org.logstash.ext.JRubyWrappedWriteClientExt,"initialize/4[org.logstash.ext.JRubyAbstractQueueWriteClientExt,java.lang.String,org.logstash.instrument.metrics.AbstractMetricExt,org.logstash.ext.IRubyObject]",False,78,7,4,2,2,1,6,17,1,5,4,6,1,1,0,0,0,0,0,0,12,0,1,0,0,0,27,1,0,False
375,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JRubyWrappedWriteClientExt.java,org.logstash.ext.JRubyWrappedWriteClientExt,IRubyObject newBatch(ThreadContext),"/**
 * @param context Ruby {@link ThreadContext}
 * @return Empty {@link RubyArray}
 * @deprecated This method exists for backwards compatibility only, it does not do anything but
 * return an empty {@link RubyArray}.
 */
@Deprecated
@JRubyMethod(name = ""get_new_batch"")
public IRubyObject newBatch(final ThreadContext context) {
    return context.runtime.newArray();
}","/**
 * @param context Ruby {@link ThreadContext}
 * @return Empty {@link RubyArray}
 * @deprecated This method exists for backwards compatibility only, it does not do anything but
 * return an empty {@link RubyArray}.
 */
", ,"/** * @param context Ruby {@link ThreadContext} * @return Empty {@link RubyArray} * @deprecated This method exists for backwards compatibility only, it does not do anything but * return an empty {@link RubyArray}. */",137,141,[1],1,[0],0,[1],1,0,0,0,newBatch(ThreadContext),org.logstash.ext.JRubyWrappedWriteClientExt,newBatch/1[org.logstash.ext.ThreadContext],False,139,3,0,0,0,1,1,3,1,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,23,1,0,True
376,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyEventExtLibrary.java,org.logstash.ext.JrubyEventExtLibrary.RubyEvent,"IRubyObject ruby_initialize(ThreadContext, IRubyObject[])","// def initialize(data = {})
@JRubyMethod(name = ""initialize"", optional = 1)
public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args) {
    final IRubyObject data = args.length > 0 ? args[0] : null;
    if (data instanceof RubyHash) {
        this.event = new Event(ConvertedMap.newFromRubyHash(context, (RubyHash) data));
    } else if (data != null && data.getJavaClass().equals(Event.class)) {
        this.event = data.toJava(Event.class);
    } else {
        initializeFallback(context, data);
    }
    return context.nil;
}","// def initialize(data = {})
", ,// def initialize(data = {}),92,105,[0],0,[0],0,[0],0,0,0,0,"ruby_initialize(ThreadContext, IRubyObject[])",org.logstash.ext.JrubyEventExtLibrary$RubyEvent,"ruby_initialize/2[org.logstash.ext.ThreadContext,org.logstash.ext.IRubyObject[]]",False,93,7,3,0,3,5,5,13,1,1,2,5,1,1,0,1,0,0,1,3,3,0,1,0,0,0,14,1,0,False
377,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyEventExtLibrary.java,org.logstash.ext.JrubyEventExtLibrary.RubyEvent,"IRubyObject ruby_from_json(ThreadContext, IRubyObject, RubyString, Block)","// @param value [String] the json string. A json object/map will newFromRubyArray to an array containing a single Event.
// and a json array will newFromRubyArray each element into individual Event
// @return Array<Event> array of events
@JRubyMethod(name = ""from_json"", required = 1, meta = true)
public static IRubyObject ruby_from_json(ThreadContext context, IRubyObject recv, RubyString value, final Block block) {
    if (!block.isGiven())
        return fromJson(context, value, BasicEventFactory.INSTANCE);
    return fromJson(context, value, (data) -> {
        // LogStash::Event works fine with a Map arg (instead of a native Hash)
        IRubyObject event = block.yield(context, RubyUtil.toRubyObject(data));
        // we unwrap just to re-wrap later
        return ((RubyEvent) event).getEvent();
    });
}","// @return Array<Event> array of events
","// LogStash::Event works fine with a Map arg (instead of a native Hash)
[[SEP]]// we unwrap just to re-wrap later
",// @param value [String] the json string. A json object/map will newFromRubyArray to an array containing a single Event.// and a json array will newFromRubyArray each element into individual Event// @return Array<Event> array of events[[SEP]]// LogStash::Event works fine with a Map arg (instead of a native Hash)[[SEP]]// we unwrap just to re-wrap later,248,256,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,0,0,"ruby_from_json(ThreadContext, IRubyObject, RubyString, Block)",org.logstash.ext.JrubyEventExtLibrary$RubyEvent,"ruby_from_json/4[org.logstash.ext.ThreadContext,org.logstash.ext.IRubyObject,org.logstash.ext.RubyString,org.logstash.ext.Block]",False,249,6,1,0,1,2,5,8,3,2,4,5,1,4,0,0,0,1,1,1,1,0,1,0,0,1,16,9,0,False
378,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyEventExtLibrary.java,org.logstash.ext.JrubyEventExtLibrary.RubyEvent,"IRubyObject fromJson(ThreadContext, RubyString, EventFactory)","private static IRubyObject fromJson(ThreadContext context, RubyString json, EventFactory eventFactory) {
    Event[] events;
    try {
        events = Event.fromJson(json.asJavaString(), eventFactory);
    } catch (Exception e) {
        throw toRubyError(context, RubyUtil.PARSER_ERROR, e);
    }
    if (events.length == 1) {
        // micro optimization for the 1 event more common use-case.
        return context.runtime.newArray(RubyEvent.newRubyEvent(context.runtime, events[0]));
    }
    IRubyObject[] rubyEvents = new IRubyObject[events.length];
    for (int i = 0; i < events.length; i++) {
        rubyEvents[i] = RubyEvent.newRubyEvent(context.runtime, events[i]);
    }
    return context.runtime.newArrayNoCopy(rubyEvents);
}", ,"// micro optimization for the 1 event more common use-case.
",// micro optimization for the 1 event more common use-case.,258,276,[0],0,[0],0,[0],0,0,0,0,"fromJson(ThreadContext, RubyString, EventFactory)",org.logstash.ext.JrubyEventExtLibrary$RubyEvent,"fromJson/3[org.logstash.ext.ThreadContext,org.logstash.ext.RubyString,co.elastic.logstash.api.EventFactory]",False,258,6,4,1,3,4,6,17,2,3,3,6,2,3,1,1,1,0,0,3,4,0,1,0,0,0,17,10,0,False
379,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyEventExtLibrary.java,org.logstash.ext.JrubyEventExtLibrary.RubyEvent,"IRubyObject ruby_validate_value(ThreadContext, IRubyObject, IRubyObject)","@JRubyMethod(name = ""validate_value"", required = 1, meta = true)
public static IRubyObject ruby_validate_value(ThreadContext context, IRubyObject recv, IRubyObject value) {
    // TODO: add UTF-8 validation
    return value;
}", ,"// TODO: add UTF-8 validation
",// TODO: add UTF-8 validation,278,283,[0],0,[1],1,[1],1,1,1,1,"ruby_validate_value(ThreadContext, IRubyObject, IRubyObject)",org.logstash.ext.JrubyEventExtLibrary$RubyEvent,"ruby_validate_value/3[org.logstash.ext.ThreadContext,org.logstash.ext.IRubyObject,org.logstash.ext.IRubyObject]",False,280,3,0,0,0,1,0,3,1,0,3,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,10,9,0,False
380,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyEventExtLibrary.java,org.logstash.ext.JrubyEventExtLibrary.RubyEvent,"IRubyObject ruby_tag(ThreadContext, RubyString)","@JRubyMethod(name = ""tag"", required = 1)
public IRubyObject ruby_tag(ThreadContext context, RubyString value) {
    // TODO(guy) should these tags be BiValues?
    this.event.tag(value.asJavaString());
    return context.nil;
}", ,"// TODO(guy) should these tags be BiValues?
",// TODO(guy) should these tags be BiValues?,285,291,[0],0,[1],1,[1],1,1,1,1,"ruby_tag(ThreadContext, RubyString)",org.logstash.ext.JrubyEventExtLibrary$RubyEvent,"ruby_tag/2[org.logstash.ext.ThreadContext,org.logstash.ext.RubyString]",False,287,5,1,0,1,1,2,4,1,0,2,2,0,0,0,0,0,0,1,1,0,0,0,0,0,0,9,1,0,False
381,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyEventExtLibrary.java,org.logstash.ext.JrubyEventExtLibrary.RubyEvent,IRubyObject ruby_timestamp(ThreadContext),"@JRubyMethod(name = ""timestamp"")
public IRubyObject ruby_timestamp(ThreadContext context) {
    // We can just cast to IRubyObject here, because we know that Event stores a
    // RubyTimestamp internally.
    return (IRubyObject) event.getUnconvertedField(FieldReference.TIMESTAMP_REFERENCE);
}", ,"// We can just cast to IRubyObject here, because we know that Event stores a
[[SEP]]// RubyTimestamp internally.
","// We can just cast to IRubyObject here, because we know that Event stores a// RubyTimestamp internally.",293,298,[0],0,"[0, 0]",0,[0],0,0,0,0,ruby_timestamp(ThreadContext),org.logstash.ext.JrubyEventExtLibrary$RubyEvent,ruby_timestamp/1[org.logstash.ext.ThreadContext],False,294,4,1,0,1,1,1,3,1,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,9,1,0,False
382,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyEventExtLibrary.java,org.logstash.ext.JrubyEventExtLibrary.RubyEvent,"void initializeFallback(ThreadContext, IRubyObject)","/**
 * Cold path for the Ruby constructor
 * {@link JrubyEventExtLibrary.RubyEvent#ruby_initialize(ThreadContext, IRubyObject[])} for
 * when its argument is not a {@link RubyHash}.
 * @param context Ruby {@link ThreadContext}
 * @param data Either {@code null}, {@link org.jruby.RubyNil} or an instance of
 * {@link MapJavaProxy}
 */
@SuppressWarnings(""unchecked"")
private void initializeFallback(final ThreadContext context, final IRubyObject data) {
    if (data == null || data.isNil()) {
        this.event = new Event();
    } else if (data instanceof MapJavaProxy) {
        this.event = new Event(ConvertedMap.newFromMap((Map<String, Object>) ((MapJavaProxy) data).getObject()));
    } else {
        throw context.runtime.newTypeError(""wrong argument type "" + data.getMetaClass() + "" (expected Hash)"");
    }
}","/**
 * Cold path for the Ruby constructor
 * {@link JrubyEventExtLibrary.RubyEvent#ruby_initialize(ThreadContext, IRubyObject[])} for
 * when its argument is not a {@link RubyHash}.
 * @param context Ruby {@link ThreadContext}
 * @param data Either {@code null}, {@link org.jruby.RubyNil} or an instance of
 * {@link MapJavaProxy}
 */
", ,"/** * Cold path for the Ruby constructor * {@link JrubyEventExtLibrary.RubyEvent#ruby_initialize(ThreadContext, IRubyObject[])} for * when its argument is not a {@link RubyHash}. * @param context Ruby {@link ThreadContext} * @param data Either {@code null}, {@link org.jruby.RubyNil} or an instance of * {@link MapJavaProxy} */",328,339,[0],0,[0],0,[0],0,0,0,0,"initializeFallback(ThreadContext, IRubyObject)",org.logstash.ext.JrubyEventExtLibrary$RubyEvent,"initializeFallback/2[org.logstash.ext.ThreadContext,org.logstash.ext.IRubyObject]",False,329,5,4,1,3,4,5,11,0,0,2,5,0,0,0,1,0,1,3,0,2,1,1,0,0,0,32,2,0,True
383,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyEventExtLibrary.java,org.logstash.ext.JrubyEventExtLibrary.RubyEvent,FieldReference extractFieldReference(RubyString),"/**
 * Shared logic to wrap {@link FieldReference.IllegalSyntaxException}s that are raised by
 * {@link FieldReference#from(RubyString)} when encountering illegal syntax in a ruby-exception
 * that can be easily handled within the ruby plugins
 *
 * @param reference a {@link RubyString} representing the path to a field
 * @return the corresponding {@link FieldReference} (see: {@link FieldReference#from(RubyString)})
 */
private static FieldReference extractFieldReference(final RubyString reference) {
    try {
        return FieldReference.from(reference);
    } catch (FieldReference.IllegalSyntaxException ise) {
        throw RubyUtil.RUBY.newRuntimeError(ise.getMessage());
    }
}","/**
 * Shared logic to wrap {@link FieldReference.IllegalSyntaxException}s that are raised by
 * {@link FieldReference#from(RubyString)} when encountering illegal syntax in a ruby-exception
 * that can be easily handled within the ruby plugins
 *
 * @param reference a {@link RubyString} representing the path to a field
 * @return the corresponding {@link FieldReference} (see: {@link FieldReference#from(RubyString)})
 */
", ,/** * Shared logic to wrap {@link FieldReference.IllegalSyntaxException}s that are raised by * {@link FieldReference#from(RubyString)} when encountering illegal syntax in a ruby-exception * that can be easily handled within the ruby plugins * * @param reference a {@link RubyString} representing the path to a field * @return the corresponding {@link FieldReference} (see: {@link FieldReference#from(RubyString)}) */,349,355,[0],0,[0],0,[0],0,0,0,0,extractFieldReference(RubyString),org.logstash.ext.JrubyEventExtLibrary$RubyEvent,extractFieldReference/1[org.logstash.ext.RubyString],False,349,2,5,4,1,2,3,8,1,0,1,3,0,0,0,0,1,0,0,0,0,0,1,0,0,0,35,10,0,True
384,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyEventExtLibrary.java,org.logstash.ext.JrubyEventExtLibrary.RubyEvent,Object safeValueifierConvert(Object),"/**
 * Shared logic to wrap {@link FieldReference.IllegalSyntaxException}s that are raised by
 * {@link Valuefier#convert(Object)} when encountering illegal syntax in a ruby-exception
 * that can be easily handled within the ruby plugins
 *
 * @param value a {@link Object} to be passed to {@link Valuefier#convert(Object)}
 * @return the resulting {@link Object} (see: {@link Valuefier#convert(Object)})
 */
private static Object safeValueifierConvert(final Object value) {
    try {
        return Valuefier.convert(value);
    } catch (FieldReference.IllegalSyntaxException ise) {
        throw RubyUtil.RUBY.newRuntimeError(ise.getMessage());
    }
}","/**
 * Shared logic to wrap {@link FieldReference.IllegalSyntaxException}s that are raised by
 * {@link Valuefier#convert(Object)} when encountering illegal syntax in a ruby-exception
 * that can be easily handled within the ruby plugins
 *
 * @param value a {@link Object} to be passed to {@link Valuefier#convert(Object)}
 * @return the resulting {@link Object} (see: {@link Valuefier#convert(Object)})
 */
", ,/** * Shared logic to wrap {@link FieldReference.IllegalSyntaxException}s that are raised by * {@link Valuefier#convert(Object)} when encountering illegal syntax in a ruby-exception * that can be easily handled within the ruby plugins * * @param value a {@link Object} to be passed to {@link Valuefier#convert(Object)} * @return the resulting {@link Object} (see: {@link Valuefier#convert(Object)}) */,365,371,[0],0,[0],0,[0],0,0,0,0,safeValueifierConvert(Object),org.logstash.ext.JrubyEventExtLibrary$RubyEvent,safeValueifierConvert/1[java.lang.Object],False,365,1,2,1,1,2,3,8,1,0,1,3,0,0,0,0,1,0,0,0,0,0,1,0,0,0,32,10,0,True
385,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyEventExtLibrary.java,org.logstash.ext.JrubyEventExtLibrary.RubyEvent,int nextHash(),"/**
 * Generates a fixed hashcode.
 * @return HashCode value
 */
private static int nextHash() {
    final long sequence = SEQUENCE_GENERATOR.incrementAndGet();
    return (int) (sequence ^ sequence >>> 32) + 31;
}","/**
 * Generates a fixed hashcode.
 * @return HashCode value
 */
", ,/** * Generates a fixed hashcode. * @return HashCode value */,382,385,[0],0,[0],0,[0],0,0,0,0,nextHash(),org.logstash.ext.JrubyEventExtLibrary$RubyEvent,nextHash/0,False,382,0,0,0,0,1,1,4,1,1,0,1,0,0,0,0,0,1,0,2,1,2,0,0,0,0,8,10,0,True
386,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyMemoryReadClientExt.java,org.logstash.ext.JrubyMemoryReadClientExt,void close(),"@Override
public void close() {
    // no-op
}", ,"// no-op
",// no-op,65,68,[0],0,[0],0,[0],0,0,0,0,close(),org.logstash.ext.JrubyMemoryReadClientExt,close/0,False,66,0,0,0,0,1,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,False
387,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyTimestampExtLibrary.java,org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp,"JrubyTimestampExtLibrary.RubyTimestamp initialize(ThreadContext, IRubyObject[])","// def initialize(time = Time.new)
@JRubyMethod(optional = 1)
public JrubyTimestampExtLibrary.RubyTimestamp initialize(final ThreadContext context, IRubyObject[] args) {
    args = Arity.scanArgs(context.runtime, args, 0, 1);
    IRubyObject time = args[0];
    if (time.isNil()) {
        this.timestamp = new Timestamp();
    } else if (time instanceof RubyTime) {
        this.timestamp = new Timestamp(((RubyTime) time).toInstant());
    } else if (time instanceof RubyString) {
        try {
            this.timestamp = new Timestamp(time.toString());
        } catch (IllegalArgumentException e) {
            throw RaiseException.from(getRuntime(), RubyUtil.TIMESTAMP_PARSER_ERROR, ""invalid timestamp string format "" + time);
        }
    } else {
        throw context.runtime.newTypeError(""wrong argument type "" + time.getMetaClass() + "" (expected Time)"");
    }
    return this;
}","// def initialize(time = Time.new)
", ,// def initialize(time = Time.new),76,100,[0],0,[0],0,[0],0,0,0,0,"initialize(ThreadContext, IRubyObject[])",org.logstash.ext.JrubyTimestampExtLibrary$RubyTimestamp,"initialize/2[org.logstash.ext.ThreadContext,org.logstash.ext.IRubyObject[]]",False,78,7,3,1,2,5,8,22,1,1,2,8,0,0,0,0,1,1,3,4,5,2,2,0,0,0,24,1,0,False
388,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyTimestampExtLibrary.java,org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp,"JrubyTimestampExtLibrary.RubyTimestamp ruby_at(ThreadContext, IRubyObject, IRubyObject[])","@JRubyMethod(name = ""at"", required = 1, optional = 1, meta = true)
public static JrubyTimestampExtLibrary.RubyTimestamp ruby_at(ThreadContext context, IRubyObject recv, IRubyObject[] args) {
    RubyTime t;
    if (args.length == 1) {
        // JRuby 9K has fixed the problem iwth BigDecimal precision see https://github.com/elastic/logstash/issues/4565
        t = (RubyTime) RubyTime.at(context, context.runtime.getTime(), args[0]);
    } else {
        t = (RubyTime) RubyTime.at(context, context.runtime.getTime(), args[0], args[1]);
    }
    return RubyTimestamp.newRubyTimestamp(context.runtime, new Timestamp(t.toInstant()));
}", ,"// JRuby 9K has fixed the problem iwth BigDecimal precision see https://github.com/elastic/logstash/issues/4565
",// JRuby 9K has fixed the problem iwth BigDecimal precision see https://github.com/elastic/logstash/issues/4565,210,221,[0],0,[0],0,[0],0,0,0,0,"ruby_at(ThreadContext, IRubyObject, IRubyObject[])",org.logstash.ext.JrubyTimestampExtLibrary$RubyTimestamp,"ruby_at/3[org.logstash.ext.ThreadContext,org.logstash.ext.IRubyObject,org.logstash.ext.IRubyObject[]]",False,212,6,3,1,2,2,4,10,1,1,3,4,1,1,0,1,0,0,1,6,2,0,1,0,0,0,11,9,0,False
389,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyWrappedSynchronousQueueExt.java,org.logstash.ext.JrubyWrappedSynchronousQueueExt,QueueReadClientBase getReadClient(),"@Override
protected QueueReadClientBase getReadClient() {
    // batch size and timeout are currently hard-coded to 125 and 50ms as values observed
    // to be reasonable tradeoffs between latency and throughput per PR #8707
    return JrubyMemoryReadClientExt.create(queue, 125, 50);
}", ,"// batch size and timeout are currently hard-coded to 125 and 50ms as values observed
[[SEP]]// to be reasonable tradeoffs between latency and throughput per PR #8707
",// batch size and timeout are currently hard-coded to 125 and 50ms as values observed// to be reasonable tradeoffs between latency and throughput per PR #8707,63,68,[0],0,"[0, 0]",0,[0],0,0,0,0,getReadClient(),org.logstash.ext.JrubyWrappedSynchronousQueueExt,getReadClient/0,False,64,2,1,0,1,1,1,3,1,0,0,1,0,0,0,0,0,0,0,2,0,0,0,0,0,0,5,4,0,False
390,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\ext\JrubyWrappedSynchronousQueueExt.java,org.logstash.ext.JrubyWrappedSynchronousQueueExt,IRubyObject doClose(ThreadContext),"@Override
public IRubyObject doClose(final ThreadContext context) {
    // no op
    return this;
}", ,"// no op
",// no op,70,74,[0],0,[0],0,[0],0,0,0,0,doClose(ThreadContext),org.logstash.ext.JrubyWrappedSynchronousQueueExt,doClose/1[org.logstash.ext.ThreadContext],False,71,3,0,0,0,1,0,3,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,1,0,False
391,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\FlowMetric.java,org.logstash.instrument.metrics.FlowMetric,void capture(),"public void capture() {
    final Capture newestHead = doCapture();
    final Capture previousHead = head.getAndSet(newestHead);
    instant.getAndAccumulate(previousHead, (current, given) -> {
        // keep our current value if the given one is less than ~100ms older than our newestHead
        // this is naive and when captures happen too frequently without relief can result in
        // our ""current"" window growing indefinitely, but we are shipping with a 5s cadence
        // and shouldn't hit this edge-case in practice.
        return (newestHead.calculateCapturePeriod(given).toMillis() > 100) ? given : current;
    });
}", ,"// keep our current value if the given one is less than ~100ms older than our newestHead
[[SEP]]// this is naive and when captures happen too frequently without relief can result in
[[SEP]]// our ""current"" window growing indefinitely, but we are shipping with a 5s cadence
[[SEP]]// and shouldn't hit this edge-case in practice.
","// keep our current value if the given one is less than ~100ms older than our newestHead// this is naive and when captures happen too frequently without relief can result in// our ""current"" window growing indefinitely, but we are shipping with a 5s cadence// and shouldn't hit this edge-case in practice.",70,80,[0],0,"[0, 0, 0, 0]",0,[0],0,0,0,0,capture(),org.logstash.instrument.metrics.FlowMetric,capture/0,False,70,2,3,1,2,2,5,8,1,4,0,5,1,1,0,0,0,1,0,1,2,0,1,0,0,1,8,1,0,False
392,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\FlowMetric.java,org.logstash.instrument.metrics.FlowMetric,"Map<String, Double> getValue()","/**
 * @return a map containing all available finite rates (see {@link Capture#calculateRate(Capture)})
 */
public Map<String, Double> getValue() {
    final Capture headCapture = head.get();
    if (Objects.isNull(headCapture)) {
        return Map.of();
    }
    final Map<String, Double> rates = new HashMap<>();
    headCapture.calculateRate(baseline).ifPresent((rate) -> rates.put(LIFETIME_KEY, rate));
    headCapture.calculateRate(instant::get).ifPresent((rate) -> rates.put(CURRENT_KEY, rate));
    return Map.copyOf(rates);
}","/**
 * @return a map containing all available finite rates (see {@link Capture#calculateRate(Capture)})
 */
", ,/** * @return a map containing all available finite rates (see {@link Capture#calculateRate(Capture)}) */,85,97,[0],0,[0],0,[0],0,0,0,0,getValue(),org.logstash.instrument.metrics.FlowMetric,getValue/0,False,85,1,3,1,2,2,8,10,2,4,0,8,0,0,0,0,0,0,0,0,2,0,1,0,0,2,14,1,0,True
393,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\FlowMetric.java,org.logstash.instrument.metrics.FlowMetric.Capture,OptionalDouble calculateRate(Capture),"/**
 * @param baseline a non-null {@link Capture} from which to compare.
 * @return an {@link OptionalDouble} that will be non-empty IFF we have sufficient information
 *         to calculate a finite rate of change of the numerator relative to the denominator.
 */
OptionalDouble calculateRate(final Capture baseline) {
    Objects.requireNonNull(baseline, ""baseline"");
    if (baseline == this) {
        return OptionalDouble.empty();
    }
    final double deltaNumerator = this.numerator.doubleValue() - baseline.numerator.doubleValue();
    final double deltaDenominator = this.denominator.doubleValue() - baseline.denominator.doubleValue();
    // divide-by-zero safeguard
    if (deltaDenominator == 0.0) {
        return OptionalDouble.empty();
    }
    // To prevent the appearance of false-precision, we round to 3 decimal places.
    return OptionalDouble.of(BigDecimal.valueOf(deltaNumerator).divide(BigDecimal.valueOf(deltaDenominator), 3, RoundingMode.HALF_UP).doubleValue());
}","/**
 * @param baseline a non-null {@link Capture} from which to compare.
 * @return an {@link OptionalDouble} that will be non-empty IFF we have sufficient information
 *         to calculate a finite rate of change of the numerator relative to the denominator.
 */
","// divide-by-zero safeguard
[[SEP]]// To prevent the appearance of false-precision, we round to 3 decimal places.
","/** * @param baseline a non-null {@link Capture} from which to compare. * @return an {@link OptionalDouble} that will be non-empty IFF we have sufficient information *         to calculate a finite rate of change of the numerator relative to the denominator. */[[SEP]]// divide-by-zero safeguard[[SEP]]// To prevent the appearance of false-precision, we round to 3 decimal places.",126,140,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,0,0,calculateRate(Capture),org.logstash.instrument.metrics.FlowMetric$Capture,calculateRate/1[org.logstash.instrument.metrics.FlowMetric.Capture],False,126,1,1,1,0,3,7,12,3,2,1,7,0,0,0,2,0,0,1,2,2,2,1,0,0,0,33,0,0,True
394,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\FlowMetric.java,org.logstash.instrument.metrics.FlowMetric.Capture,OptionalDouble calculateRate(Supplier<Capture>),"/**
 * @param possibleBaseline a {@link Supplier<Capture>} that may return null
 * @return an {@link OptionalDouble} that will be non-empty IFF we have sufficient information
 *         to calculate a finite rate of change of the numerator relative to the denominator.
 */
OptionalDouble calculateRate(final Supplier<Capture> possibleBaseline) {
    return Optional.ofNullable(possibleBaseline.get()).map(this::calculateRate).orElseGet(OptionalDouble::empty);
}","/**
 * @param possibleBaseline a {@link Supplier<Capture>} that may return null
 * @return an {@link OptionalDouble} that will be non-empty IFF we have sufficient information
 *         to calculate a finite rate of change of the numerator relative to the denominator.
 */
", ,/** * @param possibleBaseline a {@link Supplier<Capture>} that may return null * @return an {@link OptionalDouble} that will be non-empty IFF we have sufficient information *         to calculate a finite rate of change of the numerator relative to the denominator. */,147,151,[0],0,[0],0,[0],0,0,0,0,calculateRate(Supplier<Capture>),org.logstash.instrument.metrics.FlowMetric$Capture,calculateRate/1[java.util.function.Supplier<org.logstash.instrument.metrics.FlowMetric.Capture>],False,147,1,1,1,0,1,4,3,1,0,1,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,29,0,0,True
395,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\Metric.java,org.logstash.instrument.metrics.Metric,String getName(),"/**
 * This metric's name. May be used for display purposes.
 *
 * @return The name of this metric.
 */
String getName();","/**
 * This metric's name. May be used for display purposes.
 *
 * @return The name of this metric.
 */
", ,/** * This metric's name. May be used for display purposes. * * @return The name of this metric. */,36,36,[0],0,[0],0,[0],0,0,0,0,getName(),org.logstash.instrument.metrics.Metric,getName/0,False,31,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,0,0,True
396,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\Metric.java,org.logstash.instrument.metrics.Metric,T get(),"/**
 * This should be equal to #getValue, exists for passivity with legacy Ruby code. Java consumers should use #getValue().
 *
 * @return This metric value
 * @deprecated
 */
@Deprecated
default T get() {
    return getValue();
}","/**
 * This should be equal to #getValue, exists for passivity with legacy Ruby code. Java consumers should use #getValue().
 *
 * @return This metric value
 * @deprecated
 */
", ,"/** * This should be equal to #getValue, exists for passivity with legacy Ruby code. Java consumers should use #getValue(). * * @return This metric value * @deprecated */",44,47,[1],1,[0],0,[1],1,0,1,0,get(),org.logstash.instrument.metrics.Metric,get/0,False,45,2,2,1,1,1,1,3,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,18,65536,0,True
397,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\Metric.java,org.logstash.instrument.metrics.Metric,MetricType getType(),"/**
 * The enumerated Metric Type. This is a semantic type <i>(not Java type)</i> that can be useful to help identify the type of Metric. For example ""counter/long"".
 *
 * @return The {@link MetricType} that this metric represents.
 */
MetricType getType();","/**
 * The enumerated Metric Type. This is a semantic type <i>(not Java type)</i> that can be useful to help identify the type of Metric. For example ""counter/long"".
 *
 * @return The {@link MetricType} that this metric represents.
 */
", ,"/** * The enumerated Metric Type. This is a semantic type <i>(not Java type)</i> that can be useful to help identify the type of Metric. For example ""counter/long"". * * @return The {@link MetricType} that this metric represents. */",54,54,[0],0,[0],0,[0],0,0,0,0,getType(),org.logstash.instrument.metrics.Metric,getType/0,False,49,1,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,25,0,0,True
398,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\Metric.java,org.logstash.instrument.metrics.Metric,T getValue(),"/**
 * Retrieves the value associated with this metric
 *
 * @return This metric value
 */
T getValue();","/**
 * Retrieves the value associated with this metric
 *
 * @return This metric value
 */
", ,/** * Retrieves the value associated with this metric * * @return This metric value */,61,61,[0],0,[0],0,[0],0,0,0,0,getValue(),org.logstash.instrument.metrics.Metric,getValue/0,False,56,1,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,0,0,True
399,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\Metric.java,org.logstash.instrument.metrics.Metric,String inspect(),"/**
 * This may be equal to the #toString method, exists for passivity with legacy Ruby code. Java consumers should use #toString
 *
 * @return A description of this Metric that can be used for logging.
 * @deprecated
 */
@Deprecated
default String inspect() {
    return toString();
}","/**
 * This may be equal to the #toString method, exists for passivity with legacy Ruby code. Java consumers should use #toString
 *
 * @return A description of this Metric that can be used for logging.
 * @deprecated
 */
", ,"/** * This may be equal to the #toString method, exists for passivity with legacy Ruby code. Java consumers should use #toString * * @return A description of this Metric that can be used for logging. * @deprecated */",69,72,[1],1,[0],0,[1],1,0,0,1,inspect(),org.logstash.instrument.metrics.Metric,inspect/0,False,70,0,0,0,0,1,1,3,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,24,65536,0,True
400,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\Metric.java,org.logstash.instrument.metrics.Metric,String type(),"/**
 * This should be equal to {@link MetricType#asString()}, exists for passivity with legacy Ruby code. Java consumers should use #getType().
 *
 * @return The {@link String} version of the {@link MetricType}
 * @deprecated
 */
@Deprecated
default String type() {
    return getType().asString();
}","/**
 * This should be equal to {@link MetricType#asString()}, exists for passivity with legacy Ruby code. Java consumers should use #getType().
 *
 * @return The {@link String} version of the {@link MetricType}
 * @deprecated
 */
", ,"/** * This should be equal to {@link MetricType#asString()}, exists for passivity with legacy Ruby code. Java consumers should use #getType(). * * @return The {@link String} version of the {@link MetricType} * @deprecated */",80,83,[1],1,[0],0,[1],1,0,1,0,type(),org.logstash.instrument.metrics.Metric,type/0,False,81,2,2,0,2,1,2,3,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,21,65536,0,True
401,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\UptimeMetric.java,org.logstash.instrument.metrics.UptimeMetric,Long getValue(),"/**
 * @return the number of {@link TimeUnit}s that have elapsed
 *         since this {@link UptimeMetric} was instantiated
 */
@Override
public Long getValue() {
    return this.timeUnit.convert(getElapsedNanos(), TimeUnit.NANOSECONDS);
}","/**
 * @return the number of {@link TimeUnit}s that have elapsed
 *         since this {@link UptimeMetric} was instantiated
 */
", ,/** * @return the number of {@link TimeUnit}s that have elapsed *         since this {@link UptimeMetric} was instantiated */,81,84,[0],0,[0],0,[0],0,0,0,0,getValue(),org.logstash.instrument.metrics.UptimeMetric,getValue/0,False,82,1,4,3,1,1,2,3,1,0,0,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,19,1,0,True
402,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\UptimeMetric.java,org.logstash.instrument.metrics.UptimeMetric,MetricType getType(),"/**
 * @return the {@link MetricType}{@code .COUNTER_LONG} associated with
 *         long-valued metrics that only-increment, for use in Monitoring data structuring.
 */
@Override
public MetricType getType() {
    return MetricType.COUNTER_LONG;
}","/**
 * @return the {@link MetricType}{@code .COUNTER_LONG} associated with
 *         long-valued metrics that only-increment, for use in Monitoring data structuring.
 */
", ,"/** * @return the {@link MetricType}{@code .COUNTER_LONG} associated with *         long-valued metrics that only-increment, for use in Monitoring data structuring. */",94,97,[0],0,[0],0,[0],0,0,0,0,getType(),org.logstash.instrument.metrics.UptimeMetric,getType/0,False,95,1,1,1,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,1,0,True
403,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\UptimeMetric.java,org.logstash.instrument.metrics.UptimeMetric,TimeUnit getTimeUnit(),"/**
 * @return the {@link TimeUnit} associated with this {@link UptimeMetric}.
 */
public TimeUnit getTimeUnit() {
    return timeUnit;
}","/**
 * @return the {@link TimeUnit} associated with this {@link UptimeMetric}.
 */
", ,/** * @return the {@link TimeUnit} associated with this {@link UptimeMetric}. */,102,104,[0],0,[0],0,[0],0,0,0,0,getTimeUnit(),org.logstash.instrument.metrics.UptimeMetric,getTimeUnit/0,False,102,0,1,1,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,1,0,True
404,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\UptimeMetric.java,org.logstash.instrument.metrics.UptimeMetric,"UptimeMetric withTimeUnit(String, TimeUnit)","/**
 * Constructs a _copy_ of this {@link UptimeMetric} with a new name and timeUnit, but whose
 * uptime is tracking from the same instant as this instance.
 *
 * @param name the new metric's name (typically includes units)
 * @param timeUnit the new metric's units
 * @return a _copy_ of this {@link UptimeMetric}.
 */
public UptimeMetric withTimeUnit(final String name, final TimeUnit timeUnit) {
    return new UptimeMetric(name, this.nanoTimeSupplier, this.startNanos, timeUnit);
}","/**
 * Constructs a _copy_ of this {@link UptimeMetric} with a new name and timeUnit, but whose
 * uptime is tracking from the same instant as this instance.
 *
 * @param name the new metric's name (typically includes units)
 * @param timeUnit the new metric's units
 * @return a _copy_ of this {@link UptimeMetric}.
 */
", ,"/** * Constructs a _copy_ of this {@link UptimeMetric} with a new name and timeUnit, but whose * uptime is tracking from the same instant as this instance. * * @param name the new metric's name (typically includes units) * @param timeUnit the new metric's units * @return a _copy_ of this {@link UptimeMetric}. */",114,116,[0],0,[0],0,[0],0,0,0,0,"withTimeUnit(String, TimeUnit)",org.logstash.instrument.metrics.UptimeMetric,"withTimeUnit/2[java.lang.String,java.util.concurrent.TimeUnit]",False,114,1,2,1,1,1,0,3,1,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,25,1,0,True
405,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\UptimeMetric.java,org.logstash.instrument.metrics.UptimeMetric,"ScaledView withUnitsPrecise(String, ScaleUnits)","/**
 * Constructs a _view_ into this {@link UptimeMetric} whose value is a decimal number
 * containing subunit precision.
 *
 * @param name the name of the metric
 * @param scaleUnits the desired scale
 * @return a {@link BigDecimal} representing the whole-and-fractional number
 *         of {@link ScaleUnits} that have elapsed.
 */
public ScaledView withUnitsPrecise(final String name, final ScaleUnits scaleUnits) {
    return new ScaledView(name, this::getElapsedNanos, scaleUnits.nanoRelativeDecimalShift);
}","/**
 * Constructs a _view_ into this {@link UptimeMetric} whose value is a decimal number
 * containing subunit precision.
 *
 * @param name the name of the metric
 * @param scaleUnits the desired scale
 * @return a {@link BigDecimal} representing the whole-and-fractional number
 *         of {@link ScaleUnits} that have elapsed.
 */
", ,/** * Constructs a _view_ into this {@link UptimeMetric} whose value is a decimal number * containing subunit precision. * * @param name the name of the metric * @param scaleUnits the desired scale * @return a {@link BigDecimal} representing the whole-and-fractional number *         of {@link ScaleUnits} that have elapsed. */,127,129,[0],0,[0],0,[0],0,0,0,0,"withUnitsPrecise(String, ScaleUnits)",org.logstash.instrument.metrics.UptimeMetric,"withUnitsPrecise/2[java.lang.String,org.logstash.instrument.metrics.UptimeMetric.ScaleUnits]",False,127,2,3,2,1,1,0,3,1,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,34,1,0,True
406,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\UptimeMetric.java,org.logstash.instrument.metrics.UptimeMetric,ScaledView withUnitsPrecise(ScaleUnits),"/**
 * {@code name} defaults to something vaguely descriptive.
 * Useful when the caller doesn't need the metric name.
 *
 * @see UptimeMetric#withUnitsPrecise(String, ScaleUnits)
 */
public ScaledView withUnitsPrecise(final ScaleUnits scaleUnits) {
    final String name = String.format(""%s_scaled_to_%s"", getName(), scaleUnits.name());
    return this.withUnitsPrecise(name, scaleUnits);
}","/**
 * {@code name} defaults to something vaguely descriptive.
 * Useful when the caller doesn't need the metric name.
 *
 * @see UptimeMetric#withUnitsPrecise(String, ScaleUnits)
 */
", ,"/** * {@code name} defaults to something vaguely descriptive. * Useful when the caller doesn't need the metric name. * * @see UptimeMetric#withUnitsPrecise(String, ScaleUnits) */",137,140,[0],0,[0],0,[0],0,0,0,0,withUnitsPrecise(ScaleUnits),org.logstash.instrument.metrics.UptimeMetric,withUnitsPrecise/1[org.logstash.instrument.metrics.UptimeMetric.ScaleUnits],False,137,4,4,2,2,1,4,4,1,1,1,4,1,1,0,0,0,0,1,0,1,0,0,0,0,0,17,1,0,True
407,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\counter\CounterMetric.java,org.logstash.instrument.metrics.counter.CounterMetric,void increment(),"/**
 * Helper method that increments by 1
 */
void increment();","/**
 * Helper method that increments by 1
 */
", ,/** * Helper method that increments by 1 */,35,35,[0],0,[0],0,[0],0,0,0,0,increment(),org.logstash.instrument.metrics.counter.CounterMetric,increment/0,False,32,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,True
408,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\counter\CounterMetric.java,org.logstash.instrument.metrics.counter.CounterMetric,void increment(T),"/**
 * Increments the counter by the value specified. <i>The caller should be careful to avoid incrementing by values so large as to overflow the underlying type.</i>
 * @param by The value which to increment by. Can not be negative.
 */
void increment(T by);","/**
 * Increments the counter by the value specified. <i>The caller should be careful to avoid incrementing by values so large as to overflow the underlying type.</i>
 * @param by The value which to increment by. Can not be negative.
 */
", ,/** * Increments the counter by the value specified. <i>The caller should be careful to avoid incrementing by values so large as to overflow the underlying type.</i> * @param by The value which to increment by. Can not be negative. */,41,41,[0],0,[0],0,[0],0,0,0,0,increment(T),org.logstash.instrument.metrics.counter.CounterMetric,increment/1[T],False,37,1,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,25,0,0,True
409,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\counter\LongCounter.java,org.logstash.instrument.metrics.counter.LongCounter,"LongCounter fromRubyBase(AbstractNamespacedMetricExt, RubySymbol)","/**
 * Extracts the backing LongCounter from a Ruby
 * {@code LogStash::Instrument::MetricType::Counter} for efficient access by Java code.
 * @param metric Ruby {@code Logstash::Instrument::Metric}
 * @param key Identifier of the Counter
 * @return either the backing LongCounter or {@link #DUMMY_COUNTER} in case the input
 * {@code metric} was a Ruby {@code LogStash::Instrument::NullMetric}
 */
public static LongCounter fromRubyBase(final AbstractNamespacedMetricExt metric, final RubySymbol key) {
    final ThreadContext context = RubyUtil.RUBY.getCurrentContext();
    final IRubyObject counter = metric.counter(context, key);
    counter.callMethod(context, ""increment"", context.runtime.newFixnum(0));
    final LongCounter javaCounter;
    if (LongCounter.class.isAssignableFrom(counter.getJavaClass())) {
        javaCounter = counter.toJava(LongCounter.class);
    } else {
        javaCounter = DUMMY_COUNTER;
    }
    return javaCounter;
}","/**
 * Extracts the backing LongCounter from a Ruby
 * {@code LogStash::Instrument::MetricType::Counter} for efficient access by Java code.
 * @param metric Ruby {@code Logstash::Instrument::Metric}
 * @param key Identifier of the Counter
 * @return either the backing LongCounter or {@link #DUMMY_COUNTER} in case the input
 * {@code metric} was a Ruby {@code LogStash::Instrument::NullMetric}
 */
", ,/** * Extracts the backing LongCounter from a Ruby * {@code LogStash::Instrument::MetricType::Counter} for efficient access by Java code. * @param metric Ruby {@code Logstash::Instrument::Metric} * @param key Identifier of the Counter * @return either the backing LongCounter or {@link #DUMMY_COUNTER} in case the input * {@code metric} was a Ruby {@code LogStash::Instrument::NullMetric} */,53,65,[0],0,[0],0,[0],0,0,0,0,"fromRubyBase(AbstractNamespacedMetricExt, RubySymbol)",org.logstash.instrument.metrics.counter.LongCounter,"fromRubyBase/2[org.logstash.instrument.metrics.AbstractNamespacedMetricExt,org.logstash.instrument.metrics.counter.RubySymbol]",False,54,5,7,6,1,2,7,13,1,3,2,7,0,0,0,0,0,0,1,1,4,0,1,0,0,0,45,9,0,True
410,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\counter\LongCounter.java,org.logstash.instrument.metrics.counter.LongCounter,void increment(long),"/**
 * Optimized version of {@link #increment(Long)} to avoid auto-boxing.
 * @param by The value which to increment by. Can not be negative.
 */
public void increment(long by) {
    if (by < 0) {
        throw NEGATIVE_COUNT_EXCEPTION;
    }
    longAdder.add(by);
}","/**
 * Optimized version of {@link #increment(Long)} to avoid auto-boxing.
 * @param by The value which to increment by. Can not be negative.
 */
", ,/** * Optimized version of {@link #increment(Long)} to avoid auto-boxing. * @param by The value which to increment by. Can not be negative. */,104,109,[0],0,[0],0,[0],0,0,0,0,increment(long),org.logstash.instrument.metrics.counter.LongCounter,increment/1[long],False,104,0,13,13,0,2,1,6,0,0,1,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,28,1,0,True
411,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\counter\LongCounter.java,org.logstash.instrument.metrics.counter.LongCounter,void reset(),"/**
 * Resets the counter back to it's initial state.
 */
public void reset() {
    // replacing since LongAdder#reset ""is only effective if there are no concurrent updates"", we can not make that guarantee
    longAdder = new LongAdder();
}","/**
 * Resets the counter back to it's initial state.
 */
","// replacing since LongAdder#reset ""is only effective if there are no concurrent updates"", we can not make that guarantee
","/** * Resets the counter back to it's initial state. */[[SEP]]// replacing since LongAdder#reset ""is only effective if there are no concurrent updates"", we can not make that guarantee",114,117,[0],0,[0],0,"[0, 0]",0,0,0,0,reset(),org.logstash.instrument.metrics.counter.LongCounter,reset/0,False,114,0,1,1,0,1,0,3,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,10,1,0,True
412,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\gauge\GaugeMetric.java,org.logstash.instrument.metrics.gauge.GaugeMetric,void set(S),"/**
 * Sets the value
 * @param value The value to set
 */
void set(S value);","/**
 * Sets the value
 * @param value The value to set
 */
", ,/** * Sets the value * @param value The value to set */,38,38,[0],0,[0],0,[0],0,0,0,0,set(S),org.logstash.instrument.metrics.gauge.GaugeMetric,set/1[S],False,34,1,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,0,0,True
413,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\metrics\gauge\LazyDelegatingGauge.java,org.logstash.instrument.metrics.gauge.LazyDelegatingGauge,void wakeMetric(Object),"/**
 * Instantiates the metric based on the type used to set this Gauge
 *
 * @param value The object used to set this value
 */
@SuppressWarnings(""deprecation"")
private synchronized void wakeMetric(Object value) {
    if (lazyMetric == null && value != null) {
        // ""quack quack""
        if (value instanceof String) {
            lazyMetric = new TextGauge(key, (String) value);
        } else if (value instanceof Number) {
            lazyMetric = new NumberGauge(key, (Number) value);
        } else if (value instanceof Boolean) {
            lazyMetric = new BooleanGauge(key, (Boolean) value);
        } else if (value instanceof RubyHash) {
            lazyMetric = new RubyHashGauge(key, (RubyHash) value);
        } else if (value instanceof RubyTimestamp) {
            lazyMetric = new RubyTimeStampGauge(key, (RubyTimestamp) value);
        } else {
            LOGGER.warn(""A gauge metric of an unknown type ({}) has been created for key: {}. This may result in invalid serialization.  It is recommended to "" + ""log an issue to the responsible developer/development team."", value.getClass().getCanonicalName(), key);
            lazyMetric = new UnknownGauge(key, value);
        }
    }
}","/**
 * Instantiates the metric based on the type used to set this Gauge
 *
 * @param value The object used to set this value
 */
","// ""quack quack""
","/** * Instantiates the metric based on the type used to set this Gauge * * @param value The object used to set this value */[[SEP]]// ""quack quack""",99,119,[0],0,[0],0,"[0, 0]",0,0,0,0,wakeMetric(Object),org.logstash.instrument.metrics.gauge.LazyDelegatingGauge,wakeMetric/1[java.lang.Object],False,100,9,9,2,7,8,3,23,0,0,1,3,0,0,0,2,0,0,3,0,6,1,2,0,0,0,46,34,1,True
414,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\monitors\HotThreadsMonitor.java,org.logstash.instrument.monitors.HotThreadsMonitor,List<ThreadReport> detect(),"/**
 * Return the current hot threads information as provided by the JVM
 *
 * @return A list of ThreadReport including all selected threads
 */
public static List<ThreadReport> detect() {
    Map<String, String> options = new HashMap<>();
    options.put(ORDERED_BY, ""cpu"");
    return detect(options);
}","/**
 * Return the current hot threads information as provided by the JVM
 *
 * @return A list of ThreadReport including all selected threads
 */
", ,/** * Return the current hot threads information as provided by the JVM * * @return A list of ThreadReport including all selected threads */,134,138,[0],0,[0],0,[0],0,0,0,0,detect(),org.logstash.instrument.monitors.HotThreadsMonitor,detect/0,False,134,2,7,6,1,1,2,5,1,1,0,2,1,3,0,0,0,0,1,0,1,0,0,0,0,0,22,9,0,True
415,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\monitors\HotThreadsMonitor.java,org.logstash.instrument.monitors.HotThreadsMonitor,"List<ThreadReport> detect(Map<String, String>)","/**
 * Return the current hot threads information as provided by the JVM
 *
 * @param options Map of options to narrow this method functionality:
 *                Keys: ordered_by - can be ""cpu"", ""wait"" or ""block""
 *                      stacktrace_size - max depth of stack trace
 * @return A list of ThreadReport including all selected threads
 */
public static List<ThreadReport> detect(Map<String, String> options) {
    String type = ""cpu"";
    if (options.containsKey(ORDERED_BY)) {
        type = options.get(ORDERED_BY);
        if (!isValidSortOrder(type))
            throw new IllegalArgumentException(""Invalid sort order"");
    }
    Integer threadInfoMaxDepth = 50;
    if (options.containsKey(STACKTRACE_SIZE)) {
        threadInfoMaxDepth = Integer.valueOf(options.get(STACKTRACE_SIZE));
    }
    ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean();
    enableCpuTime(threadMXBean);
    Map<Long, ThreadReport> reports = new HashMap<>();
    for (long threadId : threadMXBean.getAllThreadIds()) {
        if (Thread.currentThread().getId() == threadId) {
            continue;
        }
        long cpuTime = threadMXBean.getThreadCpuTime(threadId);
        if (cpuTime == -1) {
            continue;
        }
        ThreadInfo info = threadMXBean.getThreadInfo(threadId, threadInfoMaxDepth);
        if (info != null) {
            /*
                 * Thread ID must exist and be alive, otherwise the threads just
                 * died in the meanwhile and could be ignored.
                 */
            reports.put(threadId, new ThreadReport(info, cpuTime));
        }
    }
    return sort(new ArrayList<>(reports.values()), type);
}","/**
 * Return the current hot threads information as provided by the JVM
 *
 * @param options Map of options to narrow this method functionality:
 *                Keys: ordered_by - can be ""cpu"", ""wait"" or ""block""
 *                      stacktrace_size - max depth of stack trace
 * @return A list of ThreadReport including all selected threads
 */
","/*
                 * Thread ID must exist and be alive, otherwise the threads just
                 * died in the meanwhile and could be ignored.
                 */
","/** * Return the current hot threads information as provided by the JVM * * @param options Map of options to narrow this method functionality: *                Keys: ordered_by - can be ""cpu"", ""wait"" or ""block"" *                      stacktrace_size - max depth of stack trace * @return A list of ThreadReport including all selected threads */[[SEP]]/*                 * Thread ID must exist and be alive, otherwise the threads just                 * died in the meanwhile and could be ignored.                 */",148,185,[0],0,[0],0,"[0, 0]",0,0,0,0,"detect(Map<String, String>)",org.logstash.instrument.monitors.HotThreadsMonitor,"detect/1[java.util.Map<java.lang.String,java.lang.String>]",False,148,2,10,6,4,8,14,28,1,6,1,14,3,2,1,3,0,0,2,2,8,0,2,0,0,0,73,9,0,True
416,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\monitors\HotThreadsMonitor.java,org.logstash.instrument.monitors.HotThreadsMonitor,void enableCpuTime(ThreadMXBean),"private static void enableCpuTime(ThreadMXBean threadMXBean) {
    try {
        if (threadMXBean.isThreadCpuTimeSupported()) {
            if (!threadMXBean.isThreadCpuTimeEnabled()) {
                threadMXBean.setThreadCpuTimeEnabled(true);
            }
        }
    } catch (SecurityException ex) {
        // This should not happen - the security manager should not be enabled.
        logger.debug(""Cannot enable Thread Cpu Time"", ex);
    }
}", ,"// This should not happen - the security manager should not be enabled.
",// This should not happen - the security manager should not be enabled.,206,217,[0],0,[0],0,[0],0,0,0,0,enableCpuTime(ThreadMXBean),org.logstash.instrument.monitors.HotThreadsMonitor,enableCpuTime/1[java.lang.management.ThreadMXBean],False,206,1,2,1,1,4,4,12,0,0,1,4,0,0,0,0,1,0,1,0,0,0,3,0,0,0,12,10,1,False
417,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\monitors\ProcessMonitor.java,org.logstash.instrument.monitors.ProcessMonitor,Method getCpuLoadMethod(),"/**
 * Retrieve the correct name of the method to get CPU load.
 * @return Method if the method could be found, null otherwise
 */
private static Method getCpuLoadMethod() {
    try {
        String methodName = (JavaVersionUtils.isJavaAtLeast(14)) ? ""getCpuLoad"" : ""getSystemCpuLoad"";
        return Class.forName(""com.sun.management.OperatingSystemMXBean"").getMethod(methodName);
    } catch (ReflectiveOperationException e) {
        LOGGER.warn(""OperatingSystemMXBean CPU load method not available, CPU load will not be measured"", e);
        return null;
    }
}","/**
 * Retrieve the correct name of the method to get CPU load.
 * @return Method if the method could be found, null otherwise
 */
", ,"/** * Retrieve the correct name of the method to get CPU load. * @return Method if the method could be found, null otherwise */",125,133,[0],0,[0],0,[0],0,0,0,0,getCpuLoadMethod(),org.logstash.instrument.monitors.ProcessMonitor,getCpuLoadMethod/0,False,125,2,2,0,2,3,4,10,2,1,0,4,0,0,0,0,1,1,4,1,1,0,1,0,0,0,26,10,1,True
418,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\monitors\ProcessMonitor.java,org.logstash.instrument.monitors.ProcessMonitor.Report,short getSystemCpuLoad(OperatingSystemMXBean),"// The method `getSystemCpuLoad` is deprecated in favour of `getCpuLoad` since JDK14
// This method uses reflection to use the correct method depending on the version of
// the JDK being used.
private short getSystemCpuLoad(OperatingSystemMXBean mxBeanInstance) {
    if (CPU_LOAD_METHOD == null) {
        return -1;
    }
    try {
        return scaleLoadToPercent((double) CPU_LOAD_METHOD.invoke(mxBeanInstance));
    } catch (Exception e) {
        return -1;
    }
}","// the JDK being used.
", ,// The method `getSystemCpuLoad` is deprecated in favour of `getCpuLoad` since JDK14// This method uses reflection to use the correct method depending on the version of// the JDK being used.,109,118,[0],0,[0],0,[0],0,0,0,0,getSystemCpuLoad(OperatingSystemMXBean),org.logstash.instrument.monitors.ProcessMonitor$Report,getSystemCpuLoad/1[java.lang.management.OperatingSystemMXBean],False,109,1,2,1,1,3,2,11,3,0,1,2,1,1,0,1,1,0,0,2,0,0,1,0,0,0,27,2,0,False
419,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\reports\MemoryReport.java,org.logstash.instrument.reports.MemoryReport,"Map<String, Map<String, Map<String, Object>>> generate()","/**
 * Build a report with current Memory information
 * @return Returns a Map containing information about the
 *         current state of the Java memory pools
 */
public static Map<String, Map<String, Map<String, Object>>> generate() {
    MemoryMonitor.Report report = generateReport(MemoryMonitor.Type.All);
    Map<String, Map<String, Map<String, Object>>> container = new HashMap<>();
    container.put(HEAP, report.getHeap());
    container.put(NON_HEAP, report.getNonHeap());
    return container;
}","/**
 * Build a report with current Memory information
 * @return Returns a Map containing information about the
 *         current state of the Java memory pools
 */
", ,/** * Build a report with current Memory information * @return Returns a Map containing information about the *         current state of the Java memory pools */,39,45,[0],0,[0],0,[0],0,0,0,0,generate(),org.logstash.instrument.reports.MemoryReport,generate/0,False,39,2,3,0,3,1,4,7,1,2,0,4,1,1,0,0,0,0,0,0,2,0,0,0,0,0,21,9,0,True
420,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\reports\ProcessReport.java,org.logstash.instrument.reports.ProcessReport,"Map<String, Object> generate()","/**
 * Build a report with current Process information
 * @return a Map with the current process report
 */
public static Map<String, Object> generate() {
    return ProcessMonitor.detect().toMap();
}","/**
 * Build a report with current Process information
 * @return a Map with the current process report
 */
", ,/** * Build a report with current Process information * @return a Map with the current process report */,34,36,[0],0,[0],0,[0],0,0,0,0,generate(),org.logstash.instrument.reports.ProcessReport,generate/0,False,34,2,2,0,2,1,2,3,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,9,0,True
421,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\reports\ThreadsReport.java,org.logstash.instrument.reports.ThreadsReport,"List<Map<String, Object>> generate(Map<String, String>)","/**
 * Generate a report with current Thread information
 * @param options Map of options to narrow this method functionality:
 *                Keys: ordered_by - can be ""cpu"", ""wait"" or ""block""
 *                      stacktrace_size - max depth of stack trace
 * @return A Map containing hot threads information
 */
public static List<Map<String, Object>> generate(Map<String, String> options) {
    List<HotThreadsMonitor.ThreadReport> reports = HotThreadsMonitor.detect(options);
    return reports.stream().map(HotThreadsMonitor.ThreadReport::toMap).collect(Collectors.toList());
}","/**
 * Generate a report with current Thread information
 * @param options Map of options to narrow this method functionality:
 *                Keys: ordered_by - can be ""cpu"", ""wait"" or ""block""
 *                      stacktrace_size - max depth of stack trace
 * @return A Map containing hot threads information
 */
", ,"/** * Generate a report with current Thread information * @param options Map of options to narrow this method functionality: *                Keys: ordered_by - can be ""cpu"", ""wait"" or ""block"" *                      stacktrace_size - max depth of stack trace * @return A Map containing hot threads information */",43,49,[0],0,[0],0,[0],0,0,0,0,"generate(Map<String, String>)",org.logstash.instrument.reports.ThreadsReport,"generate/1[java.util.Map<java.lang.String,java.lang.String>]",False,43,2,2,1,1,1,5,4,1,1,1,5,0,0,0,0,0,0,0,0,1,0,0,0,0,0,30,9,0,True
422,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\instrument\reports\ThreadsReport.java,org.logstash.instrument.reports.ThreadsReport,"List<Map<String, Object>> generate()","/**
 * Generate a report with current Thread information
 * @return A Map containing the hot threads information
 */
public static List<Map<String, Object>> generate() {
    Map<String, String> options = new HashMap<>();
    options.put(""order_by"", ""cpu"");
    return generate(options);
}","/**
 * Generate a report with current Thread information
 * @return A Map containing the hot threads information
 */
", ,/** * Generate a report with current Thread information * @return A Map containing the hot threads information */,56,60,[0],0,[0],0,[0],0,0,0,0,generate(),org.logstash.instrument.reports.ThreadsReport,generate/0,False,56,1,1,0,1,1,2,5,1,1,0,2,1,1,0,0,0,0,2,0,1,0,0,0,0,0,15,9,0,True
423,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\log\LoggableExt.java,org.logstash.log.LoggableExt,String log4jName(RubyModule),"private static String log4jName(final RubyModule self) {
    String name;
    if (self.getBaseName() == null) {
        // anonymous module/class
        RubyModule real = self;
        if (self instanceof RubyClass) {
            real = ((RubyClass) self).getRealClass();
        }
        // for anonymous: ""#<Class:0xcafebabe>""
        name = real.getName();
    } else {
        name = self.getName();
    }
    return name.replace(""::"", ""."").toLowerCase(Locale.ENGLISH);
}", ,"// anonymous module/class
[[SEP]]// for anonymous: ""#<Class:0xcafebabe>""
","// anonymous module/class[[SEP]]// for anonymous: ""#<Class:0xcafebabe>""",70,82,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,log4jName(RubyModule),org.logstash.log.LoggableExt,log4jName/1[org.logstash.log.RubyModule],False,70,2,3,3,0,3,5,14,1,2,1,5,0,0,0,1,0,1,2,0,4,0,2,0,0,0,9,10,0,False
424,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\log\LogstashConfigurationFactory.java,org.logstash.log.LogstashConfigurationFactory,"PropertiesConfiguration getConfiguration(LoggerContext, ConfigurationSource)","@Override
public PropertiesConfiguration getConfiguration(final LoggerContext loggerContext, final ConfigurationSource source) {
    final Properties properties = new Properties();
    try (final InputStream configStream = source.getInputStream()) {
        properties.load(configStream);
    } catch (final IOException ioe) {
        throw new ConfigurationException(""Unable to load "" + source.toString(), ioe);
    }
    PropertiesConfiguration propertiesConfiguration = new PropertiesConfigurationBuilder().setConfigurationSource(source).setRootProperties(properties).setLoggerContext(loggerContext).build();
    if (System.getProperty(PIPELINE_SEPARATE_LOGS, ""false"").equals(""false"")) {
        // force init to avoid overwrite of appenders section
        propertiesConfiguration.initialize();
        propertiesConfiguration.removeAppender(PIPELINE_ROUTING_APPENDER_NAME);
    }
    return propertiesConfiguration;
}", ,"// force init to avoid overwrite of appenders section
",// force init to avoid overwrite of appenders section,52,73,[0],0,[0],0,[0],0,0,0,0,"getConfiguration(LoggerContext, ConfigurationSource)",org.logstash.log.LogstashConfigurationFactory,"getConfiguration/2[org.apache.logging.log4j.core.LoggerContext,org.apache.logging.log4j.core.config.ConfigurationSource]",False,53,6,10,0,10,3,11,15,1,3,2,11,0,0,0,0,1,0,3,0,3,1,1,0,0,0,31,1,0,False
425,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\log\LogstashLoggerContextFactory.java,org.logstash.log.LogstashLoggerContextFactory,void removeContext(LoggerContext),"@Override
public void removeContext(LoggerContext context) {
    // do nothing
}", ,"// do nothing
",// do nothing,57,60,[0],0,[0],0,[0],0,0,0,0,removeContext(LoggerContext),org.logstash.log.LogstashLoggerContextFactory,removeContext/1[org.apache.logging.log4j.spi.LoggerContext],False,58,1,0,0,0,1,0,2,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,1,0,False
426,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\log\PipelineRoutingAppender.java,org.logstash.log.PipelineRoutingAppender,B newBuilder(),"/**
 * Factory method to instantiate the appender
 */
@PluginBuilderFactory
public static <B extends PipelineRoutingAppender.Builder<B>> B newBuilder() {
    return new PipelineRoutingAppender.Builder<B>().asBuilder();
}","/**
 * Factory method to instantiate the appender
 */
", ,/** * Factory method to instantiate the appender */,69,72,[0],0,[0],0,[0],0,0,0,0,newBuilder(),org.logstash.log.PipelineRoutingAppender,newBuilder/0,False,70,4,2,0,2,1,1,3,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,9,0,True
427,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\log\PipelineRoutingAppender.java,org.logstash.log.PipelineRoutingAppender,"Map<String, AppenderControl> getAppenders()","/**
 * Returns an unmodifiable view of the appenders created by this {@link PipelineRoutingAppender}.
 */
public Map<String, AppenderControl> getAppenders() {
    return createdAppendersUnmodifiableView;
}","/**
 * Returns an unmodifiable view of the appenders created by this {@link PipelineRoutingAppender}.
 */
", ,/** * Returns an unmodifiable view of the appenders created by this {@link PipelineRoutingAppender}. */,89,91,[0],0,[0],0,[0],0,0,0,0,getAppenders(),org.logstash.log.PipelineRoutingAppender,getAppenders/0,False,89,1,0,0,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,1,0,True
428,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\log\PipelineRoutingAppender.java,org.logstash.log.PipelineRoutingAppender,void append(LogEvent),"/**
 * Core method to apply the logic of routing.
 */
@Override
public void append(LogEvent event) {
    AppenderControl appenderControl = getControl(event);
    if (appenderControl != null) {
        appenderControl.callAppender(event);
    }
}","/**
 * Core method to apply the logic of routing.
 */
", ,/** * Core method to apply the logic of routing. */,96,103,[0],0,[0],0,[0],0,0,0,0,append(LogEvent),org.logstash.log.PipelineRoutingAppender,append/1[org.apache.logging.log4j.core.LogEvent],False,97,3,2,0,2,2,2,6,0,1,1,2,1,2,0,1,0,0,0,0,1,0,1,0,0,0,16,1,0,True
429,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\log\PipelineRoutingAppender.java,org.logstash.log.PipelineRoutingAppender,AppenderControl getControl(LogEvent),"/**
 * Create or retrieve the sub appender for the pipeline.id provided into the event
 */
private AppenderControl getControl(LogEvent event) {
    String key = event.getContextData().getValue(""pipeline.id"");
    if (key == null) {
        LOGGER.debug(""Unable to find the pipeline.id in event's context data in routing appender, skip it"");
        // this prevent to create an appender when log events are not fish-tagged with pipeline.id,
        // avoid to create log file like ""pipeline_${ctx:pipeline.id}.log"" which contains duplicated
        // logs from the logstash-* files
        return null;
    }
    AppenderControl appenderControl = createdAppenders.get(key);
    if (appenderControl == null) {
        synchronized (this) {
            appenderControl = createdAppenders.get(key);
            if (appenderControl == null) {
                // create new appender and control
                final Appender app = createAppender(event);
                if (app == null) {
                    return null;
                }
                AppenderControl created = new AppenderControl(app, null, null);
                appenderControl = created;
                createdAppenders.put(key, created);
            }
        }
    }
    return appenderControl;
}","/**
 * Create or retrieve the sub appender for the pipeline.id provided into the event
 */
","// this prevent to create an appender when log events are not fish-tagged with pipeline.id,
[[SEP]]// avoid to create log file like ""pipeline_${ctx:pipeline.id}.log"" which contains duplicated
[[SEP]]// logs from the logstash-* files
[[SEP]]// create new appender and control
","/** * Create or retrieve the sub appender for the pipeline.id provided into the event */[[SEP]]// this prevent to create an appender when log events are not fish-tagged with pipeline.id,// avoid to create log file like ""pipeline_${ctx:pipeline.id}.log"" which contains duplicated// logs from the logstash-* files[[SEP]]// create new appender and control",108,135,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,getControl(LogEvent),org.logstash.log.PipelineRoutingAppender,getControl/1[org.apache.logging.log4j.core.LogEvent],False,108,6,6,1,5,5,6,23,3,4,1,6,1,1,0,4,0,0,2,0,6,0,4,0,0,0,26,2,1,True
430,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\log\PipelineRoutingAppender.java,org.logstash.log.PipelineRoutingAppender,Appender createAppender(LogEvent),"/**
 * Used by @{@link #getControl(LogEvent)} to create new subappenders for not yet encountered pipelines.
 */
private Appender createAppender(final LogEvent event) {
    for (final Node node : appenderNode.getChildren()) {
        if (node.getType().getElementName().equals(Appender.ELEMENT_TYPE)) {
            final Node appNode = new Node(node);
            configuration.createConfiguration(appNode, event);
            if (appNode.getObject() instanceof Appender) {
                final Appender app = appNode.getObject();
                app.start();
                return app;
            }
            error(""Unable to create Appender of type "" + node.getName());
            return null;
        }
    }
    error(""No Appender was configured for  "" + getName());
    return null;
}","/**
 * Used by @{@link #getControl(LogEvent)} to create new subappenders for not yet encountered pipelines.
 */
", ,/** * Used by @{@link #getControl(LogEvent)} to create new subappenders for not yet encountered pipelines. */,141,157,[0],0,[0],0,[0],0,0,0,0,createAppender(LogEvent),org.logstash.log.PipelineRoutingAppender,createAppender/1[org.apache.logging.log4j.core.LogEvent],False,141,7,11,1,10,4,10,17,3,2,1,10,0,0,1,0,0,0,2,0,2,2,3,0,0,0,23,2,0,True
431,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\log\PipelineRoutingFilter.java,org.logstash.log.PipelineRoutingFilter,PipelineRoutingFilter createFilter(),"/**
 * Factory method to instantiate the filter
 */
@PluginFactory
public static PipelineRoutingFilter createFilter() {
    return new PipelineRoutingFilter();
}","/**
 * Factory method to instantiate the filter
 */
", ,/** * Factory method to instantiate the filter */,21,24,[0],0,[0],0,[0],0,0,0,0,createFilter(),org.logstash.log.PipelineRoutingFilter,createFilter/0,False,22,2,1,0,1,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,9,0,True
432,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\log\PipelineRoutingFilter.java,org.logstash.log.PipelineRoutingFilter,Result filter(LogEvent),"/**
 * Contains the main logic to execute in filtering.
 *
 * Deny the logging of an event when separate logs feature is enabled and the event is fish tagged with a
 * pipeline id.
 *
 * @param event the log to filter.
 */
@Override
public Result filter(LogEvent event) {
    final boolean directedToPipelineLog = isSeparateLogs && event.getContextData().containsKey(""pipeline.id"");
    if (directedToPipelineLog) {
        return Result.DENY;
    }
    return Result.NEUTRAL;
}","/**
 * Contains the main logic to execute in filtering.
 *
 * Deny the logging of an event when separate logs feature is enabled and the event is fish tagged with a
 * pipeline id.
 *
 * @param event the log to filter.
 */
", ,/** * Contains the main logic to execute in filtering. * * Deny the logging of an event when separate logs feature is enabled and the event is fish tagged with a * pipeline id. * * @param event the log to filter. */,41,49,[0],0,[0],0,[0],0,0,0,0,filter(LogEvent),org.logstash.log.PipelineRoutingFilter,filter/1[org.apache.logging.log4j.core.LogEvent],False,42,3,2,0,2,2,2,7,2,1,1,2,0,0,0,0,0,0,1,0,1,0,1,0,0,0,34,1,0,True
433,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\AliasRegistry.java,org.logstash.plugins.AliasRegistry,"String resolveAlias(String, String)","/**
 * if pluginName is an alias then return the real plugin name else return it unchanged
 */
public String resolveAlias(String type, String pluginName) {
    final PluginCoordinate pluginCoord = new PluginCoordinate(PluginType.valueOf(type.toUpperCase()), pluginName);
    return aliases.getOrDefault(pluginCoord, pluginName);
}","/**
 * if pluginName is an alias then return the real plugin name else return it unchanged
 */
", ,/** * if pluginName is an alias then return the real plugin name else return it unchanged */,272,275,[0],0,[0],0,[0],0,0,0,0,"resolveAlias(String, String)",org.logstash.plugins.AliasRegistry,"resolveAlias/2[java.lang.String,java.lang.String]",False,272,2,2,0,2,1,3,4,1,1,2,3,0,0,0,0,0,0,0,0,1,0,0,0,0,0,16,1,0,True
434,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\AliasRegistry.java,org.logstash.plugins.AliasRegistry.YamlWithChecksum,YamlWithChecksum load(InputStream),"private static YamlWithChecksum load(InputStream in) {
    try (Scanner scanner = new Scanner(in, StandardCharsets.UTF_8.name())) {
        // read the header line
        final String header = scanner.nextLine();
        if (!header.startsWith(""#CHECKSUM:"")) {
            throw new IllegalArgumentException(""Bad header format, expected '#CHECKSUM: ...' but found "" + header);
        }
        final String extractedHash = header.substring(""#CHECKSUM:"".length()).trim();
        // read the comment
        scanner.nextLine();
        // collect all remaining lines
        final StringBuilder yamlBuilder = new StringBuilder();
        // EOF
        scanner.useDelimiter(""\\z"");
        if (scanner.hasNext()) {
            yamlBuilder.append(scanner.next());
        }
        final String yamlContents = yamlBuilder.toString();
        return new YamlWithChecksum(yamlContents, extractedHash);
    }
}", ,"// read the header line
[[SEP]]// read the comment
[[SEP]]// collect all remaining lines
[[SEP]]// EOF
",// read the header line[[SEP]]// read the comment[[SEP]]// collect all remaining lines[[SEP]]// EOF,70,91,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,load(InputStream),org.logstash.plugins.AliasRegistry$YamlWithChecksum,load/1[java.io.InputStream],False,70,1,2,1,1,3,11,17,1,5,1,11,0,0,0,0,1,0,4,0,5,1,2,0,0,0,22,10,0,False
435,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\AliasRegistry.java,org.logstash.plugins.AliasRegistry.AliasYamlLoader,"Map<PluginCoordinate, String> loadAliasesDefinitionsFromInputStream(InputStream)","private Map<PluginCoordinate, String> loadAliasesDefinitionsFromInputStream(InputStream in) {
    final YamlWithChecksum aliasYml = YamlWithChecksum.load(in);
    final String calculatedHash = aliasYml.computeHashFromContent();
    if (!calculatedHash.equals(aliasYml.checksumHash)) {
        LOGGER.warn(""Bad checksum value, expected {} but found {}"", calculatedHash, aliasYml.checksumHash);
        return Collections.emptyMap();
    }
    // decode yaml to Map<PluginType, List<AliasPlugin>> structure
    final Map<PluginType, List<AliasPlugin>> aliasedDescriptions;
    try {
        aliasedDescriptions = aliasYml.decodeYaml();
    } catch (IOException ioex) {
        LOGGER.error(""Error decoding the yaml aliases file"", ioex);
        return Collections.emptyMap();
    }
    // convert aliases nested maps definitions to plugin alias definitions
    final Map<PluginCoordinate, String> defaultDefinitions = new HashMap<>();
    defaultDefinitions.putAll(extractDefinitions(PluginType.INPUT, aliasedDescriptions));
    defaultDefinitions.putAll(extractDefinitions(PluginType.CODEC, aliasedDescriptions));
    defaultDefinitions.putAll(extractDefinitions(PluginType.FILTER, aliasedDescriptions));
    defaultDefinitions.putAll(extractDefinitions(PluginType.OUTPUT, aliasedDescriptions));
    return defaultDefinitions;
}", ,"// decode yaml to Map<PluginType, List<AliasPlugin>> structure
[[SEP]]// convert aliases nested maps definitions to plugin alias definitions
","// decode yaml to Map<PluginType, List<AliasPlugin>> structure[[SEP]]// convert aliases nested maps definitions to plugin alias definitions",150,174,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,loadAliasesDefinitionsFromInputStream(InputStream),org.logstash.plugins.AliasRegistry$AliasYamlLoader,loadAliasesDefinitionsFromInputStream/1[java.io.InputStream],False,150,6,8,2,6,3,9,22,3,4,1,9,1,1,0,0,1,0,2,0,4,0,1,0,0,0,30,2,2,False
436,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\ConfigVariableExpander.java,org.logstash.plugins.ConfigVariableExpander,ConfigVariableExpander withoutSecret(EnvironmentVariableProvider),"/**
 * Creates a ConfigVariableExpander that doesn't lookup any secreted placeholder.
 *
 * @param envVarProvider EnvironmentVariableProvider to use as source of substitutions
 * @return an variable expander that uses envVarProvider as source
 */
public static ConfigVariableExpander withoutSecret(EnvironmentVariableProvider envVarProvider) {
    return new ConfigVariableExpander(null, envVarProvider);
}","/**
 * Creates a ConfigVariableExpander that doesn't lookup any secreted placeholder.
 *
 * @param envVarProvider EnvironmentVariableProvider to use as source of substitutions
 * @return an variable expander that uses envVarProvider as source
 */
", ,/** * Creates a ConfigVariableExpander that doesn't lookup any secreted placeholder. * * @param envVarProvider EnvironmentVariableProvider to use as source of substitutions * @return an variable expander that uses envVarProvider as source */,50,52,[0],0,[0],0,[0],0,0,0,0,withoutSecret(EnvironmentVariableProvider),org.logstash.plugins.ConfigVariableExpander,withoutSecret/1[org.logstash.common.EnvironmentVariableProvider],False,50,2,33,32,1,1,0,3,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,25,9,0,True
437,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\ConfigVariableExpander.java,org.logstash.plugins.ConfigVariableExpander,"Object expand(Object, boolean)","/**
 * Replace all substitution variable references and returns the substituted value or the original value
 * if a substitution cannot be made.
 *
 * Substitution variables have the patterns: <code>${VAR}</code> or <code>${VAR:defaultValue}</code>
 *
 * If a substitution variable is found, the following precedence applies:
 *   Secret store value
 *   Environment entry value
 *   Default value if provided in the pattern
 *   Exception raised
 *
 * If a substitution variable is not found, the value is return unchanged
 *
 * @param value Config value in which substitution variables, if any, should be replaced.
 * @param keepSecrets True if secret stores resolved variables must be kept secret in a Password instance
 * @return Config value with any substitution variables replaced
 */
public Object expand(Object value, boolean keepSecrets) {
    if (!(value instanceof String)) {
        return value;
    }
    String variable = (String) value;
    Matcher m = substitutionPattern.matcher(variable);
    if (!m.matches()) {
        return variable;
    }
    String variableName = m.group(""name"");
    if (secretStore != null) {
        byte[] ssValue = secretStore.retrieveSecret(new SecretIdentifier(variableName));
        if (ssValue != null) {
            if (keepSecrets) {
                return new SecretVariable(variableName, new String(ssValue, StandardCharsets.UTF_8));
            } else {
                return new String(ssValue, StandardCharsets.UTF_8);
            }
        }
    }
    if (envVarProvider != null) {
        String evValue = envVarProvider.get(variableName);
        if (evValue != null) {
            return evValue;
        }
    }
    String defaultValue = m.group(""default"");
    if (defaultValue == null) {
        throw new IllegalStateException(String.format(""Cannot evaluate `%s`. Replacement variable `%s` is not defined in a Logstash "" + ""secret store or an environment entry and there is no default value given."", variable, variableName));
    }
    return defaultValue;
}","/**
 * Replace all substitution variable references and returns the substituted value or the original value
 * if a substitution cannot be made.
 *
 * Substitution variables have the patterns: <code>${VAR}</code> or <code>${VAR:defaultValue}</code>
 *
 * If a substitution variable is found, the following precedence applies:
 *   Secret store value
 *   Environment entry value
 *   Default value if provided in the pattern
 *   Exception raised
 *
 * If a substitution variable is not found, the value is return unchanged
 *
 * @param value Config value in which substitution variables, if any, should be replaced.
 * @param keepSecrets True if secret stores resolved variables must be kept secret in a Password instance
 * @return Config value with any substitution variables replaced
 */
", ,"/** * Replace all substitution variable references and returns the substituted value or the original value * if a substitution cannot be made. * * Substitution variables have the patterns: <code>${VAR}</code> or <code>${VAR:defaultValue}</code> * * If a substitution variable is found, the following precedence applies: *   Secret store value *   Environment entry value *   Default value if provided in the pattern *   Exception raised * * If a substitution variable is not found, the value is return unchanged * * @param value Config value in which substitution variables, if any, should be replaced. * @param keepSecrets True if secret stores resolved variables must be kept secret in a Password instance * @return Config value with any substitution variables replaced */",77,116,[0],0,[0],0,[0],0,0,0,0,"expand(Object, boolean)",org.logstash.plugins.ConfigVariableExpander,"expand/2[java.lang.Object,boolean]",False,77,5,7,3,4,9,6,33,6,6,2,6,0,0,0,5,0,1,4,0,6,1,3,0,0,0,80,1,0,True
438,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\ConfigVariableExpander.java,org.logstash.plugins.ConfigVariableExpander,void close(),"@Override
public void close() {
    // most keystore implementations will have close() methods
}", ,"// most keystore implementations will have close() methods
",// most keystore implementations will have close() methods,122,126,[0],0,[0],0,[0],0,0,0,0,close(),org.logstash.plugins.ConfigVariableExpander,close/0,False,123,0,0,0,0,1,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,False
439,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\PluginClassLoader.java,org.logstash.plugins.PluginClassLoader,"PluginClassLoader create(String, ClassLoader)","/**
 * Creates an instance of the plugin classloader.
 * @param jarPath Full path to the Java plugin's JAR file.
 * @param appClassLoader Application classloader to be used for classes not found
 *                       in the plugin's JAR file.
 * @return New instance of the plugin classloader.
 */
public static PluginClassLoader create(String jarPath, ClassLoader appClassLoader) {
    Path pluginJar = Paths.get(jarPath);
    if (!Files.exists(pluginJar)) {
        throw new IllegalStateException(""PluginClassLoader unable to locate jar file: "" + jarPath);
    }
    try {
        URL[] pluginJarUrl = new URL[] { pluginJar.toUri().toURL() };
        return new PluginClassLoader(pluginJarUrl, appClassLoader);
    } catch (MalformedURLException e) {
        throw new IllegalStateException(e);
    }
}","/**
 * Creates an instance of the plugin classloader.
 * @param jarPath Full path to the Java plugin's JAR file.
 * @param appClassLoader Application classloader to be used for classes not found
 *                       in the plugin's JAR file.
 * @return New instance of the plugin classloader.
 */
", ,/** * Creates an instance of the plugin classloader. * @param jarPath Full path to the Java plugin's JAR file. * @param appClassLoader Application classloader to be used for classes not found *                       in the plugin's JAR file. * @return New instance of the plugin classloader. */,52,63,[0],0,[0],0,[0],0,0,0,0,"create(String, ClassLoader)",org.logstash.plugins.PluginClassLoader,"create/2[java.lang.String,java.lang.ClassLoader]",False,52,1,1,0,1,3,4,13,1,2,2,4,0,0,0,0,1,0,1,0,2,1,1,0,0,0,39,9,0,True
440,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\PluginUtil.java,org.logstash.plugins.PluginUtil,"List<String> doValidateConfig(Plugin, Configuration)","@VisibleForTesting
public static List<String> doValidateConfig(Plugin plugin, Configuration config) {
    List<String> configErrors = new ArrayList<>();
    List<String> configSchemaNames = plugin.configSchema().stream().map(PluginConfigSpec::name).collect(Collectors.toList());
    // find config options that are invalid for the specified plugin
    Collection<String> providedConfig = config.allKeys();
    for (String configKey : providedConfig) {
        if (!configSchemaNames.contains(configKey)) {
            configErrors.add(String.format(""Unknown setting '%s' specified for plugin '%s'"", configKey, plugin.getName()));
        }
    }
    // find required config options that are missing
    for (PluginConfigSpec<?> configSpec : plugin.configSchema()) {
        if (configSpec.required() && !providedConfig.contains(configSpec.name())) {
            configErrors.add(String.format(""Required setting '%s' not specified for plugin '%s'"", configSpec.name(), plugin.getName()));
        }
    }
    return configErrors;
}", ,"// find config options that are invalid for the specified plugin
[[SEP]]// find required config options that are missing
",// find config options that are invalid for the specified plugin[[SEP]]// find required config options that are missing,55,80,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,"doValidateConfig(Plugin, Configuration)",org.logstash.plugins.PluginUtil,"doValidateConfig/2[co.elastic.logstash.api.Plugin,co.elastic.logstash.api.Configuration]",False,56,4,6,1,5,6,13,16,1,3,2,13,0,0,2,0,0,0,2,0,3,0,2,0,0,0,15,9,0,False
441,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\codecs\Line.java,org.logstash.plugins.codecs.Line,"void decode(ByteBuffer, Consumer<Map<String, Object>>)","@Override
public void decode(ByteBuffer buffer, Consumer<Map<String, Object>> eventConsumer) {
    int bufferPosition = buffer.position();
    CoderResult result = decoder.decode(buffer, charBuffer, false);
    charBuffer.flip();
    String s = (remainder == null ? """" : remainder) + charBuffer.toString();
    charBuffer.clear();
    if (s.endsWith(delimiter)) {
        // strip trailing delimiter, if any, to match Ruby implementation
        s = s.substring(0, s.length() - delimiter.length());
    } else {
        int lastIndex = s.lastIndexOf(delimiter);
        if (lastIndex == -1) {
            buffer.position(bufferPosition);
            s = """";
        } else {
            remainder = s.substring(lastIndex + delimiter.length(), s.length());
            s = s.substring(0, lastIndex);
        }
    }
    if (s.length() > 0) {
        String[] lines = s.split(delimiter, 0);
        for (int k = 0; k < lines.length; k++) {
            eventConsumer.accept(simpleMap(lines[k]));
        }
    }
}", ,"// strip trailing delimiter, if any, to match Ruby implementation
","// strip trailing delimiter, if any, to match Ruby implementation",110,138,[0],0,[0],0,[0],0,0,0,0,"decode(ByteBuffer, Consumer<Map<String, Object>>)",org.logstash.plugins.codecs.Line,"decode/2[java.nio.ByteBuffer,java.util.function.Consumer<java.util.Map<java.lang.String,java.lang.Object>>]",False,111,1,5,4,1,6,13,27,0,6,2,13,1,1,1,2,0,1,2,6,10,3,2,0,0,0,20,1,0,False
442,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\discovery\PluginRegistry.java,org.logstash.plugins.discovery.PluginRegistry,void discoverPlugins(),"@SuppressWarnings(""unchecked"")
private void discoverPlugins() {
    // the constructor of Reflection must be called only by one thread, else there is a
    // risk that the first thread that completes close the Zip files for the others.
    // scan all .class present in package classpath
    final ConfigurationBuilder configurationBuilder = new ConfigurationBuilder().setUrls(ClasspathHelper.forPackage(""org.logstash.plugins"")).filterInputsBy(input -> input.endsWith("".class""));
    Reflections reflections = new Reflections(configurationBuilder);
    Set<Class<?>> annotated = reflections.getTypesAnnotatedWith(LogstashPlugin.class);
    for (final Class<?> cls : annotated) {
        for (final Annotation annotation : cls.getAnnotations()) {
            if (annotation instanceof LogstashPlugin) {
                String name = ((LogstashPlugin) annotation).name();
                if (Filter.class.isAssignableFrom(cls)) {
                    filters.put(name, (Class<Filter>) cls);
                }
                if (Output.class.isAssignableFrom(cls)) {
                    outputs.put(name, (Class<Output>) cls);
                }
                if (Input.class.isAssignableFrom(cls)) {
                    inputs.put(name, (Class<Input>) cls);
                }
                if (Codec.class.isAssignableFrom(cls)) {
                    codecs.put(name, (Class<Codec>) cls);
                }
                break;
            }
        }
    }
    // after loaded all plugins, check if aliases has to be provided
    addAliasedPlugins(PluginType.FILTER, filters);
    addAliasedPlugins(PluginType.OUTPUT, outputs);
    addAliasedPlugins(PluginType.INPUT, inputs);
    addAliasedPlugins(PluginType.CODEC, codecs);
}", ,"// the constructor of Reflection must be called only by one thread, else there is a
[[SEP]]// risk that the first thread that completes close the Zip files for the others.
[[SEP]]// scan all .class present in package classpath
[[SEP]]// after loaded all plugins, check if aliases has to be provided
","// the constructor of Reflection must be called only by one thread, else there is a// risk that the first thread that completes close the Zip files for the others.// scan all .class present in package classpath[[SEP]]// after loaded all plugins, check if aliases has to be provided",80,118,[0],0,"[0, 0, 0, 0]",0,"[0, 0]",0,0,0,0,discoverPlugins(),org.logstash.plugins.discovery.PluginRegistry,discoverPlugins/0,False,81,9,10,1,9,8,19,29,0,5,0,19,0,0,2,0,0,1,3,0,4,0,4,0,0,1,17,2,0,False
443,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\discovery\PluginRegistry.java,org.logstash.plugins.discovery.PluginRegistry,"void addAliasedPlugins(PluginType, Map<String, Class<T>>)","private <T> void addAliasedPlugins(PluginType type, Map<String, Class<T>> pluginCache) {
    final Map<String, Class<T>> aliasesToAdd = new HashMap<>();
    for (Map.Entry<String, Class<T>> e : pluginCache.entrySet()) {
        final String realPluginName = e.getKey();
        final Optional<String> alias = aliasRegistry.aliasFromOriginal(type, realPluginName);
        if (alias.isPresent()) {
            final String aliasName = alias.get();
            if (!pluginCache.containsKey(aliasName)) {
                // no real plugin with same alias name was found
                aliasesToAdd.put(aliasName, e.getValue());
                final String typeStr = type.name().toLowerCase();
                LOGGER.info(""Plugin {}-{} is aliased as {}-{}"", typeStr, realPluginName, typeStr, aliasName);
            }
        }
    }
    for (Map.Entry<String, Class<T>> e : aliasesToAdd.entrySet()) {
        pluginCache.put(e.getKey(), e.getValue());
    }
}", ,"// no real plugin with same alias name was found
",// no real plugin with same alias name was found,120,138,[0],0,[0],0,[0],0,0,0,0,"addAliasedPlugins(PluginType, Map<String, Class<T>>)",org.logstash.plugins.discovery.PluginRegistry,"addAliasedPlugins/2[org.logstash.plugins.PluginLookup.PluginType,java.util.Map<java.lang.String,java.lang.Class<T>>]",False,120,4,2,0,2,5,11,18,0,5,2,11,0,0,2,0,0,0,1,0,5,0,3,0,0,0,19,2,1,False
444,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\factory\ContextualizerExt.java,org.logstash.plugins.factory.ContextualizerExt,"IRubyObject initializePlugin(ThreadContext, IRubyObject, IRubyObject[], Block)","/*
     * @overload PluginContextualizer::initialize_plugin(execution_context, plugin_class, *plugin_args, &pluginBlock)
     */
@JRubyMethod(name = ""initialize_plugin"", meta = true, required = 2, rest = true)
public static IRubyObject initializePlugin(final ThreadContext context, final IRubyObject recv, final IRubyObject[] args, final Block block) {
    final List<IRubyObject> argsList = new ArrayList<>(Arrays.asList(args));
    final ExecutionContextExt executionContext = RubyUtil.nilSafeCast(argsList.remove(0));
    final RubyClass pluginClass = (RubyClass) argsList.remove(0);
    final IRubyObject[] pluginArgs = argsList.toArray(new IRubyObject[] {});
    return initializePlugin(context, (RubyModule) recv, executionContext, pluginClass, pluginArgs, block);
}","/*
     * @overload PluginContextualizer::initialize_plugin(execution_context, plugin_class, *plugin_args, &pluginBlock)
     */
", ,"/*     * @overload PluginContextualizer::initialize_plugin(execution_context, plugin_class, *plugin_args, &pluginBlock)     */",36,47,[0],0,[0],0,[0],0,0,0,0,"initializePlugin(ThreadContext, IRubyObject, IRubyObject[], Block)",org.logstash.plugins.factory.ContextualizerExt,"initializePlugin/4[org.logstash.plugins.factory.ThreadContext,org.logstash.plugins.factory.IRubyObject,org.logstash.plugins.factory.IRubyObject[],org.logstash.plugins.factory.Block]",False,40,9,2,0,2,1,5,7,1,4,4,5,1,1,0,0,0,0,1,3,4,0,0,0,0,0,20,9,0,False
445,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\factory\ContextualizerExt.java,org.logstash.plugins.factory.ContextualizerExt,"IRubyObject initializePlugin(ThreadContext, RubyModule, ExecutionContextExt, RubyClass, IRubyObject[], Block)","private static IRubyObject initializePlugin(final ThreadContext context, final RubyModule recv, @Nullable final ExecutionContextExt executionContext, final RubyClass pluginClass, final IRubyObject[] pluginArgs, final Block block) {
    synchronized (ContextualizerExt.class) {
        if (!pluginClass.hasModuleInPrepends(recv)) {
            pluginClass.prepend(context, new IRubyObject[] { recv });
        }
    }
    final IRubyObject[] pluginInitArgs;
    if (executionContext == null) {
        pluginInitArgs = pluginArgs;
    } else {
        List<IRubyObject> pluginInitArgList = new ArrayList<>(1 + pluginArgs.length);
        pluginInitArgList.add(executionContext);
        pluginInitArgList.addAll(Arrays.asList(pluginArgs));
        pluginInitArgs = pluginInitArgList.toArray(new IRubyObject[] {});
    }
    // We must use IRubyObject#callMethod(...,""new"",...) here to continue supporting
    // mocking/validating from rspec.
    return pluginClass.callMethod(context, ""new"", pluginInitArgs, block);
}", ,"// We must use IRubyObject#callMethod(...,""new"",...) here to continue supporting
[[SEP]]// mocking/validating from rspec.
","// We must use IRubyObject#callMethod(...,""new"",...) here to continue supporting// mocking/validating from rspec.",56,81,[0],0,"[0, 0]",0,[0],0,0,0,0,"initializePlugin(ThreadContext, RubyModule, ExecutionContextExt, RubyClass, IRubyObject[], Block)",org.logstash.plugins.factory.ContextualizerExt,"initializePlugin/6[org.logstash.plugins.factory.ThreadContext,org.logstash.plugins.factory.RubyModule,org.logstash.execution.ExecutionContextExt,org.logstash.plugins.factory.RubyClass,org.logstash.plugins.factory.IRubyObject[],org.logstash.plugins.factory.Block]",False,61,8,2,2,0,3,7,18,1,2,6,7,0,0,0,1,0,0,1,1,3,1,2,0,0,0,21,10,0,False
446,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\factory\ContextualizerExt.java,org.logstash.plugins.factory.ContextualizerExt,"IRubyObject initialize(ThreadContext, IRubyObject, IRubyObject[], Block)","// framed for invokeSuper
@JRubyMethod(name = ""initialize"", rest = true, frame = true)
public static IRubyObject initialize(final ThreadContext context, final IRubyObject recv, final IRubyObject[] args, final Block block) {
    final List<IRubyObject> argsList = new ArrayList<>(Arrays.asList(args));
    if (args.length > 0 && args[0] instanceof ExecutionContextExt) {
        final ExecutionContextExt executionContext = (ExecutionContextExt) argsList.remove(0);
        recv.getInstanceVariables().setInstanceVariable(EXECUTION_CONTEXT_IVAR_NAME, executionContext);
    }
    final IRubyObject[] restArgs = argsList.toArray(new IRubyObject[] {});
    return invokeSuper(context, recv, restArgs, block);
}", ,"// framed for invokeSuper
",// framed for invokeSuper,83,98,[0],0,[0],0,[0],0,0,0,0,"initialize(ThreadContext, IRubyObject, IRubyObject[], Block)",org.logstash.plugins.factory.ContextualizerExt,"initialize/4[org.logstash.plugins.factory.ThreadContext,org.logstash.plugins.factory.IRubyObject,org.logstash.plugins.factory.IRubyObject[],org.logstash.plugins.factory.Block]",False,87,5,0,0,0,3,6,9,1,3,4,6,0,0,0,0,0,0,1,3,3,0,1,0,0,0,19,9,0,False
447,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\factory\PluginFactoryExt.java,org.logstash.plugins.factory.PluginFactoryExt,"IRubyObject filterDelegator(ThreadContext, IRubyObject, IRubyObject...)","@JRubyMethod(name = ""filter_delegator"", meta = true, required = 5)
public static IRubyObject filterDelegator(final ThreadContext context, final IRubyObject recv, final IRubyObject... args) {
    // filterDelegatorClass, klass, rubyArgs, typeScopedMetric, executionCntx
    final RubyClass filterDelegatorClass = (RubyClass) args[0];
    final RubyClass klass = (RubyClass) args[1];
    final RubyHash arguments = (RubyHash) args[2];
    final AbstractMetricExt typeScopedMetric = (AbstractMetricExt) args[3];
    final ExecutionContextExt executionContext = (ExecutionContextExt) args[4];
    final IRubyObject filterInstance = ContextualizerExt.initializePlugin(context, executionContext, klass, arguments);
    final RubyString id = (RubyString) arguments.op_aref(context, ID_KEY);
    filterInstance.callMethod(context, ""metric="", typeScopedMetric.namespace(context, id.intern()));
    return filterDelegatorClass.newInstance(context, filterInstance, id, Block.NULL_BLOCK);
}", ,"// filterDelegatorClass, klass, rubyArgs, typeScopedMetric, executionCntx
","// filterDelegatorClass, klass, rubyArgs, typeScopedMetric, executionCntx",63,82,[0],0,[0],0,[0],0,0,0,0,"filterDelegator(ThreadContext, IRubyObject, IRubyObject[])",org.logstash.plugins.factory.PluginFactoryExt,"filterDelegator/3[org.logstash.config.ir.compiler.ThreadContext,org.logstash.config.ir.compiler.IRubyObject,org.logstash.config.ir.compiler.IRubyObject[]]",False,65,9,3,1,2,1,6,11,1,7,3,6,0,0,0,0,0,0,2,6,7,0,0,0,0,0,23,9,0,False
448,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\factory\PluginFactoryExt.java,org.logstash.plugins.factory.PluginFactoryExt,Codec buildDefaultCodec(String),"@Override
public Codec buildDefaultCodec(String codecName) {
    final IRubyObject pluginInstance = plugin(RubyUtil.RUBY.getCurrentContext(), PluginLookup.PluginType.CODEC, codecName, RubyHash.newHash(RubyUtil.RUBY), null);
    final Codec codec = (Codec) JavaUtil.unwrapJavaValue(pluginInstance);
    if (codec != null) {
        return codec;
    }
    // no unwrap is possible so this is a real Ruby instance
    return new RubyCodecDelegator(RubyUtil.RUBY.getCurrentContext(), pluginInstance);
}", ,"// no unwrap is possible so this is a real Ruby instance
",// no unwrap is possible so this is a real Ruby instance,167,183,[0],0,[0],0,[0],0,0,0,0,buildDefaultCodec(String),org.logstash.plugins.factory.PluginFactoryExt,buildDefaultCodec/1[java.lang.String],False,168,4,2,0,2,2,4,8,2,2,1,4,1,4,0,1,0,0,0,0,2,0,1,0,0,0,12,1,0,False
449,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\factory\PluginFactoryExt.java,org.logstash.plugins.factory.PluginFactoryExt,"Map<String, Object> convertToJavaCoercible(Map<String, Object>)","private Map<String, Object> convertToJavaCoercible(Map<String, Object> input) {
    final Map<String, Object> output = new HashMap<>(input);
    // Intercept Codecs
    for (final Map.Entry<String, Object> entry : input.entrySet()) {
        final String key = entry.getKey();
        final Object value = entry.getValue();
        if (value instanceof IRubyObject) {
            final Object unwrapped = JavaUtil.unwrapJavaValue((IRubyObject) value);
            if (unwrapped instanceof Codec) {
                output.put(key, unwrapped);
            }
        }
    }
    return output;
}", ,"// Intercept Codecs
",// Intercept Codecs,272,288,[0],0,[0],0,[0],0,0,0,0,"convertToJavaCoercible(Map<String, Object>)",org.logstash.plugins.factory.PluginFactoryExt,"convertToJavaCoercible/1[java.util.Map<java.lang.String,java.lang.Object>]",False,272,2,1,1,0,4,5,14,1,4,1,5,0,0,1,0,0,0,0,0,4,0,3,0,0,0,14,2,0,False
450,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\factory\PluginFactoryExt.java,org.logstash.plugins.factory.PluginFactoryExt,"String generateOrRetrievePluginId(PluginLookup.PluginType, SourceWithMetadata, Map<String, ?>)","// TODO: caller seems to think that the args is `Map<String, IRubyObject>`, but
// at least any `id` present is actually a `String`.
private String generateOrRetrievePluginId(final PluginLookup.PluginType type, final SourceWithMetadata source, final Map<String, ?> args) {
    final Optional<String> unprocessedId;
    if (source == null) {
        unprocessedId = extractId(() -> extractIdFromArgs(args), this::generateUUID);
    } else {
        unprocessedId = extractId(() -> extractIdFromLIR(source), () -> extractIdFromArgs(args), () -> generateUUIDForCodecs(type));
    }
    return unprocessedId.map(configVariables::expand).filter(String.class::isInstance).map(String.class::cast).orElse(null);
}","// at least any `id` present is actually a `String`.
", ,"// TODO: caller seems to think that the args is `Map<String, IRubyObject>`, but// at least any `id` present is actually a `String`.",292,310,[0],0,[0],0,[1],1,1,1,1,"generateOrRetrievePluginId(PluginType, SourceWithMetadata, Map<String, ?>)",org.logstash.plugins.factory.PluginFactoryExt,"generateOrRetrievePluginId/3[org.logstash.plugins.PluginLookup.PluginType,org.logstash.common.SourceWithMetadata,java.util.Map<java.lang.String,?>]",False,294,3,5,1,4,2,8,10,1,1,3,8,4,2,0,1,0,0,0,0,2,0,1,0,0,4,23,2,0,False
451,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\factory\RubyCodecDelegator.java,org.logstash.plugins.factory.RubyCodecDelegator,"void decode(ByteBuffer, Consumer<Map<String, Object>>)","@Override
public void decode(ByteBuffer buffer, Consumer<Map<String, Object>> eventConsumer) {
    // invoke Ruby's codec #decode(data, block) and use a Block to capture the yielded LogStash::Event to
    // back to Java and pass to the eventConsumer.
    if (buffer.remaining() == 0) {
        // no data to decode
        return;
    }
    // setup the block callback bridge to invoke eventConsumer
    final Block consumerWrapper = new Block(new JavaInternalBlockBody(currentContext.runtime, Signature.ONE_ARGUMENT) {

        @Override
        @SuppressWarnings(""unchecked"")
        public IRubyObject yield(ThreadContext context, IRubyObject[] args) {
            // Expect only one argument, the LogStash::Event instantiated by the Ruby codec
            final IRubyObject event = args[0];
            eventConsumer.accept(((JrubyEventExtLibrary.RubyEvent) event).getEvent().getData());
            return event;
        }
    });
    byte[] byteInput = new byte[buffer.remaining()];
    buffer.get(byteInput);
    final RubyString data = RubyUtil.RUBY.newString(new String(byteInput));
    IRubyObject[] methodParams = new IRubyObject[] { data };
    pluginInstance.callMethod(this.currentContext, ""decode"", methodParams, consumerWrapper);
}", ,"// invoke Ruby's codec #decode(data, block) and use a Block to capture the yielded LogStash::Event to
[[SEP]]// back to Java and pass to the eventConsumer.
[[SEP]]// no data to decode
[[SEP]]// setup the block callback bridge to invoke eventConsumer
[[SEP]]// Expect only one argument, the LogStash::Event instantiated by the Ruby codec
","// invoke Ruby's codec #decode(data, block) and use a Block to capture the yielded LogStash::Event to// back to Java and pass to the eventConsumer.[[SEP]]// no data to decode[[SEP]]// setup the block callback bridge to invoke eventConsumer[[SEP]]// Expect only one argument, the LogStash::Event instantiated by the Ruby codec",65,91,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,"decode(ByteBuffer, Consumer<Map<String, Object>>)",org.logstash.plugins.factory.RubyCodecDelegator,"decode/2[java.nio.ByteBuffer,java.util.function.Consumer<java.util.Map<java.lang.String,java.lang.Object>>]",False,66,5,1,1,0,2,4,18,1,4,2,4,0,0,0,1,0,0,1,1,4,0,1,1,0,0,24,1,0,False
452,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\factory\RubyCodecDelegator.java,org.logstash.plugins.factory.RubyCodecDelegator,"void encode(Event, OutputStream)","@Override
@SuppressWarnings({ ""uncheked"", ""rawtypes"" })
public void encode(Event event, OutputStream output) throws IOException {
    // convert co.elastic.logstash.api.Event to JrubyEventExtLibrary.RubyEvent
    if (!(event instanceof org.logstash.Event)) {
        throw new IllegalStateException(""The object to encode must be of type org.logstash.Event"");
    }
    final JrubyEventExtLibrary.RubyEvent rubyEvent = JrubyEventExtLibrary.RubyEvent.newRubyEvent(currentContext.runtime, (org.logstash.Event) event);
    final RubyArray param = RubyArray.newArray(currentContext.runtime, rubyEvent);
    final RubyArray encoded = (RubyArray) pluginInstance.callMethod(this.currentContext, ""multi_encode"", param);
    // method return an nested array, the outer contains just one element
    // while the inner contains the original event and encoded event in form of String
    final RubyString result = ((RubyArray) encoded.eltInternal(0)).eltInternal(1).convertToString();
    output.write(result.getByteList().getUnsafeBytes(), result.getByteList().getBegin(), result.getByteList().getRealSize());
}", ,"// method return an nested array, the outer contains just one element
[[SEP]]// convert co.elastic.logstash.api.Event to JrubyEventExtLibrary.RubyEvent
[[SEP]]// while the inner contains the original event and encoded event in form of String
","// convert co.elastic.logstash.api.Event to JrubyEventExtLibrary.RubyEvent[[SEP]]// method return an nested array, the outer contains just one element// while the inner contains the original event and encoded event in form of String",98,114,[0],0,"[0, 0, 0]",0,"[0, 0]",0,0,0,0,"encode(Event, OutputStream)",org.logstash.plugins.factory.RubyCodecDelegator,"encode/2[co.elastic.logstash.api.Event,java.io.OutputStream]",False,100,5,1,0,1,2,10,10,0,4,2,10,0,0,0,0,0,2,4,2,4,0,1,0,0,0,22,1,0,False
453,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\factory\RubyCodecDelegator.java,org.logstash.plugins.factory.RubyCodecDelegator,Collection<PluginConfigSpec<?>> configSchema(),"@Override
public Collection<PluginConfigSpec<?>> configSchema() {
    // this method is invoked only for real java codecs, the one that are configured
    // in pipeline config that needs configuration validation. In this case the validation
    // is already done on the Ruby codec.
    return null;
}", ,"// this method is invoked only for real java codecs, the one that are configured
[[SEP]]// in pipeline config that needs configuration validation. In this case the validation
[[SEP]]// is already done on the Ruby codec.
","// this method is invoked only for real java codecs, the one that are configured// in pipeline config that needs configuration validation. In this case the validation// is already done on the Ruby codec.",121,127,[0],0,"[0, 0, 0]",0,[0],0,0,0,0,configSchema(),org.logstash.plugins.factory.RubyCodecDelegator,configSchema/0,False,122,1,0,0,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,1,0,False
454,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\inputs\Generator.java,org.logstash.plugins.inputs.Generator,"void startUnthrottledGenerator(Consumer<Map<String, Object>>)","private void startUnthrottledGenerator(Consumer<Map<String, Object>> writer) {
    sequence = new long[(int) threads];
    events = new ArrayList<>();
    linesIndex = new int[(int) threads];
    for (int k = 0; k < threads; k++) {
        Map<String, Object> event = new HashMap<>();
        event.put(""hostname"", hostname);
        event.put(""thread_number"", k);
        events.add(event);
        if (k > 0) {
            final int finalK = k;
            Thread t = new Thread(() -> {
                while (runGenerator(writer, finalK, () -> countDownLatch.countDown())) {
                }
            });
            t.setName(""generator_"" + getId() + ""_"" + k);
            t.start();
        }
    }
    // run first generator on this thread
    while (runGenerator(writer, 0, () -> countDownLatch.countDown())) {
    }
}", ,"// run first generator on this thread
",// run first generator on this thread,122,145,[0],0,[0],0,[0],0,0,0,0,"startUnthrottledGenerator(Consumer<Map<String, Object>>)",org.logstash.plugins.inputs.Generator,"startUnthrottledGenerator/1[java.util.function.Consumer<java.util.Map<java.lang.String,java.lang.Object>>]",False,122,3,3,1,2,5,7,23,0,4,1,7,2,1,3,0,0,0,4,3,7,1,4,0,0,3,18,2,0,False
455,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\inputs\Generator.java,org.logstash.plugins.inputs.Generator,"void startThrottledGenerator(Consumer<Map<String, Object>>)","private void startThrottledGenerator(Consumer<Map<String, Object>> writer) {
    ScheduledExecutorService ses = Executors.newScheduledThreadPool((int) threads);
    int delayMilli = (int) (1000.0 / eps);
    sequence = new long[(int) threads];
    futures = new ScheduledFuture<?>[(int) threads];
    events = new ArrayList<>();
    linesIndex = new int[(int) threads];
    for (int k = 0; k < threads; k++) {
        Map<String, Object> event = new HashMap<>();
        event.put(""hostname"", hostname);
        event.put(""thread_number"", k);
        events.add(event);
        final int finalk = k;
        futures[k] = ses.scheduleAtFixedRate(() -> runGenerator(writer, finalk, () -> {
            countDownLatch.countDown();
            futures[finalk].cancel(false);
        }), 0, delayMilli, TimeUnit.MILLISECONDS);
    }
    boolean finished = false;
    while (!stopRequested && !finished) {
        try {
            Thread.sleep(1000);
            boolean allCancelled = true;
            for (int k = 0; k < threads; k++) {
                allCancelled = allCancelled && futures[k].isCancelled();
            }
            if (allCancelled) {
                finished = true;
                ses.shutdownNow();
            }
        } catch (InterruptedException ex) {
            // do nothing
        }
    }
}", ,"// do nothing
",// do nothing,147,182,[0],0,[0],0,[0],0,0,0,0,"startThrottledGenerator(Consumer<Map<String, Object>>)",org.logstash.plugins.inputs.Generator,"startThrottledGenerator/1[java.util.function.Consumer<java.util.Map<java.lang.String,java.lang.Object>>]",False,147,3,2,1,1,7,10,36,0,8,1,10,1,1,3,0,1,1,2,5,15,1,3,0,0,2,31,2,0,False
456,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\inputs\Stdin.java,org.logstash.plugins.inputs.Stdin,"void start(Consumer<Map<String, Object>>)","@Override
public void start(Consumer<Map<String, Object>> writer) {
    this.writer = writer;
    final ByteBuffer buffer = ByteBuffer.allocateDirect(BUFFER_SIZE);
    try {
        while (!stopRequested && (input.read(buffer) > -1)) {
            buffer.flip();
            codec.decode(buffer, this);
            buffer.compact();
        }
    } catch (AsynchronousCloseException e2) {
        // do nothing -- this happens when stop is called while the read loop is blocked on input.read()
    } catch (IOException e) {
        stopRequested = true;
        logger.error(""Stopping stdin after read error"", e);
        throw new IllegalStateException(e);
    } finally {
        try {
            input.close();
        } catch (IOException e) {
            // do nothing
        }
        buffer.flip();
        codec.flush(buffer, this);
        isStopped.countDown();
    }
}", ,"// do nothing -- this happens when stop is called while the read loop is blocked on input.read()
[[SEP]]// do nothing
",// do nothing -- this happens when stop is called while the read loop is blocked on input.read()[[SEP]]// do nothing,93,120,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,"start(Consumer<Map<String, Object>>)",org.logstash.plugins.inputs.Stdin,"start/1[java.util.function.Consumer<java.util.Map<java.lang.String,java.lang.Object>>]",False,94,2,3,0,3,6,9,28,0,1,1,9,0,0,1,0,2,1,1,1,3,0,2,0,0,0,28,1,1,False
457,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\inputs\Stdin.java,org.logstash.plugins.inputs.Stdin,void stop(),"@Override
public void stop() {
    stopRequested = true;
    try {
        // interrupts any pending reads
        input.close();
    } catch (IOException e) {
        // do nothing
    }
}", ,"// interrupts any pending reads
[[SEP]]// do nothing
",// interrupts any pending reads[[SEP]]// do nothing,128,136,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,stop(),org.logstash.plugins.inputs.Stdin,stop/0,False,129,0,0,0,0,2,1,8,0,0,0,1,0,0,0,0,1,0,0,0,1,0,1,0,0,0,7,1,0,False
458,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\AddressState.java,org.logstash.plugins.pipeline.AddressState,boolean addOutput(PipelineOutput),"/**
 * Add the given output and ensure associated input's receivers are updated
 * @param output output to be added
 * @return true if the output was not already added
 */
public boolean addOutput(PipelineOutput output) {
    return outputs.add(output);
}","/**
 * Add the given output and ensure associated input's receivers are updated
 * @param output output to be added
 * @return true if the output was not already added
 */
", ,/** * Add the given output and ensure associated input's receivers are updated * @param output output to be added * @return true if the output was not already added */,39,41,[0],0,[0],0,[0],0,0,0,0,addOutput(PipelineOutput),org.logstash.plugins.pipeline.AddressState,addOutput/1[org.logstash.plugins.pipeline.PipelineOutput],False,39,1,1,1,0,1,1,3,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,20,1,0,True
459,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\AddressState.java,org.logstash.plugins.pipeline.AddressState,boolean assignInputIfMissing(PipelineInput),"/**
 * Assigns an input to listen on this address. Will return false if another input is already listening.
 * @param newInput input to assign as listener
 * @return true if successful, false if another input is listening
 */
public synchronized boolean assignInputIfMissing(PipelineInput newInput) {
    // We aren't changing anything
    if (input == null) {
        input = newInput;
        return true;
    } else {
        return input == newInput;
    }
}","/**
 * Assigns an input to listen on this address. Will return false if another input is already listening.
 * @param newInput input to assign as listener
 * @return true if successful, false if another input is listening
 */
","// We aren't changing anything
","/** * Assigns an input to listen on this address. Will return false if another input is already listening. * @param newInput input to assign as listener * @return true if successful, false if another input is listening */[[SEP]]// We aren't changing anything",56,64,[0],0,[0],0,"[0, 0]",0,0,0,0,assignInputIfMissing(PipelineInput),org.logstash.plugins.pipeline.AddressState,assignInputIfMissing/1[org.logstash.plugins.pipeline.PipelineInput],False,56,1,1,1,0,2,0,9,2,0,1,0,0,0,0,2,0,0,0,0,1,0,1,0,0,0,22,33,0,True
460,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\AddressState.java,org.logstash.plugins.pipeline.AddressState,boolean unassignInput(PipelineInput),"/**
 * Unsubscribes the given input from this address
 * @param unsubscribingInput input to unsubscribe from this address
 * @return true if this input was listening, false otherwise
 */
public synchronized boolean unassignInput(PipelineInput unsubscribingInput) {
    if (input != unsubscribingInput)
        return false;
    input = null;
    return true;
}","/**
 * Unsubscribes the given input from this address
 * @param unsubscribingInput input to unsubscribe from this address
 * @return true if this input was listening, false otherwise
 */
", ,"/** * Unsubscribes the given input from this address * @param unsubscribingInput input to unsubscribe from this address * @return true if this input was listening, false otherwise */",71,76,[0],0,[0],0,[0],0,0,0,0,unassignInput(PipelineInput),org.logstash.plugins.pipeline.AddressState,unassignInput/1[org.logstash.plugins.pipeline.PipelineInput],False,71,1,2,2,0,2,0,5,2,0,1,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,17,33,0,True
461,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\AddressState.java,org.logstash.plugins.pipeline.AddressState,boolean hasOutput(PipelineOutput),"// Just for tests
boolean hasOutput(PipelineOutput output) {
    return outputs.contains(output);
}","// Just for tests
", ,// Just for tests,87,89,[0],0,[0],0,[0],0,0,0,0,hasOutput(PipelineOutput),org.logstash.plugins.pipeline.AddressState,hasOutput/1[org.logstash.plugins.pipeline.PipelineOutput],False,87,1,2,2,0,1,1,3,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,False
462,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\PipelineBus.java,org.logstash.plugins.pipeline.PipelineBus,"void sendEvents(PipelineOutput, Collection<JrubyEventExtLibrary.RubyEvent>, boolean)","/**
 * Sends events from the provided output.
 *
 * @param sender         The output sending the events.
 * @param events         A collection of JRuby events
 * @param ensureDelivery set to true if you want this to retry indefinitely in the event an event send fails
 */
public void sendEvents(final PipelineOutput sender, final Collection<JrubyEventExtLibrary.RubyEvent> events, final boolean ensureDelivery) {
    // This can happen on pipeline shutdown or in some other situations
    if (events.isEmpty())
        return;
    synchronized (sender) {
        final ConcurrentHashMap<String, AddressState> addressesToInputs = outputsToAddressStates.get(sender);
        // In case of retry on the same set events, a stable order is needed, else
        // the risk is to reprocess twice some events. Collection can't guarantee order stability.
        JrubyEventExtLibrary.RubyEvent[] orderedEvents = events.toArray(new JrubyEventExtLibrary.RubyEvent[0]);
        addressesToInputs.forEach((address, addressState) -> {
            boolean sendWasSuccess = false;
            ReceiveResponse lastResponse = null;
            boolean partialProcessing;
            int lastFailedPosition = 0;
            do {
                Stream<JrubyEventExtLibrary.RubyEvent> clones = Arrays.stream(orderedEvents).skip(lastFailedPosition).map(e -> e.rubyClone(RubyUtil.RUBY));
                // Save on calls to getInput since it's volatile
                PipelineInput input = addressState.getInput();
                if (input != null) {
                    lastResponse = input.internalReceive(clones);
                    sendWasSuccess = lastResponse.wasSuccess();
                }
                partialProcessing = ensureDelivery && !sendWasSuccess;
                if (partialProcessing) {
                    if (lastResponse != null && lastResponse.getStatus() == PipelineInput.ReceiveStatus.FAIL) {
                        // when last call to internalReceive generated a fail, restart from the
                        // fail position to avoid reprocessing of some events in the downstream.
                        lastFailedPosition = lastResponse.getSequencePosition();
                        logger.warn(""Attempted to send event to '{}' but that address reached error condition. "" + ""Will Retry. Root cause {}"", address, lastResponse.getCauseMessage());
                    } else {
                        logger.warn(""Attempted to send event to '{}' but that address was unavailable. "" + ""Maybe the destination pipeline is down or stopping? Will Retry."", address);
                    }
                    try {
                        Thread.sleep(1000);
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                        logger.error(""Sleep unexpectedly interrupted in bus retry loop"", e);
                    }
                }
            } while (partialProcessing);
        });
    }
}","/**
 * Sends events from the provided output.
 *
 * @param sender         The output sending the events.
 * @param events         A collection of JRuby events
 * @param ensureDelivery set to true if you want this to retry indefinitely in the event an event send fails
 */
","// This can happen on pipeline shutdown or in some other situations
[[SEP]]// In case of retry on the same set events, a stable order is needed, else
[[SEP]]// the risk is to reprocess twice some events. Collection can't guarantee order stability.
[[SEP]]// Save on calls to getInput since it's volatile
[[SEP]]// when last call to internalReceive generated a fail, restart from the
[[SEP]]// fail position to avoid reprocessing of some events in the downstream.
","/** * Sends events from the provided output. * * @param sender         The output sending the events. * @param events         A collection of JRuby events * @param ensureDelivery set to true if you want this to retry indefinitely in the event an event send fails */[[SEP]]// This can happen on pipeline shutdown or in some other situations[[SEP]]// In case of retry on the same set events, a stable order is needed, else// the risk is to reprocess twice some events. Collection can't guarantee order stability.[[SEP]]// Save on calls to getInput since it's volatile[[SEP]]// when last call to internalReceive generated a fail, restart from the// fail position to avoid reprocessing of some events in the downstream.",53,104,[0],0,"[0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,"sendEvents(PipelineOutput, Collection<RubyEvent>, boolean)",org.logstash.plugins.pipeline.PipelineBus,"sendEvents/3[org.logstash.plugins.pipeline.PipelineOutput,java.util.Collection<org.logstash.ext.JrubyEventExtLibrary.RubyEvent>,boolean]",False,55,6,13,3,10,8,20,40,1,11,3,20,0,0,1,3,1,0,5,3,11,2,5,0,0,2,69,1,3,True
463,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\PipelineBus.java,org.logstash.plugins.pipeline.PipelineBus,"void registerSender(PipelineOutput, Iterable<String>)","/**
 * Should be called by an output on register
 *
 * @param output    output to be registered
 * @param addresses collection of addresses on which to register this sender
 */
public void registerSender(final PipelineOutput output, final Iterable<String> addresses) {
    synchronized (output) {
        addresses.forEach((String address) -> {
            addressStates.compute(address, (k, value) -> {
                final AddressState state = value != null ? value : new AddressState();
                state.addOutput(output);
                return state;
            });
        });
        updateOutputReceivers(output);
    }
}","/**
 * Should be called by an output on register
 *
 * @param output    output to be registered
 * @param addresses collection of addresses on which to register this sender
 */
", ,/** * Should be called by an output on register * * @param output    output to be registered * @param addresses collection of addresses on which to register this sender */,112,125,[0],0,[0],0,[0],0,0,0,0,"registerSender(PipelineOutput, Iterable<String>)",org.logstash.plugins.pipeline.PipelineBus,"registerSender/2[org.logstash.plugins.pipeline.PipelineOutput,java.lang.Iterable<java.lang.String>]",False,112,3,13,10,3,2,4,14,1,3,2,4,1,1,0,1,0,0,0,0,1,0,3,0,0,2,26,1,0,True
464,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\PipelineBus.java,org.logstash.plugins.pipeline.PipelineBus,"void unregisterSender(PipelineOutput, Iterable<String>)","/**
 * Should be called by an output on close
 *
 * @param output    output that will be unregistered
 * @param addresses collection of addresses this sender was registered with
 */
public void unregisterSender(final PipelineOutput output, final Iterable<String> addresses) {
    synchronized (output) {
        addresses.forEach(address -> {
            addressStates.computeIfPresent(address, (k, state) -> {
                state.removeOutput(output);
                if (state.isEmpty())
                    return null;
                return state;
            });
        });
        outputsToAddressStates.remove(output);
    }
}","/**
 * Should be called by an output on close
 *
 * @param output    output that will be unregistered
 * @param addresses collection of addresses this sender was registered with
 */
", ,/** * Should be called by an output on close * * @param output    output that will be unregistered * @param addresses collection of addresses this sender was registered with */,133,147,[0],0,[0],0,[0],0,0,0,0,"unregisterSender(PipelineOutput, Iterable<String>)",org.logstash.plugins.pipeline.PipelineBus,"unregisterSender/2[org.logstash.plugins.pipeline.PipelineOutput,java.lang.Iterable<java.lang.String>]",False,133,2,8,6,2,2,5,14,2,3,2,5,0,0,0,0,0,0,0,0,0,0,4,0,0,2,25,1,0,True
465,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\PipelineBus.java,org.logstash.plugins.pipeline.PipelineBus,void updateOutputReceivers(PipelineOutput),"/**
 * Updates the internal state for this output to reflect the fact that there may be a change
 * in the inputs receiving events from it.
 *
 * @param output output to update
 */
private void updateOutputReceivers(final PipelineOutput output) {
    outputsToAddressStates.compute(output, (k, value) -> {
        ConcurrentHashMap<String, AddressState> outputAddressToInputMapping = value != null ? value : new ConcurrentHashMap<>();
        addressStates.forEach((address, state) -> {
            if (state.hasOutput(output))
                outputAddressToInputMapping.put(address, state);
        });
        return outputAddressToInputMapping;
    });
}","/**
 * Updates the internal state for this output to reflect the fact that there may be a change
 * in the inputs receiving events from it.
 *
 * @param output output to update
 */
", ,/** * Updates the internal state for this output to reflect the fact that there may be a change * in the inputs receiving events from it. * * @param output output to update */,155,165,[0],0,[0],0,[0],0,0,0,0,updateOutputReceivers(PipelineOutput),org.logstash.plugins.pipeline.PipelineBus,updateOutputReceivers/1[org.logstash.plugins.pipeline.PipelineOutput],False,155,2,2,1,1,3,4,11,1,5,1,4,0,0,0,1,0,0,0,0,1,0,3,0,0,2,29,2,0,True
466,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\PipelineBus.java,org.logstash.plugins.pipeline.PipelineBus,"boolean listen(PipelineInput, String)","/**
 * Listens to a given address with the provided listener
 * Only one listener can listen on an address at a time
 *
 * @param input   Input to register as listener
 * @param address Address on which to listen
 * @return true if the listener successfully subscribed
 */
public boolean listen(final PipelineInput input, final String address) {
    synchronized (input) {
        final boolean[] result = new boolean[1];
        addressStates.compute(address, (k, value) -> {
            AddressState state = value != null ? value : new AddressState();
            if (state.assignInputIfMissing(input)) {
                state.getOutputs().forEach(this::updateOutputReceivers);
                result[0] = true;
            } else {
                result[0] = false;
            }
            return state;
        });
        return result[0];
    }
}","/**
 * Listens to a given address with the provided listener
 * Only one listener can listen on an address at a time
 *
 * @param input   Input to register as listener
 * @param address Address on which to listen
 * @return true if the listener successfully subscribed
 */
", ,/** * Listens to a given address with the provided listener * Only one listener can listen on an address at a time * * @param input   Input to register as listener * @param address Address on which to listen * @return true if the listener successfully subscribed */,175,194,[0],0,[0],0,[0],0,0,0,0,"listen(PipelineInput, String)",org.logstash.plugins.pipeline.PipelineBus,"listen/2[org.logstash.plugins.pipeline.PipelineInput,java.lang.String]",False,175,3,12,9,3,3,4,18,2,4,2,4,0,0,0,1,0,0,0,4,4,0,3,0,0,1,33,1,0,True
467,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\PipelineBus.java,org.logstash.plugins.pipeline.PipelineBus,"void unlisten(PipelineInput, String)","/**
 * Stop listening on the given address with the given listener
 * Will change behavior depending on whether {@link #isBlockOnUnlisten()} is true or not.
 * Will call a blocking method if it is, a non-blocking one if it isn't
 *
 * @param input   Input that should stop listening
 * @param address Address on which the input should stop listening
 * @throws InterruptedException if interrupted while attempting to stop listening
 */
public void unlisten(final PipelineInput input, final String address) throws InterruptedException {
    synchronized (input) {
        if (isBlockOnUnlisten()) {
            unlistenBlock(input, address);
        } else {
            unlistenNonblock(input, address);
        }
    }
}","/**
 * Stop listening on the given address with the given listener
 * Will change behavior depending on whether {@link #isBlockOnUnlisten()} is true or not.
 * Will call a blocking method if it is, a non-blocking one if it isn't
 *
 * @param input   Input that should stop listening
 * @param address Address on which the input should stop listening
 * @throws InterruptedException if interrupted while attempting to stop listening
 */
", ,"/** * Stop listening on the given address with the given listener * Will change behavior depending on whether {@link #isBlockOnUnlisten()} is true or not. * Will call a blocking method if it is, a non-blocking one if it isn't * * @param input   Input that should stop listening * @param address Address on which the input should stop listening * @throws InterruptedException if interrupted while attempting to stop listening */",205,213,[0],0,[0],0,[0],0,0,0,0,"unlisten(PipelineInput, String)",org.logstash.plugins.pipeline.PipelineBus,"unlisten/2[org.logstash.plugins.pipeline.PipelineInput,java.lang.String]",False,205,2,8,5,3,2,3,10,0,0,2,3,3,1,0,0,0,0,0,0,0,0,2,0,0,0,41,1,0,True
468,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\PipelineBus.java,org.logstash.plugins.pipeline.PipelineBus,"void unlistenBlock(PipelineInput, String)","/**
 * Stop listening on the given address with the given listener. Blocks until upstream outputs have
 * stopped.
 *
 * @param input Input that should stop listening
 * @param address Address on which to stop listening
 * @throws InterruptedException if interrupted while attempting to stop listening
 */
private void unlistenBlock(final PipelineInput input, final String address) throws InterruptedException {
    final boolean[] waiting = { true };
    // Block until all senders are done
    // Outputs shutdown before their connected inputs
    while (true) {
        addressStates.compute(address, (k, state) -> {
            // If this happens the pipeline was asked to shutdown
            // twice, so there's no work to do
            if (state == null) {
                waiting[0] = false;
                return null;
            }
            if (state.getOutputs().isEmpty()) {
                state.unassignInput(input);
                waiting[0] = false;
                return null;
            }
            return state;
        });
        if (!waiting[0]) {
            break;
        } else {
            Thread.sleep(100);
        }
    }
}","/**
 * Stop listening on the given address with the given listener. Blocks until upstream outputs have
 * stopped.
 *
 * @param input Input that should stop listening
 * @param address Address on which to stop listening
 * @throws InterruptedException if interrupted while attempting to stop listening
 */
","// Block until all senders are done
[[SEP]]// Outputs shutdown before their connected inputs
[[SEP]]// If this happens the pipeline was asked to shutdown
[[SEP]]// twice, so there's no work to do
","/** * Stop listening on the given address with the given listener. Blocks until upstream outputs have * stopped. * * @param input Input that should stop listening * @param address Address on which to stop listening * @throws InterruptedException if interrupted while attempting to stop listening */[[SEP]]// Block until all senders are done// Outputs shutdown before their connected inputs[[SEP]]// If this happens the pipeline was asked to shutdown// twice, so there's no work to do",223,253,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,"unlistenBlock(PipelineInput, String)",org.logstash.plugins.pipeline.PipelineBus,"unlistenBlock/2[org.logstash.plugins.pipeline.PipelineInput,java.lang.String]",False,223,3,3,1,2,5,5,24,3,3,2,5,0,0,1,1,0,0,0,4,3,0,3,0,0,1,32,2,0,True
469,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\PipelineBus.java,org.logstash.plugins.pipeline.PipelineBus,"void unlistenNonblock(PipelineInput, String)","/**
 * Unlisten to use during reloads. This lets upstream outputs block while this input is missing
 *
 * @param input   Input that should stop listening
 * @param address Address on which to stop listening
 */
@VisibleForTesting
void unlistenNonblock(final PipelineInput input, final String address) {
    addressStates.computeIfPresent(address, (k, state) -> {
        state.unassignInput(input);
        state.getOutputs().forEach(this::updateOutputReceivers);
        return state.isEmpty() ? null : state;
    });
}","/**
 * Unlisten to use during reloads. This lets upstream outputs block while this input is missing
 *
 * @param input   Input that should stop listening
 * @param address Address on which to stop listening
 */
", ,/** * Unlisten to use during reloads. This lets upstream outputs block while this input is missing * * @param input   Input that should stop listening * @param address Address on which to stop listening */,261,268,[0],0,[0],0,[0],0,0,1,0,"unlistenNonblock(PipelineInput, String)",org.logstash.plugins.pipeline.PipelineBus,"unlistenNonblock/2[org.logstash.plugins.pipeline.PipelineInput,java.lang.String]",False,262,3,5,2,3,2,5,8,1,2,2,5,0,0,0,0,0,0,0,0,0,0,1,0,0,1,26,0,0,True
470,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\PipelineInput.java,org.logstash.plugins.pipeline.PipelineInput,ReceiveResponse internalReceive(Stream<JrubyEventExtLibrary.RubyEvent>),"/**
 * Accepts an event. It might be rejected if the input is stopping.
 *
 * @param events a collection of events
 * @return response instance which contains the status of the execution, if events were successfully received
 *      or reached an error or the input was closing.
 */
ReceiveResponse internalReceive(Stream<JrubyEventExtLibrary.RubyEvent> events);","/**
 * Accepts an event. It might be rejected if the input is stopping.
 *
 * @param events a collection of events
 * @return response instance which contains the status of the execution, if events were successfully received
 *      or reached an error or the input was closing.
 */
", ,"/** * Accepts an event. It might be rejected if the input is stopping. * * @param events a collection of events * @return response instance which contains the status of the execution, if events were successfully received *      or reached an error or the input was closing. */",41,41,[0],0,[0],0,[0],0,0,0,0,internalReceive(Stream<RubyEvent>),org.logstash.plugins.pipeline.PipelineInput,internalReceive/1[java.util.stream.Stream<org.logstash.ext.JrubyEventExtLibrary.RubyEvent>],False,34,2,2,2,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,28,0,0,True
471,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\plugins\pipeline\PipelineInput.java,org.logstash.plugins.pipeline.PipelineInput,boolean isRunning(),"/**
 * @return true if the input is running
 */
boolean isRunning();","/**
 * @return true if the input is running
 */
", ,/** * @return true if the input is running */,46,46,[0],0,[0],0,[0],0,0,0,0,isRunning(),org.logstash.plugins.pipeline.PipelineInput,isRunning/0,False,43,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,True
472,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\SecretIdentifier.java,org.logstash.secret.SecretIdentifier,SecretIdentifier fromExternalForm(String),"/**
 * Converts an external URN format to a {@link SecretIdentifier} object.
 *
 * @param urn The {@link String} formatted identifier obtained originally from {@link SecretIdentifier#toExternalForm()}
 * @return The {@link SecretIdentifier} object used to identify secrets, null if not valid external form.
 */
public static SecretIdentifier fromExternalForm(String urn) {
    if (urn == null || !urnPattern.matcher(urn).matches()) {
        throw new IllegalArgumentException(""Invalid external form "" + urn);
    }
    String[] parts = colonPattern.split(urn, 5);
    return new SecretIdentifier(validateWithTransform(parts[4], ""key""));
}","/**
 * Converts an external URN format to a {@link SecretIdentifier} object.
 *
 * @param urn The {@link String} formatted identifier obtained originally from {@link SecretIdentifier#toExternalForm()}
 * @return The {@link SecretIdentifier} object used to identify secrets, null if not valid external form.
 */
", ,"/** * Converts an external URN format to a {@link SecretIdentifier} object. * * @param urn The {@link String} formatted identifier obtained originally from {@link SecretIdentifier#toExternalForm()} * @return The {@link SecretIdentifier} object used to identify secrets, null if not valid external form. */",52,58,[0],0,[0],0,[0],0,0,0,0,fromExternalForm(String),org.logstash.secret.SecretIdentifier,fromExternalForm/1[java.lang.String],False,52,1,5,3,2,3,4,7,1,1,1,4,1,1,0,1,0,0,2,2,1,1,1,0,0,0,34,9,0,True
473,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\SecretIdentifier.java,org.logstash.secret.SecretIdentifier,"String validateWithTransform(String, String)","/**
 * Minor validation and downcases the parts
 *
 * @param part     The part of the URN to validate
 * @param partName The name of the part used for logging.
 * @return The validated and transformed part.
 */
private static String validateWithTransform(String part, String partName) {
    if (part == null || part.isEmpty()) {
        throw new IllegalArgumentException(String.format(""%s may not be null or empty"", partName));
    }
    return part.toLowerCase(Locale.US);
}","/**
 * Minor validation and downcases the parts
 *
 * @param part     The part of the URN to validate
 * @param partName The name of the part used for logging.
 * @return The validated and transformed part.
 */
", ,/** * Minor validation and downcases the parts * * @param part     The part of the URN to validate * @param partName The name of the part used for logging. * @return The validated and transformed part. */,67,72,[0],0,[0],0,[0],0,0,0,0,"validateWithTransform(String, String)",org.logstash.secret.SecretIdentifier,"validateWithTransform/2[java.lang.String,java.lang.String]",False,67,0,2,2,0,3,3,6,1,0,2,3,0,0,0,1,0,0,1,0,0,0,1,0,0,0,29,10,0,True
474,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\SecretIdentifier.java,org.logstash.secret.SecretIdentifier,String getKey(),"/**
 * Get the key (unique part) of the identifier
 *
 * @return the unique part of the identifier
 */
public String getKey() {
    return key;
}","/**
 * Get the key (unique part) of the identifier
 *
 * @return the unique part of the identifier
 */
", ,/** * Get the key (unique part) of the identifier * * @return the unique part of the identifier */,90,92,[0],0,[0],0,[0],0,0,0,0,getKey(),org.logstash.secret.SecretIdentifier,getKey/0,False,90,0,13,13,0,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,1,0,True
475,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\SecretIdentifier.java,org.logstash.secret.SecretIdentifier,String toExternalForm(),"/**
 * Converts this object to a format acceptable external {@link String} format. Note - no guarantees are made with respect to encoding or safe use. For example, the external
 * format may not be URL safely encoded.
 *
 * @return the externally formatted {@link String}
 */
public String toExternalForm() {
    StringBuilder sb = new StringBuilder(100);
    sb.append(""urn"").append("":"");
    sb.append(""logstash"").append("":"");
    sb.append(""secret"").append("":"");
    sb.append(VERSION).append("":"");
    sb.append(this.key);
    return sb.toString();
}","/**
 * Converts this object to a format acceptable external {@link String} format. Note - no guarantees are made with respect to encoding or safe use. For example, the external
 * format may not be URL safely encoded.
 *
 * @return the externally formatted {@link String}
 */
", ,"/** * Converts this object to a format acceptable external {@link String} format. Note - no guarantees are made with respect to encoding or safe use. For example, the external * format may not be URL safely encoded. * * @return the externally formatted {@link String} */",107,115,[0],0,[0],0,[0],0,0,0,0,toExternalForm(),org.logstash.secret.SecretIdentifier,toExternalForm/0,False,107,0,12,12,0,1,2,9,1,1,0,2,0,0,0,0,0,0,7,1,1,0,0,0,0,0,40,1,0,True
476,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\SecretVariable.java,org.logstash.secret.SecretVariable,String getValue(),"// Ruby code compatibility, value attribute
public String getValue() {
    return getSecretValue();
}","// Ruby code compatibility, value attribute
", ,"// Ruby code compatibility, value attribute",28,30,[0],0,[0],0,[0],0,0,0,0,getValue(),org.logstash.secret.SecretVariable,getValue/0,False,28,1,1,0,1,1,1,3,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,3,1,0,False
477,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\SecretVariable.java,org.logstash.secret.SecretVariable,String inspect(),"// Ruby code compatibility, inspect method
public String inspect() {
    return toString();
}","// Ruby code compatibility, inspect method
", ,"// Ruby code compatibility, inspect method",33,35,[0],0,[0],0,[0],0,0,0,0,inspect(),org.logstash.secret.SecretVariable,inspect/0,False,33,1,1,0,1,1,1,3,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,3,1,0,False
478,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\cli\SecretStoreCli.java,org.logstash.secret.cli.SecretStoreCli,"void command(String, SecureConfig, String)","/**
 * Entry point to issue a command line command.
 * @param primaryCommand The string representation of a {@link SecretStoreCli.Command}, if the String does not map to a {@link SecretStoreCli.Command}, then it will show the help menu.
 * @param config The configuration needed to work a secret store. May be null for help.
 * @param argument This can be either the identifier for a secret, or a sub command like --help. May be null.
 */
public void command(String primaryCommand, SecureConfig config, String argument) {
    terminal.writeLine("""");
    final Command command = Command.fromString(primaryCommand).orElse(Command.HELP);
    final Optional<Command> sub = Command.fromString(argument);
    boolean help = Command.HELP.equals(sub.orElse(null));
    switch(command) {
        case CREATE:
            {
                if (help) {
                    terminal.writeLine(""Creates a new keystore. For example: 'bin/logstash-keystore create'"");
                    return;
                }
                if (secretStoreFactory.exists(config.clone())) {
                    terminal.write(""An Logstash keystore already exists. Overwrite ? [y/N] "");
                    if (isYes(terminal.readLine())) {
                        create(config);
                    }
                } else {
                    create(config);
                }
                break;
            }
        case LIST:
            {
                if (help) {
                    terminal.writeLine(""List all secret identifiers from the keystore. For example: "" + ""`bin/logstash-keystore list`. Note - only the identifiers will be listed, not the secrets."");
                    return;
                }
                Collection<SecretIdentifier> ids = secretStoreFactory.load(config).list();
                List<String> keys = ids.stream().filter(id -> !id.equals(LOGSTASH_MARKER)).map(id -> id.getKey()).collect(Collectors.toList());
                Collections.sort(keys);
                keys.forEach(terminal::writeLine);
                break;
            }
        case ADD:
            {
                if (help) {
                    terminal.writeLine(""Adds a new secret to the keystore. For example: "" + ""`bin/logstash-keystore add my-secret`, at the prompt enter your secret. You will use the identifier ${my-secret} in your Logstash configuration."");
                    return;
                }
                if (argument == null || argument.isEmpty()) {
                    terminal.writeLine(""ERROR: You must supply a identifier to add. (e.g. bin/logstash-keystore add my-secret)"");
                    return;
                }
                if (secretStoreFactory.exists(config.clone())) {
                    SecretIdentifier id = new SecretIdentifier(argument);
                    SecretStore secretStore = secretStoreFactory.load(config);
                    byte[] s = secretStore.retrieveSecret(id);
                    if (s == null) {
                        terminal.write(String.format(""Enter value for %s: "", argument));
                        char[] secret = terminal.readSecret();
                        if (secret == null || secret.length == 0) {
                            terminal.writeLine(""ERROR: You must supply a identifier to add. (e.g. bin/logstash-keystore add my-secret)"");
                            return;
                        }
                        add(secretStore, id, SecretStoreUtil.asciiCharToBytes(secret));
                    } else {
                        SecretStoreUtil.clearBytes(s);
                        terminal.write(String.format(""%s already exists. Overwrite ? [y/N] "", argument));
                        if (isYes(terminal.readLine())) {
                            terminal.write(String.format(""Enter value for %s: "", argument));
                            char[] secret = terminal.readSecret();
                            add(secretStore, id, SecretStoreUtil.asciiCharToBytes(secret));
                        }
                    }
                } else {
                    terminal.writeLine(String.format(""ERROR: Logstash keystore not found. Use 'create' command to create one.""));
                }
                break;
            }
        case REMOVE:
            {
                if (help) {
                    terminal.writeLine(""Removes a secret from the keystore. For example: "" + ""`bin/logstash-keystore remove my-secret`"");
                    return;
                }
                if (argument == null || argument.isEmpty()) {
                    terminal.writeLine(""ERROR: You must supply a value to remove. (e.g. bin/logstash-keystore remove my-secret)"");
                    return;
                }
                SecretIdentifier id = new SecretIdentifier(argument);
                SecretStore secretStore = secretStoreFactory.load(config);
                if (secretStore.containsSecret(id)) {
                    secretStore.purgeSecret(id);
                    terminal.writeLine(String.format(""Removed '%s' from the Logstash keystore."", id.getKey()));
                } else {
                    terminal.writeLine(String.format(""ERROR: '%s' does not exist in the Logstash keystore."", argument));
                }
                break;
            }
        case HELP:
            {
                terminal.writeLine(""Usage:"");
                terminal.writeLine(""--------"");
                terminal.writeLine(""bin/logstash-keystore [option] command [argument]"");
                terminal.writeLine("""");
                terminal.writeLine(""Commands:"");
                terminal.writeLine(""--------"");
                terminal.writeLine(""create - Creates a new Logstash keystore  (e.g. bin/logstash-keystore create)"");
                terminal.writeLine(""list   - List entries in the keystore  (e.g. bin/logstash-keystore list)"");
                terminal.writeLine(""add    - Add a value to the keystore (e.g. bin/logstash-keystore add my-secret)"");
                terminal.writeLine(""remove - Remove a value from the keystore  (e.g. bin/logstash-keystore remove my-secret)"");
                terminal.writeLine("""");
                terminal.writeLine(""Argument:"");
                terminal.writeLine(""--------"");
                terminal.writeLine(""--help - Display command specific help  (e.g. bin/logstash-keystore add --help)"");
                terminal.writeLine("""");
                terminal.writeLine(""Options:"");
                terminal.writeLine(""--------"");
                terminal.writeLine(""--path.settings - Set the directory for the keystore. This is should be the same directory as the logstash.yml settings file. "" + ""The default is the config directory under Logstash home. (e.g. bin/logstash-keystore --path.settings /tmp/foo create)"");
                terminal.writeLine("""");
                break;
            }
    }
}","/**
 * Entry point to issue a command line command.
 * @param primaryCommand The string representation of a {@link SecretStoreCli.Command}, if the String does not map to a {@link SecretStoreCli.Command}, then it will show the help menu.
 * @param config The configuration needed to work a secret store. May be null for help.
 * @param argument This can be either the identifier for a secret, or a sub command like --help. May be null.
 */
", ,"/** * Entry point to issue a command line command. * @param primaryCommand The string representation of a {@link SecretStoreCli.Command}, if the String does not map to a {@link SecretStoreCli.Command}, then it will show the help menu. * @param config The configuration needed to work a secret store. May be null for help. * @param argument This can be either the identifier for a secret, or a sub command like --help. May be null. */",70,184,[0],0,[0],0,[0],0,0,0,0,"command(String, SecureConfig, String)",org.logstash.secret.cli.SecretStoreCli,"command/3[java.lang.String,org.logstash.secret.store.SecureConfig,java.lang.String]",False,70,10,34,14,20,22,30,119,7,14,3,30,3,2,0,5,0,0,38,1,12,4,4,0,0,2,104,1,0,True
479,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\cli\Terminal.java,org.logstash.secret.cli.Terminal,void writeLine(String),"/**
 * Writes a single line to the output.
 *
 * @param line the line to write.
 */
public void writeLine(String line) {
    if (useConsole) {
        System.console().writer().println(line);
        System.console().writer().flush();
    } else {
        System.out.println(line);
    }
}","/**
 * Writes a single line to the output.
 *
 * @param line the line to write.
 */
", ,/** * Writes a single line to the output. * * @param line the line to write. */,38,45,[0],0,[0],0,[0],0,0,0,0,writeLine(String),org.logstash.secret.cli.Terminal,writeLine/1[java.lang.String],False,38,0,3,3,0,2,5,9,0,0,1,5,0,0,0,0,0,0,0,0,0,0,1,0,0,0,10,1,0,True
480,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\cli\Terminal.java,org.logstash.secret.cli.Terminal,void write(String),"/**
 * Writes text to the output, but does not include a new line.
 *
 * @param text the text to write.
 */
public void write(String text) {
    if (useConsole) {
        System.console().writer().print(text);
        System.console().writer().flush();
    } else {
        System.out.print(text);
    }
}","/**
 * Writes text to the output, but does not include a new line.
 *
 * @param text the text to write.
 */
", ,"/** * Writes text to the output, but does not include a new line. * * @param text the text to write. */",52,59,[0],0,[0],0,[0],0,0,0,0,write(String),org.logstash.secret.cli.Terminal,write/1[java.lang.String],False,52,0,2,2,0,2,5,9,0,0,1,5,0,0,0,0,0,0,0,0,0,0,1,0,0,0,12,1,0,True
481,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\cli\Terminal.java,org.logstash.secret.cli.Terminal,String readLine(),"/**
 * Reads a single line
 *
 * @return the line
 */
public String readLine() {
    if (useConsole) {
        return System.console().readLine();
    } else {
        return scanner.next();
    }
}","/**
 * Reads a single line
 *
 * @return the line
 */
", ,/** * Reads a single line * * @return the line */,66,73,[0],0,[0],0,[0],0,0,0,0,readLine(),org.logstash.secret.cli.Terminal,readLine/0,False,66,0,2,2,0,2,3,8,2,0,0,3,0,0,0,0,0,0,0,0,0,0,1,0,0,0,9,1,0,True
482,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\cli\Terminal.java,org.logstash.secret.cli.Terminal,char[] readSecret(),"/**
 * Reads a secret
 *
 * @return the char[] representation of the secret.
 */
public char[] readSecret() {
    if (useConsole) {
        return System.console().readPassword();
    } else {
        return scanner.next().toCharArray();
    }
}","/**
 * Reads a secret
 *
 * @return the char[] representation of the secret.
 */
", ,/** * Reads a secret * * @return the char[] representation of the secret. */,80,86,[0],0,[0],0,[0],0,0,0,0,readSecret(),org.logstash.secret.cli.Terminal,readSecret/0,False,80,1,1,1,0,2,4,8,2,0,0,4,0,0,0,0,0,0,0,0,0,0,1,0,0,0,10,1,0,True
483,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\password\PasswordParamConverter.java,org.logstash.secret.password.PasswordParamConverter,"T convert(Class<T>, String)","/**
 * Converts given value to expected klass.
 * @param klass a class type of the desired output value.
 * @param value a value to be converted.
 * @param <T> desired type.
 * @return converted value.
 * throws {@link IllegalArgumentException} if klass is not supported or value is empty.
 */
@SuppressWarnings(""unchecked"")
public static <T> T convert(Class<T> klass, String value) {
    if (Strings.isNullOrEmpty(value)) {
        throw new IllegalArgumentException(""Value must not be empty."");
    }
    if (Objects.isNull(converters.get(klass))) {
        throw new IllegalArgumentException(""No conversion supported for given class."");
    }
    return (T) converters.get(klass).apply(value);
}","/**
 * Converts given value to expected klass.
 * @param klass a class type of the desired output value.
 * @param value a value to be converted.
 * @param <T> desired type.
 * @return converted value.
 * throws {@link IllegalArgumentException} if klass is not supported or value is empty.
 */
", ,/** * Converts given value to expected klass. * @param klass a class type of the desired output value. * @param value a value to be converted. * @param <T> desired type. * @return converted value. * throws {@link IllegalArgumentException} if klass is not supported or value is empty. */,53,63,[0],0,[0],0,[0],0,0,0,0,"convert(Class<T>, String)",org.logstash.secret.password.PasswordParamConverter,"convert/2[java.lang.Class<T>,java.lang.String]",False,54,1,0,0,0,3,4,9,1,0,2,4,0,0,0,0,0,0,3,0,0,0,1,0,0,0,25,9,0,True
484,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\password\PasswordValidator.java,org.logstash.secret.password.PasswordValidator,String validate(String),"/**
 * Validates given string against strong password policy and returns the list of failure reasoning.
 * Empty return list means password policy requirements meet.
 * @param password a password string going to be validated.
 * @return List of failure reasoning.
 */
public String validate(String password) {
    return validators.stream().map(validator -> validator.validate(password)).filter(Optional::isPresent).map(Optional::get).reduce("""", (partialString, element) -> (partialString.isEmpty() ? """" : partialString + "", "") + element);
}","/**
 * Validates given string against strong password policy and returns the list of failure reasoning.
 * Empty return list means password policy requirements meet.
 * @param password a password string going to be validated.
 * @return List of failure reasoning.
 */
", ,/** * Validates given string against strong password policy and returns the list of failure reasoning. * Empty return list means password policy requirements meet. * @param password a password string going to be validated. * @return List of failure reasoning. */,85,90,[0],0,[0],0,[0],0,0,0,0,validate(String),org.logstash.secret.password.PasswordValidator,validate/1[java.lang.String],False,85,1,1,0,1,2,7,3,1,3,1,7,0,0,0,0,0,1,3,0,0,2,0,0,0,2,26,1,0,True
485,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\password\Validator.java,org.logstash.secret.password.Validator,Optional<String> validate(String),"/**
 * Validates the input password.
 * @param password a password string
 * @return optional empty if succeeds or value for reasoning.
 */
Optional<String> validate(String password);","/**
 * Validates the input password.
 * @param password a password string
 * @return optional empty if succeeds or value for reasoning.
 */
", ,/** * Validates the input password. * @param password a password string * @return optional empty if succeeds or value for reasoning. */,14,14,[0],0,[0],0,[0],0,0,0,0,validate(String),org.logstash.secret.password.Validator,validate/1[java.lang.String],False,9,0,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,12,0,0,True
486,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStore.java,org.logstash.secret.store.SecretStore,SecretStore create(SecureConfig),"/**
 * Creates a new secret store
 *
 * @param secureConfig the configuration necessary to create the store
 * @return the newly created secret store.
 */
SecretStore create(SecureConfig secureConfig);","/**
 * Creates a new secret store
 *
 * @param secureConfig the configuration necessary to create the store
 * @return the newly created secret store.
 */
", ,/** * Creates a new secret store * * @param secureConfig the configuration necessary to create the store * @return the newly created secret store. */,41,41,[0],0,[0],0,[0],0,0,0,0,create(SecureConfig),org.logstash.secret.store.SecretStore,create/1[org.logstash.secret.store.SecureConfig],False,35,2,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,0,0,True
487,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStore.java,org.logstash.secret.store.SecretStore,void delete(SecureConfig),"/**
 * Delete secret store
 *
 * @param secureConfig the configuration necessary to delete the store
 */
void delete(SecureConfig secureConfig);","/**
 * Delete secret store
 *
 * @param secureConfig the configuration necessary to delete the store
 */
", ,/** * Delete secret store * * @param secureConfig the configuration necessary to delete the store */,48,48,[0],0,[0],0,[0],0,0,0,0,delete(SecureConfig),org.logstash.secret.store.SecretStore,delete/1[org.logstash.secret.store.SecureConfig],False,43,1,2,2,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,0,0,True
488,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStore.java,org.logstash.secret.store.SecretStore,boolean exists(SecureConfig),"/**
 * Queries if a secret store matching this configuration exists
 *
 * @param secureConfig the configuration necessary to determine if the secret store exists
 * @return true if the secret store exists, false other wise. Note - this does not provide any validity of the keystore, merely it's existence or not. It is recommended to
 * use the {@link SecretStoreFactory#LOGSTASH_MARKER} to test validity.
 */
boolean exists(SecureConfig secureConfig);","/**
 * Queries if a secret store matching this configuration exists
 *
 * @param secureConfig the configuration necessary to determine if the secret store exists
 * @return true if the secret store exists, false other wise. Note - this does not provide any validity of the keystore, merely it's existence or not. It is recommended to
 * use the {@link SecretStoreFactory#LOGSTASH_MARKER} to test validity.
 */
", ,"/** * Queries if a secret store matching this configuration exists * * @param secureConfig the configuration necessary to determine if the secret store exists * @return true if the secret store exists, false other wise. Note - this does not provide any validity of the keystore, merely it's existence or not. It is recommended to * use the {@link SecretStoreFactory#LOGSTASH_MARKER} to test validity. */",57,57,[0],0,[0],0,[0],0,0,0,0,exists(SecureConfig),org.logstash.secret.store.SecretStore,exists/1[org.logstash.secret.store.SecureConfig],False,50,1,2,2,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,0,0,True
489,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStore.java,org.logstash.secret.store.SecretStore,Collection<SecretIdentifier> list(),"/**
 * Gets all of the known {@link SecretIdentifier}
 *
 * @return a Collection of {@link SecretIdentifier}
 */
Collection<SecretIdentifier> list();","/**
 * Gets all of the known {@link SecretIdentifier}
 *
 * @return a Collection of {@link SecretIdentifier}
 */
", ,/** * Gets all of the known {@link SecretIdentifier} * * @return a Collection of {@link SecretIdentifier} */,64,64,[0],0,[0],0,[0],0,0,0,0,list(),org.logstash.secret.store.SecretStore,list/0,False,59,1,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,0,0,True
490,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStore.java,org.logstash.secret.store.SecretStore,SecretStore load(SecureConfig),"/**
 * Loads an existing secret store
 *
 * @param secureConfig the configuration necessary to load the store
 * @return the loaded secret store.
 */
SecretStore load(SecureConfig secureConfig);","/**
 * Loads an existing secret store
 *
 * @param secureConfig the configuration necessary to load the store
 * @return the loaded secret store.
 */
", ,/** * Loads an existing secret store * * @param secureConfig the configuration necessary to load the store * @return the loaded secret store. */,72,72,[0],0,[0],0,[0],0,0,0,0,load(SecureConfig),org.logstash.secret.store.SecretStore,load/1[org.logstash.secret.store.SecureConfig],False,66,2,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,0,0,True
491,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStore.java,org.logstash.secret.store.SecretStore,"void persistSecret(SecretIdentifier, byte[])","/**
 * Persist a secret to the store. Implementations should overwrite existing secrets with same identifier without error unless explicitly documented otherwise.
 *
 * @param id     The {@link SecretIdentifier} to identify the secret to persist
 * @param secret The byte[] representation of the secret. Implementations should zero out this byte[] once it has been persisted.
 */
void persistSecret(SecretIdentifier id, byte[] secret);","/**
 * Persist a secret to the store. Implementations should overwrite existing secrets with same identifier without error unless explicitly documented otherwise.
 *
 * @param id     The {@link SecretIdentifier} to identify the secret to persist
 * @param secret The byte[] representation of the secret. Implementations should zero out this byte[] once it has been persisted.
 */
", ,/** * Persist a secret to the store. Implementations should overwrite existing secrets with same identifier without error unless explicitly documented otherwise. * * @param id     The {@link SecretIdentifier} to identify the secret to persist * @param secret The byte[] representation of the secret. Implementations should zero out this byte[] once it has been persisted. */,80,80,[0],0,[0],0,[0],0,0,0,0,"persistSecret(SecretIdentifier, byte[])",org.logstash.secret.store.SecretStore,"persistSecret/2[org.logstash.secret.SecretIdentifier,byte[]]",False,74,2,2,2,0,1,0,1,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,0,0,True
492,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStore.java,org.logstash.secret.store.SecretStore,void purgeSecret(SecretIdentifier),"/**
 * Purges the secret from the store.
 *
 * @param id The {@link SecretIdentifier} to identify the secret to purge
 */
void purgeSecret(SecretIdentifier id);","/**
 * Purges the secret from the store.
 *
 * @param id The {@link SecretIdentifier} to identify the secret to purge
 */
", ,/** * Purges the secret from the store. * * @param id The {@link SecretIdentifier} to identify the secret to purge */,87,87,[0],0,[0],0,[0],0,0,0,0,purgeSecret(SecretIdentifier),org.logstash.secret.store.SecretStore,purgeSecret/1[org.logstash.secret.SecretIdentifier],False,82,1,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,0,0,True
493,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStore.java,org.logstash.secret.store.SecretStore,byte[] retrieveSecret(SecretIdentifier),"/**
 * Retrieves a secret.
 *
 * @param id The {@link SecretIdentifier} to identify the secret to retrieve
 * @return the byte[] of the secret, null if no secret is found.
 */
byte[] retrieveSecret(SecretIdentifier id);","/**
 * Retrieves a secret.
 *
 * @param id The {@link SecretIdentifier} to identify the secret to retrieve
 * @return the byte[] of the secret, null if no secret is found.
 */
", ,"/** * Retrieves a secret. * * @param id The {@link SecretIdentifier} to identify the secret to retrieve * @return the byte[] of the secret, null if no secret is found. */",95,95,[0],0,[0],0,[0],0,0,0,0,retrieveSecret(SecretIdentifier),org.logstash.secret.store.SecretStore,retrieveSecret/1[org.logstash.secret.SecretIdentifier],False,89,2,4,4,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,0,0,True
494,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStore.java,org.logstash.secret.store.SecretStore,boolean containsSecret(SecretIdentifier),"/**
 * Check if a secret exists in the store.
 *
 * @param id The {@link SecretIdentifier} to identify the secret to find
 * @return true if a secret exists, false otherwise
 */
boolean containsSecret(SecretIdentifier id);","/**
 * Check if a secret exists in the store.
 *
 * @param id The {@link SecretIdentifier} to identify the secret to find
 * @return true if a secret exists, false otherwise
 */
", ,"/** * Check if a secret exists in the store. * * @param id The {@link SecretIdentifier} to identify the secret to find * @return true if a secret exists, false otherwise */",103,103,[0],0,[0],0,[0],0,0,0,0,containsSecret(SecretIdentifier),org.logstash.secret.store.SecretStore,containsSecret/1[org.logstash.secret.SecretIdentifier],False,97,1,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,17,0,0,True
495,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreFactory.java,org.logstash.secret.store.SecretStoreFactory,SecretStoreFactory fromEnvironment(),"/**
 * @return a {@link SecretStoreFactory} that is capable of pulling credentials from the process' environment
 *         variables using the platform-specific {@link System#getenv(String)}.
 */
public static SecretStoreFactory fromEnvironment() {
    return new SecretStoreFactory(System::getenv);
}","/**
 * @return a {@link SecretStoreFactory} that is capable of pulling credentials from the process' environment
 *         variables using the platform-specific {@link System#getenv(String)}.
 */
", ,/** * @return a {@link SecretStoreFactory} that is capable of pulling credentials from the process' environment *         variables using the platform-specific {@link System#getenv(String)}. */,53,55,[0],0,[0],0,[0],0,0,0,0,fromEnvironment(),org.logstash.secret.store.SecretStoreFactory,fromEnvironment/0,False,53,1,2,1,1,1,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,17,9,0,True
496,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreFactory.java,org.logstash.secret.store.SecretStoreFactory,"SecretStoreFactory withEnvironment(Map<String, String>)","/**
 * Get a {@link SecretStoreFactory} that is capable of pulling credentials from the specified environment variable map.
 *
 * Useful for testing scenarios, where overwriting the pseudo-immutable mapping of environment variables available
 * via official APIs would require platform-dependent hackery.
 *
 * @param environmentVariables a Map of environment variables that wholly-overrides the process' environment for
 *                             purposes of credential extraction by the returned instance of {@link SecretStoreFactory}.
 * @return a {@link SecretStoreFactory} that uses only the provided environment variables.
 */
public static SecretStoreFactory withEnvironment(final Map<String, String> environmentVariables) {
    return new SecretStoreFactory(environmentVariables::get);
}","/**
 * Get a {@link SecretStoreFactory} that is capable of pulling credentials from the specified environment variable map.
 *
 * Useful for testing scenarios, where overwriting the pseudo-immutable mapping of environment variables available
 * via official APIs would require platform-dependent hackery.
 *
 * @param environmentVariables a Map of environment variables that wholly-overrides the process' environment for
 *                             purposes of credential extraction by the returned instance of {@link SecretStoreFactory}.
 * @return a {@link SecretStoreFactory} that uses only the provided environment variables.
 */
", ,"/** * Get a {@link SecretStoreFactory} that is capable of pulling credentials from the specified environment variable map. * * Useful for testing scenarios, where overwriting the pseudo-immutable mapping of environment variables available * via official APIs would require platform-dependent hackery. * * @param environmentVariables a Map of environment variables that wholly-overrides the process' environment for *                             purposes of credential extraction by the returned instance of {@link SecretStoreFactory}. * @return a {@link SecretStoreFactory} that uses only the provided environment variables. */",67,69,[0],0,[0],0,[0],0,1,0,0,"withEnvironment(Map<String, String>)",org.logstash.secret.store.SecretStoreFactory,"withEnvironment/1[java.util.Map<java.lang.String,java.lang.String>]",False,67,1,3,2,1,1,0,3,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,48,9,0,True
497,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreFactory.java,org.logstash.secret.store.SecretStoreFactory,boolean exists(SecureConfig),"/**
 * Determine if this secret store currently exists
 * @param secureConfig The configuration to pass to the implementation
 * @return true if the secret store exists, false otherwise
 */
public boolean exists(SecureConfig secureConfig) {
    return doIt(MODE.EXISTS, secureConfig).exists(secureConfig);
}","/**
 * Determine if this secret store currently exists
 * @param secureConfig The configuration to pass to the implementation
 * @return true if the secret store exists, false otherwise
 */
", ,"/** * Determine if this secret store currently exists * @param secureConfig The configuration to pass to the implementation * @return true if the secret store exists, false otherwise */",84,86,[0],0,[0],0,[0],0,0,0,0,exists(SecureConfig),org.logstash.secret.store.SecretStoreFactory,exists/1[org.logstash.secret.store.SecureConfig],False,84,3,5,3,2,1,2,3,1,0,1,2,1,2,0,0,0,0,0,0,0,0,0,0,0,0,19,1,0,True
498,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreFactory.java,org.logstash.secret.store.SecretStoreFactory,SecretStore create(SecureConfig),"/**
 * Creates a new {@link SecretStore} based on the provided configuration
 *
 * @param secureConfig The configuration to pass to the implementation
 * @return the newly created SecretStore, throws {@link SecretStoreException} if errors occur while loading, or if store already exists
 */
public SecretStore create(SecureConfig secureConfig) {
    return doIt(MODE.CREATE, secureConfig);
}","/**
 * Creates a new {@link SecretStore} based on the provided configuration
 *
 * @param secureConfig The configuration to pass to the implementation
 * @return the newly created SecretStore, throws {@link SecretStoreException} if errors occur while loading, or if store already exists
 */
", ,"/** * Creates a new {@link SecretStore} based on the provided configuration * * @param secureConfig The configuration to pass to the implementation * @return the newly created SecretStore, throws {@link SecretStoreException} if errors occur while loading, or if store already exists */",94,96,[0],0,[0],0,[0],0,0,0,0,create(SecureConfig),org.logstash.secret.store.SecretStoreFactory,create/1[org.logstash.secret.store.SecureConfig],False,94,3,4,3,1,1,1,3,1,0,1,1,1,2,0,0,0,0,0,0,0,0,0,0,0,0,28,1,0,True
499,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreFactory.java,org.logstash.secret.store.SecretStoreFactory,void delete(SecureConfig),"/**
 * Deletes a {@link SecretStore} based on the provided configuration
 *
 * @param secureConfig The configuration to pass to the implementation
 * throws {@link SecretStoreException} if errors occur
 */
public void delete(SecureConfig secureConfig) {
    doIt(MODE.DELETE, secureConfig);
}","/**
 * Deletes a {@link SecretStore} based on the provided configuration
 *
 * @param secureConfig The configuration to pass to the implementation
 * throws {@link SecretStoreException} if errors occur
 */
", ,/** * Deletes a {@link SecretStore} based on the provided configuration * * @param secureConfig The configuration to pass to the implementation * throws {@link SecretStoreException} if errors occur */,104,106,[0],0,[0],0,[0],0,0,0,0,delete(SecureConfig),org.logstash.secret.store.SecretStoreFactory,delete/1[org.logstash.secret.store.SecureConfig],False,104,2,2,1,1,1,1,3,0,0,1,1,1,2,0,0,0,0,0,0,0,0,0,0,0,0,22,1,0,True
500,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreFactory.java,org.logstash.secret.store.SecretStoreFactory,SecretStore load(SecureConfig),"/**
 * Loads an existing {@link SecretStore} based on the provided configuration
 *
 * @param secureConfig The configuration to pass to the implementation
 * @return the loaded SecretStore, throws {@link SecretStoreException} if errors occur while loading, or if store does not exist
 */
public SecretStore load(SecureConfig secureConfig) {
    return doIt(MODE.LOAD, secureConfig);
}","/**
 * Loads an existing {@link SecretStore} based on the provided configuration
 *
 * @param secureConfig The configuration to pass to the implementation
 * @return the loaded SecretStore, throws {@link SecretStoreException} if errors occur while loading, or if store does not exist
 */
", ,"/** * Loads an existing {@link SecretStore} based on the provided configuration * * @param secureConfig The configuration to pass to the implementation * @return the loaded SecretStore, throws {@link SecretStoreException} if errors occur while loading, or if store does not exist */",114,116,[0],0,[0],0,[0],0,0,0,0,load(SecureConfig),org.logstash.secret.store.SecretStoreFactory,load/1[org.logstash.secret.store.SecureConfig],False,114,3,8,7,1,1,1,3,1,0,1,1,1,2,0,0,0,0,0,0,0,0,0,0,0,0,29,1,0,True
501,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreFactory.java,org.logstash.secret.store.SecretStoreFactory,void addSecretStoreAccess(SecureConfig),"/**
 * <p>Adds the credential to the {@link SecureConfig} that is needed to access the {@link SecretStore}. Value read from environment variable ""LOGSTASH_KEYSTORE_PASS""</p>
 *
 * @param secureConfig The configuration to add the secret store access
 */
private void addSecretStoreAccess(SecureConfig secureConfig) {
    String keystore_pass = environmentVariableProvider.get(ENVIRONMENT_PASS_KEY);
    if (keystore_pass != null) {
        secureConfig.add(KEYSTORE_ACCESS_KEY, keystore_pass.toCharArray());
        keystore_pass = null;
    }
}","/**
 * <p>Adds the credential to the {@link SecureConfig} that is needed to access the {@link SecretStore}. Value read from environment variable ""LOGSTASH_KEYSTORE_PASS""</p>
 *
 * @param secureConfig The configuration to add the secret store access
 */
", ,"/** * <p>Adds the credential to the {@link SecureConfig} that is needed to access the {@link SecretStore}. Value read from environment variable ""LOGSTASH_KEYSTORE_PASS""</p> * * @param secureConfig The configuration to add the secret store access */",156,163,[0],0,[0],0,[0],0,0,0,0,addSecretStoreAccess(SecureConfig),org.logstash.secret.store.SecretStoreFactory,addSecretStoreAccess/1[org.logstash.secret.store.SecureConfig],False,156,2,3,1,2,2,3,7,0,1,1,3,0,0,0,1,0,0,0,0,2,0,1,0,0,0,42,2,0,True
502,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreUtil.java,org.logstash.secret.store.SecretStoreUtil,char[] asciiBytesToChar(byte[]),"/**
 * Converts bytes from ascii encoded text to a char[] and zero outs the original byte[]
 *
 * @param bytes the bytes from an ascii encoded text (note - no validation is done to ensure ascii encoding)
 * @return the corresponding char[]
 */
public static char[] asciiBytesToChar(byte[] bytes) {
    char[] chars = new char[bytes.length];
    for (int i = 0; i < bytes.length; i++) {
        chars[i] = (char) bytes[i];
        bytes[i] = '\0';
    }
    return chars;
}","/**
 * Converts bytes from ascii encoded text to a char[] and zero outs the original byte[]
 *
 * @param bytes the bytes from an ascii encoded text (note - no validation is done to ensure ascii encoding)
 * @return the corresponding char[]
 */
", ,/** * Converts bytes from ascii encoded text to a char[] and zero outs the original byte[] * * @param bytes the bytes from an ascii encoded text (note - no validation is done to ensure ascii encoding) * @return the corresponding char[] */,47,54,[0],0,[0],0,[0],0,0,0,0,asciiBytesToChar(byte[]),org.logstash.secret.store.SecretStoreUtil,asciiBytesToChar/1[byte[]],False,47,2,8,8,0,2,0,8,1,2,1,0,0,0,1,0,0,0,0,1,4,0,1,0,0,0,27,9,0,True
503,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreUtil.java,org.logstash.secret.store.SecretStoreUtil,byte[] asciiCharToBytes(char[]),"/**
 * Converts characters from ascii encoded text to a byte[] and zero outs the original char[]
 *
 * @param chars the chars from an ascii encoded text (note - no validation is done to ensure ascii encoding)
 * @return the corresponding byte[]
 */
public static byte[] asciiCharToBytes(char[] chars) {
    byte[] bytes = new byte[chars.length];
    for (int i = 0; i < chars.length; i++) {
        bytes[i] = (byte) chars[i];
        chars[i] = '\0';
    }
    return bytes;
}","/**
 * Converts characters from ascii encoded text to a byte[] and zero outs the original char[]
 *
 * @param chars the chars from an ascii encoded text (note - no validation is done to ensure ascii encoding)
 * @return the corresponding byte[]
 */
", ,/** * Converts characters from ascii encoded text to a byte[] and zero outs the original char[] * * @param chars the chars from an ascii encoded text (note - no validation is done to ensure ascii encoding) * @return the corresponding byte[] */,62,69,[0],0,[0],0,[0],0,0,0,0,asciiCharToBytes(char[]),org.logstash.secret.store.SecretStoreUtil,asciiCharToBytes/1[char[]],False,62,2,7,7,0,2,0,8,1,2,1,0,0,0,1,0,0,0,0,1,4,0,1,0,0,0,28,9,0,True
504,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreUtil.java,org.logstash.secret.store.SecretStoreUtil,byte[] base64Encode(byte[]),"/**
 * Base64 encode the given byte[], then zero the original byte[]
 *
 * @param b the byte[] to base64 encode
 * @return the base64 encoded bytes
 */
public static byte[] base64Encode(byte[] b) {
    byte[] bytes = Base64.getEncoder().encode(b);
    clearBytes(b);
    return bytes;
}","/**
 * Base64 encode the given byte[], then zero the original byte[]
 *
 * @param b the byte[] to base64 encode
 * @return the base64 encoded bytes
 */
", ,"/** * Base64 encode the given byte[], then zero the original byte[] * * @param b the byte[] to base64 encode * @return the base64 encoded bytes */",77,81,[0],0,[0],0,[0],0,0,0,0,base64Encode(byte[]),org.logstash.secret.store.SecretStoreUtil,base64Encode/1[byte[]],False,77,2,6,5,1,1,3,5,1,1,1,3,1,1,0,0,0,0,0,0,1,0,0,0,0,0,15,9,0,True
505,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreUtil.java,org.logstash.secret.store.SecretStoreUtil,char[] base64EncodeToChars(byte[]),"/**
 * Base64 encode the given byte[], then zero out the original byte[]
 *
 * @param bytes the byte[] to base64 encode
 * @return the char[] representation of the base64 encoding
 */
public static char[] base64EncodeToChars(byte[] bytes) {
    return asciiBytesToChar(base64Encode(bytes));
}","/**
 * Base64 encode the given byte[], then zero out the original byte[]
 *
 * @param bytes the byte[] to base64 encode
 * @return the char[] representation of the base64 encoding
 */
", ,"/** * Base64 encode the given byte[], then zero out the original byte[] * * @param bytes the byte[] to base64 encode * @return the char[] representation of the base64 encoding */",89,91,[0],0,[0],0,[0],0,0,0,0,base64EncodeToChars(byte[]),org.logstash.secret.store.SecretStoreUtil,base64EncodeToChars/1[byte[]],False,89,3,5,3,2,1,2,3,1,0,1,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,20,9,0,True
506,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreUtil.java,org.logstash.secret.store.SecretStoreUtil,char[] base64Encode(char[]),"/**
 * Base64 encode the given char[], then zero out the original char[]
 *
 * @param chars the char[] to base64 encode
 * @return the char[] representation of the base64 encoding
 */
public static char[] base64Encode(char[] chars) {
    return asciiBytesToChar(base64Encode(asciiCharToBytes(chars)));
}","/**
 * Base64 encode the given char[], then zero out the original char[]
 *
 * @param chars the char[] to base64 encode
 * @return the char[] representation of the base64 encoding
 */
", ,"/** * Base64 encode the given char[], then zero out the original char[] * * @param chars the char[] to base64 encode * @return the char[] representation of the base64 encoding */",99,101,[0],0,[0],0,[0],0,0,0,0,base64Encode(char[]),org.logstash.secret.store.SecretStoreUtil,base64Encode/1[char[]],False,99,2,5,2,3,1,3,3,1,0,1,3,3,2,0,0,0,0,0,0,0,0,0,0,0,0,19,9,0,True
507,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreUtil.java,org.logstash.secret.store.SecretStoreUtil,byte[] base64Decode(byte[]),"/**
 * Decodes a Base64 encoded byte[], then zero out the original byte[]
 *
 * @param b the base64 bytes
 * @return the non-base64 encoded bytes
 */
public static byte[] base64Decode(byte[] b) {
    byte[] bytes = Base64.getDecoder().decode(b);
    clearBytes(b);
    return bytes;
}","/**
 * Decodes a Base64 encoded byte[], then zero out the original byte[]
 *
 * @param b the base64 bytes
 * @return the non-base64 encoded bytes
 */
", ,"/** * Decodes a Base64 encoded byte[], then zero out the original byte[] * * @param b the base64 bytes * @return the non-base64 encoded bytes */",109,113,[0],0,[0],0,[0],0,0,0,0,base64Decode(byte[]),org.logstash.secret.store.SecretStoreUtil,base64Decode/1[byte[]],False,109,2,3,2,1,1,3,5,1,1,1,3,1,1,0,0,0,0,0,0,1,0,0,0,0,0,16,9,0,True
508,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreUtil.java,org.logstash.secret.store.SecretStoreUtil,byte[] base64Decode(char[]),"/**
 * Decodes a Base64 encoded char[], then zero out the original char[]
 *
 * @param chars the base64 chars
 * @return the non-base64 encoded chars
 */
public static byte[] base64Decode(char[] chars) {
    return base64Decode(asciiCharToBytes(chars));
}","/**
 * Decodes a Base64 encoded char[], then zero out the original char[]
 *
 * @param chars the base64 chars
 * @return the non-base64 encoded chars
 */
", ,"/** * Decodes a Base64 encoded char[], then zero out the original char[] * * @param chars the base64 chars * @return the non-base64 encoded chars */",121,123,[0],0,[0],0,[0],0,0,0,0,base64Decode(char[]),org.logstash.secret.store.SecretStoreUtil,base64Decode/1[char[]],False,121,3,5,3,2,1,2,3,1,0,1,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,17,9,0,True
509,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreUtil.java,org.logstash.secret.store.SecretStoreUtil,void clearChars(char[]),"/**
 * Attempt to keep data out of the heap.
 *
 * @param chars the bytes to zero out
 */
public static void clearChars(char[] chars) {
    Arrays.fill(chars, '\0');
}","/**
 * Attempt to keep data out of the heap.
 *
 * @param chars the bytes to zero out
 */
", ,/** * Attempt to keep data out of the heap. * * @param chars the bytes to zero out */,130,132,[0],0,[0],0,[0],0,0,0,0,clearChars(char[]),org.logstash.secret.store.SecretStoreUtil,clearChars/1[char[]],False,130,1,2,2,0,1,1,3,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,12,9,0,True
510,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreUtil.java,org.logstash.secret.store.SecretStoreUtil,void clearBytes(byte[]),"/**
 * Attempt to keep data out of the heap.
 *
 * @param bytes the bytes to zero out
 */
public static void clearBytes(byte[] bytes) {
    Arrays.fill(bytes, (byte) '\0');
}","/**
 * Attempt to keep data out of the heap.
 *
 * @param bytes the bytes to zero out
 */
", ,/** * Attempt to keep data out of the heap. * * @param bytes the bytes to zero out */,139,141,[0],0,[0],0,[0],0,0,0,0,clearBytes(byte[]),org.logstash.secret.store.SecretStoreUtil,clearBytes/1[byte[]],False,139,1,6,6,0,1,1,3,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,9,0,True
511,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreUtil.java,org.logstash.secret.store.SecretStoreUtil,char[] deObfuscate(char[]),"/**
 * De-obfuscates the obfuscated char[] generated by {@link SecretStoreUtil#obfuscate(char[])}
 *
 * @param chars The chars to de-obscure
 * @return the de-obscured chars
 */
public static char[] deObfuscate(char[] chars) {
    byte[] bytes = asciiCharToBytes(chars);
    byte[] random = Arrays.copyOfRange(bytes, bytes.length / 2, bytes.length);
    byte[] deObfuscated = new byte[random.length];
    for (int i = 0; i < random.length; i++) {
        int xor = bytes[i] ^ random[i];
        deObfuscated[i] = ((byte) (xor & 0xff));
    }
    return asciiBytesToChar(deObfuscated);
}","/**
 * De-obfuscates the obfuscated char[] generated by {@link SecretStoreUtil#obfuscate(char[])}
 *
 * @param chars The chars to de-obscure
 * @return the de-obscured chars
 */
", ,/** * De-obfuscates the obfuscated char[] generated by {@link SecretStoreUtil#obfuscate(char[])} * * @param chars The chars to de-obscure * @return the de-obscured chars */,150,159,[0],0,[0],0,[0],0,0,0,0,deObfuscate(char[]),org.logstash.secret.store.SecretStoreUtil,deObfuscate/1[char[]],False,150,3,6,4,2,2,3,10,1,5,1,3,2,1,1,0,0,2,0,3,6,1,1,0,0,0,23,9,0,True
512,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecretStoreUtil.java,org.logstash.secret.store.SecretStoreUtil,char[] obfuscate(char[]),"/**
 * <p>Simple obfuscation that adds a bit of randomness and shuffles the bits of a char[].</p>
 * <p>Note - this is NOT security and will only deter the lazy.</p>
 *
 * @param chars The chars to obscure
 * @return the obscured bytes
 */
public static char[] obfuscate(char[] chars) {
    byte[] bytes = asciiCharToBytes(chars);
    byte[] random = new byte[bytes.length];
    RANDOM.nextBytes(random);
    ByteBuffer obfuscated = ByteBuffer.allocate(bytes.length * 2);
    for (int i = 0; i < bytes.length; i++) {
        int xor = bytes[i] ^ random[i];
        obfuscated.put((byte) (0xff & xor));
    }
    obfuscated.put(random);
    char[] result = asciiBytesToChar(obfuscated.array());
    clearBytes(obfuscated.array());
    return result;
}","/**
 * <p>Simple obfuscation that adds a bit of randomness and shuffles the bits of a char[].</p>
 * <p>Note - this is NOT security and will only deter the lazy.</p>
 *
 * @param chars The chars to obscure
 * @return the obscured bytes
 */
", ,/** * <p>Simple obfuscation that adds a bit of randomness and shuffles the bits of a char[].</p> * <p>Note - this is NOT security and will only deter the lazy.</p> * * @param chars The chars to obscure * @return the obscured bytes */,168,182,[0],0,[0],0,[0],0,0,0,0,obfuscate(char[]),org.logstash.secret.store.SecretStoreUtil,obfuscate/1[char[]],False,168,3,6,3,3,2,8,14,1,6,1,8,3,1,1,0,0,1,0,3,6,1,1,0,0,0,42,9,0,True
513,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecureConfig.java,org.logstash.secret.store.SecureConfig,"void add(String, char[])","/**
 * adds a value to the secure config
 *
 * @param key   the reference to the configuration value
 * @param value the configuration value
 * @throws IllegalStateException if this configuration has been cleared
 */
public void add(String key, char[] value) {
    if (cleared) {
        throw new IllegalStateException(""This configuration has been cleared and can not be re-used."");
    }
    config.put(key, CharBuffer.wrap(SecretStoreUtil.obfuscate(value)));
}","/**
 * adds a value to the secure config
 *
 * @param key   the reference to the configuration value
 * @param value the configuration value
 * @throws IllegalStateException if this configuration has been cleared
 */
", ,/** * adds a value to the secure config * * @param key   the reference to the configuration value * @param value the configuration value * @throws IllegalStateException if this configuration has been cleared */,44,49,[0],0,[0],0,[0],0,0,0,0,"add(String, char[])",org.logstash.secret.store.SecureConfig,"add/2[java.lang.String,char[]]",False,44,2,30,29,1,2,3,6,0,0,2,3,0,0,0,0,0,0,1,0,0,0,1,0,0,0,22,1,0,True
514,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecureConfig.java,org.logstash.secret.store.SecureConfig,void clearValues(),"/**
 * Zero outs all internally held char[]
 */
public void clearValues() {
    config.forEach((k, v) -> SecretStoreUtil.clearChars(v.array()));
    cleared = true;
}","/**
 * Zero outs all internally held char[]
 */
", ,/** * Zero outs all internally held char[] */,54,57,[0],0,[0],0,[0],0,0,0,0,clearValues(),org.logstash.secret.store.SecureConfig,clearValues/0,False,54,1,8,7,1,1,3,4,0,2,0,3,0,0,0,0,0,0,0,0,1,0,0,0,0,1,9,1,0,True
515,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecureConfig.java,org.logstash.secret.store.SecureConfig,SecureConfig clone(),"/**
 * Creates a full clone of this object.
 *
 * @return a copy of this {@link SecureConfig}
 * @throws IllegalStateException if this configuration has been cleared
 */
public SecureConfig clone() {
    if (cleared) {
        throw new IllegalStateException(""This configuration has been cleared and can not be re-used."");
    }
    SecureConfig clone = new SecureConfig();
    config.forEach((k, v) -> clone.add(k, SecretStoreUtil.deObfuscate(v.array().clone())));
    return clone;
}","/**
 * Creates a full clone of this object.
 *
 * @return a copy of this {@link SecureConfig}
 * @throws IllegalStateException if this configuration has been cleared
 */
", ,/** * Creates a full clone of this object. * * @return a copy of this {@link SecureConfig} * @throws IllegalStateException if this configuration has been cleared */,65,72,[0],0,[0],0,[0],0,0,0,0,clone(),org.logstash.secret.store.SecureConfig,clone/0,False,65,2,20,17,3,2,5,8,1,3,0,5,1,1,0,0,0,0,1,0,1,0,1,0,0,1,20,1,0,True
516,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecureConfig.java,org.logstash.secret.store.SecureConfig,char[] getPlainText(String),"/**
 * Retrieve the un-obfuscated value
 *
 * @param key the reference to the configuration value
 * @return the un-obfuscated configuration value.
 */
public char[] getPlainText(String key) {
    if (cleared) {
        throw new IllegalStateException(""This configuration has been cleared and can not be re-used."");
    }
    return config.get(key) == null ? null : SecretStoreUtil.deObfuscate(config.get(key).array().clone());
}","/**
 * Retrieve the un-obfuscated value
 *
 * @param key the reference to the configuration value
 * @return the un-obfuscated configuration value.
 */
", ,/** * Retrieve the un-obfuscated value * * @param key the reference to the configuration value * @return the un-obfuscated configuration value. */,80,85,[0],0,[0],0,[0],0,0,0,0,getPlainText(String),org.logstash.secret.store.SecureConfig,getPlainText/1[java.lang.String],False,80,2,11,10,1,3,4,6,1,0,1,4,0,0,0,1,0,0,1,0,0,0,1,0,0,0,24,1,0,True
517,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\SecureConfig.java,org.logstash.secret.store.SecureConfig,boolean has(String),"/**
 * Determine if a value for this key exists. No guarantees if the value has been zero'ed (cleared) or not.
 *
 * @param key the reference to the configuration value.
 * @return true if this key has ever been added, false otherwise
 */
public boolean has(String key) {
    return config.get(key) != null;
}","/**
 * Determine if a value for this key exists. No guarantees if the value has been zero'ed (cleared) or not.
 *
 * @param key the reference to the configuration value.
 * @return true if this key has ever been added, false otherwise
 */
", ,"/** * Determine if a value for this key exists. No guarantees if the value has been zero'ed (cleared) or not. * * @param key the reference to the configuration value. * @return true if this key has ever been added, false otherwise */",93,95,[0],0,[0],0,[0],0,0,0,0,has(String),org.logstash.secret.store.SecureConfig,has/1[java.lang.String],False,93,0,2,2,0,2,1,3,1,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,19,1,0,True
518,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\backend\JavaKeyStore.java,org.logstash.secret.store.backend.JavaKeyStore,JavaKeyStore create(SecureConfig),"/**
 * {@inheritDoc}
 *
 * @param config The configuration for this keystore <p>Requires ""keystore.file"" in the configuration,</p><p>WARNING! this method clears all values
 *               from this configuration, meaning this config is NOT reusable after passed in here.</p>
 * @throws SecretStoreException.CreateException if the store can not be created
 * @throws SecretStoreException                 (of other sub types) if contributing factors prevent the creation
 */
@Override
public JavaKeyStore create(SecureConfig config) {
    if (exists(config)) {
        throw new SecretStoreException.AlreadyExistsException(String.format(""Logstash keystore at %s already exists."", new String(config.getPlainText(PATH_KEY))));
    }
    try {
        init(config);
        lock.lock();
        LOGGER.debug(""Creating new keystore at {}."", keyStorePath.toAbsolutePath());
        String keyStorePermissions = filePermissions;
        // create the keystore on disk with a default entry to identify this as a logstash keystore
        // can not set posix attributes on create here since not all Windows are posix, *nix will get the umask default and posix permissions will be set below
        Files.createFile(keyStorePath);
        try {
            keyStore = KeyStore.Builder.newInstance(KEYSTORE_TYPE, null, protectionParameter).getKeyStore();
            SecretKeyFactory factory = SecretKeyFactory.getInstance(""PBE"");
            byte[] base64 = SecretStoreUtil.base64Encode(LOGSTASH_MARKER.getKey().getBytes(StandardCharsets.UTF_8));
            SecretKey secretKey = factory.generateSecret(new PBEKeySpec(SecretStoreUtil.asciiBytesToChar(base64)));
            keyStore.setEntry(LOGSTASH_MARKER.toExternalForm(), new KeyStore.SecretKeyEntry(secretKey), protectionParameter);
            saveKeyStore();
            PosixFileAttributeView attrs = Files.getFileAttributeView(keyStorePath, PosixFileAttributeView.class);
            if (attrs != null) {
                // the directory umask applies when creating the file, so re-apply permissions here
                attrs.setPermissions(PosixFilePermissions.fromString(keyStorePermissions));
            }
            LOGGER.info(""Created Logstash keystore at {}"", keyStorePath.toAbsolutePath());
            return this;
        } catch (Exception e) {
            throw new SecretStoreException.CreateException(""Failed to create Logstash keystore."", e);
        }
    } catch (SecretStoreException sse) {
        throw sse;
    } catch (NoSuchFileException | AccessDeniedException fe) {
        throw new SecretStoreException.CreateException(""Error while trying to create the Logstash keystore. Please ensure that path to "" + keyStorePath.toAbsolutePath() + "" exists and is writable"", fe);
    } catch (Exception e) {
        // should never happen
        throw new SecretStoreException.UnknownException(""Error while trying to create the Logstash keystore. "", e);
    } finally {
        releaseLock(lock);
        config.clearValues();
    }
}","/**
 * {@inheritDoc}
 *
 * @param config The configuration for this keystore <p>Requires ""keystore.file"" in the configuration,</p><p>WARNING! this method clears all values
 *               from this configuration, meaning this config is NOT reusable after passed in here.</p>
 * @throws SecretStoreException.CreateException if the store can not be created
 * @throws SecretStoreException                 (of other sub types) if contributing factors prevent the creation
 */
","// create the keystore on disk with a default entry to identify this as a logstash keystore
[[SEP]]// can not set posix attributes on create here since not all Windows are posix, *nix will get the umask default and posix permissions will be set below
[[SEP]]// the directory umask applies when creating the file, so re-apply permissions here
[[SEP]]// should never happen
","/** * {@inheritDoc} * * @param config The configuration for this keystore <p>Requires ""keystore.file"" in the configuration,</p><p>WARNING! this method clears all values *               from this configuration, meaning this config is NOT reusable after passed in here.</p> * @throws SecretStoreException.CreateException if the store can not be created * @throws SecretStoreException                 (of other sub types) if contributing factors prevent the creation */[[SEP]]// create the keystore on disk with a default entry to identify this as a logstash keystore// can not set posix attributes on create here since not all Windows are posix, *nix will get the umask default and posix permissions will be set below[[SEP]]// the directory umask applies when creating the file, so re-apply permissions here[[SEP]]// should never happen",100,142,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,create(SecureConfig),org.logstash.secret.store.backend.JavaKeyStore,create/1[org.logstash.secret.store.SecureConfig],False,101,10,29,14,15,7,25,42,1,5,1,25,4,4,0,1,2,0,8,0,6,1,3,0,0,0,91,1,2,True
519,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\backend\JavaKeyStore.java,org.logstash.secret.store.backend.JavaKeyStore,void delete(SecureConfig),"@Override
public void delete(SecureConfig config) {
    try {
        initLocks();
        lock.lock();
        if (exists(config)) {
            Files.delete(Paths.get(new String(config.getPlainText(PATH_KEY))));
        }
    } catch (SecretStoreException sse) {
        throw sse;
    } catch (Exception e) {
        // should never happen
        throw new SecretStoreException.UnknownException(""Error while trying to delete the Logstash keystore"", e);
    } finally {
        releaseLock(lock);
        config.clearValues();
    }
}", ,"// should never happen
",// should never happen,144,160,[0],0,[0],0,[0],0,0,0,0,delete(SecureConfig),org.logstash.secret.store.backend.JavaKeyStore,delete/1[org.logstash.secret.store.SecureConfig],False,145,4,6,0,6,4,8,19,0,0,1,8,3,2,0,0,1,0,1,0,0,0,2,0,0,0,27,1,0,False
520,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\backend\JavaKeyStore.java,org.logstash.secret.store.backend.JavaKeyStore,boolean exists(SecureConfig),"/**
 * {@inheritDoc}
 *
 * @param config The configuration for this keystore <p>Requires ""keystore.file"" in the configuration</p>
 */
@Override
public boolean exists(SecureConfig config) {
    char[] path = config.getPlainText(PATH_KEY);
    if (!valid(path)) {
        // should only every happen via tests
        LOGGER.warn(""keystore.file configuration is not defined"");
        return false;
    }
    return new File(new String(path)).exists();
}","/**
 * {@inheritDoc}
 *
 * @param config The configuration for this keystore <p>Requires ""keystore.file"" in the configuration</p>
 */
","// should only every happen via tests
","/** * {@inheritDoc} * * @param config The configuration for this keystore <p>Requires ""keystore.file"" in the configuration</p> */[[SEP]]// should only every happen via tests",167,175,[0],0,[0],0,"[0, 0]",0,0,0,0,exists(SecureConfig),org.logstash.secret.store.backend.JavaKeyStore,exists/1[org.logstash.secret.store.SecureConfig],False,168,4,7,4,3,2,4,8,2,1,1,4,1,1,0,0,0,0,1,0,1,0,1,0,0,0,25,1,1,True
521,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\backend\JavaKeyStore.java,org.logstash.secret.store.backend.JavaKeyStore,char[] getKeyStorePassword(SecureConfig),"/**
 * Obtains the keystore password depending on if the password is explicitly defined and/or if this is a new keystore.
 *
 * @param config the configuration
 * @return the char[] of the keystore password
 * @throws IOException on io errors
 */
private char[] getKeyStorePassword(SecureConfig config) throws IOException {
    char[] plainText = config.getPlainText(SecretStoreFactory.KEYSTORE_ACCESS_KEY);
    boolean existing = exists(config);
    // ensure if a password is configured, that we don't allow empty passwords
    if (config.has(SecretStoreFactory.KEYSTORE_ACCESS_KEY) && (plainText == null || plainText.length == 0)) {
        String message = String.format(""Empty keystore passwords are not allowed. Please ensure configured password is not empty for Logstash keystore %s."", keyStorePath.toAbsolutePath());
        if (existing) {
            throw new SecretStoreException.AccessException(message);
        } else {
            throw new SecretStoreException.CreateException(message);
        }
    }
    useDefaultPass = !config.has(SecretStoreFactory.KEYSTORE_ACCESS_KEY);
    if (!useDefaultPass) {
        // explicit user defined pass
        // keystore passwords require ascii encoding, only base64 encode if necessary
        return asciiEncoder.canEncode(CharBuffer.wrap(plainText)) ? plainText : SecretStoreUtil.base64Encode(plainText);
    }
    if (!existing) {
        // create the pass
        byte[] randomBytes = new byte[32];
        new Random().nextBytes(randomBytes);
        return SecretStoreUtil.base64EncodeToChars(randomBytes);
    }
    // read the pass
    SeekableByteChannel byteChannel = Files.newByteChannel(keyStorePath, StandardOpenOption.READ);
    if (byteChannel.size() == 0) {
        throw new SecretStoreException.AccessException(String.format(""Could not determine keystore password. Keystore file is empty. Please ensure the file at %s is a valid Logstash keystore"", keyStorePath.toAbsolutePath()));
    }
    byteChannel.position(byteChannel.size() - 1);
    ByteBuffer byteBuffer = ByteBuffer.allocate(1);
    byteChannel.read(byteBuffer);
    int size = byteBuffer.array()[0] & 0xff;
    if (size <= 0 || byteChannel.size() < size + 1) {
        throw new SecretStoreException.AccessException(String.format(""Could not determine keystore password. Please ensure the file at %s is a valid Logstash keystore"", keyStorePath.toAbsolutePath()));
    }
    byteBuffer = ByteBuffer.allocate(size);
    byteChannel.position(byteChannel.size() - size - 1);
    byteChannel.read(byteBuffer);
    return SecretStoreUtil.deObfuscate(SecretStoreUtil.asciiBytesToChar(byteBuffer.array()));
}","/**
 * Obtains the keystore password depending on if the password is explicitly defined and/or if this is a new keystore.
 *
 * @param config the configuration
 * @return the char[] of the keystore password
 * @throws IOException on io errors
 */
","// ensure if a password is configured, that we don't allow empty passwords
[[SEP]]// explicit user defined pass
[[SEP]]// keystore passwords require ascii encoding, only base64 encode if necessary
[[SEP]]// create the pass
[[SEP]]// read the pass
","/** * Obtains the keystore password depending on if the password is explicitly defined and/or if this is a new keystore. * * @param config the configuration * @return the char[] of the keystore password * @throws IOException on io errors */[[SEP]]// ensure if a password is configured, that we don't allow empty passwords[[SEP]]// explicit user defined pass// keystore passwords require ascii encoding, only base64 encode if necessary[[SEP]]// create the pass[[SEP]]// read the pass",184,230,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,getKeyStorePassword(SecureConfig),org.logstash.secret.store.backend.JavaKeyStore,getKeyStorePassword/1[org.logstash.secret.store.SecureConfig],False,184,7,10,1,9,11,18,37,3,7,1,18,1,2,0,3,0,1,3,10,9,3,2,0,0,0,58,2,0,True
522,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\backend\JavaKeyStore.java,org.logstash.secret.store.backend.JavaKeyStore,JavaKeyStore load(SecureConfig),"/**
 * {@inheritDoc}
 *
 * @param config The configuration for this keystore <p>Requires ""keystore.file"" in the configuration</p><p>WARNING! this method clears all values
 *               from this configuration, meaning this config is NOT reusable after passed in here.</p>
 * @throws SecretStoreException.CreateException if the store can not be created
 * @throws SecretStoreException                 (of other sub types) if contributing factors prevent the creation
 */
@Override
public JavaKeyStore load(SecureConfig config) {
    if (!exists(config)) {
        throw new SecretStoreException.LoadException(String.format(""Can not find Logstash keystore at %s. Please verify this file exists and is a valid Logstash keystore."", config.getPlainText(""keystore.file"") == null ? ""<undefined>"" : new String(config.getPlainText(""keystore.file""))));
    }
    try {
        init(config);
        lock.lock();
        try (final InputStream is = Files.newInputStream(keyStorePath)) {
            try {
                keyStore.load(is, this.keyStorePass);
            } catch (IOException ioe) {
                if (ioe.getCause() instanceof UnrecoverableKeyException) {
                    throw new SecretStoreException.AccessException(String.format(""Can not access Logstash keystore at %s. Please verify correct file permissions and keystore password."", keyStorePath.toAbsolutePath()), ioe);
                } else {
                    throw new SecretStoreException.LoadException(String.format(""Found a file at %s, but it is not a valid Logstash keystore."", keyStorePath.toAbsolutePath().toString()), ioe);
                }
            }
            byte[] marker = retrieveSecret(LOGSTASH_MARKER);
            if (marker == null) {
                throw new SecretStoreException.LoadException(String.format(""Found a keystore at %s, but it is not a Logstash keystore."", keyStorePath.toAbsolutePath().toString()));
            }
            LOGGER.debug(""Using existing keystore at {}"", keyStorePath.toAbsolutePath());
            return this;
        }
    } catch (SecretStoreException sse) {
        throw sse;
    } catch (Exception e) {
        // should never happen
        throw new SecretStoreException.UnknownException(""Error while trying to load the Logstash keystore"", e);
    } finally {
        releaseLock(lock);
        config.clearValues();
    }
}","/**
 * {@inheritDoc}
 *
 * @param config The configuration for this keystore <p>Requires ""keystore.file"" in the configuration</p><p>WARNING! this method clears all values
 *               from this configuration, meaning this config is NOT reusable after passed in here.</p>
 * @throws SecretStoreException.CreateException if the store can not be created
 * @throws SecretStoreException                 (of other sub types) if contributing factors prevent the creation
 */
","// should never happen
","/** * {@inheritDoc} * * @param config The configuration for this keystore <p>Requires ""keystore.file"" in the configuration</p><p>WARNING! this method clears all values *               from this configuration, meaning this config is NOT reusable after passed in here.</p> * @throws SecretStoreException.CreateException if the store can not be created * @throws SecretStoreException                 (of other sub types) if contributing factors prevent the creation */[[SEP]]// should never happen",275,314,[0],0,[0],0,"[0, 0]",0,0,0,0,load(SecureConfig),org.logstash.secret.store.backend.JavaKeyStore,load/1[org.logstash.secret.store.SecureConfig],False,276,8,27,16,11,8,14,38,1,2,1,14,4,5,0,2,3,0,9,0,2,0,4,0,0,0,86,1,1,True
523,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\backend\JavaKeyStore.java,org.logstash.secret.store.backend.JavaKeyStore,void loadKeyStore(),"/**
 * Need to load the keystore before any operations in case an external (or different JVM) has modified the keystore on disk.
 */
private void loadKeyStore() throws CertificateException, NoSuchAlgorithmException, IOException {
    try (final InputStream is = Files.newInputStream(keyStorePath)) {
        keyStore.load(is, keyStorePass);
    }
}","/**
 * Need to load the keystore before any operations in case an external (or different JVM) has modified the keystore on disk.
 */
", ,/** * Need to load the keystore before any operations in case an external (or different JVM) has modified the keystore on disk. */,319,323,[0],0,[0],0,[0],0,0,0,0,loadKeyStore(),org.logstash.secret.store.backend.JavaKeyStore,loadKeyStore/0,False,319,0,5,5,0,1,2,5,0,1,0,2,0,0,0,0,1,0,0,0,1,0,1,0,0,0,29,2,0,True
524,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\backend\JavaKeyStore.java,org.logstash.secret.store.backend.JavaKeyStore,"void persistSecret(SecretIdentifier, byte[])","@Override
public void persistSecret(SecretIdentifier identifier, byte[] secret) {
    try {
        lock.lock();
        loadKeyStore();
        SecretKeyFactory factory = SecretKeyFactory.getInstance(""PBE"");
        // PBEKey requires an ascii password, so base64 encode it
        byte[] base64 = SecretStoreUtil.base64Encode(secret);
        PBEKeySpec passwordBasedKeySpec = new PBEKeySpec(SecretStoreUtil.asciiBytesToChar(base64));
        SecretKey secretKey = factory.generateSecret(passwordBasedKeySpec);
        keyStore.setEntry(identifier.toExternalForm(), new KeyStore.SecretKeyEntry(secretKey), protectionParameter);
        try {
            saveKeyStore();
        } finally {
            passwordBasedKeySpec.clearPassword();
            SecretStoreUtil.clearBytes(secret);
        }
        LOGGER.debug(""persisted secret {}"", identifier.toExternalForm());
    } catch (Exception e) {
        throw new SecretStoreException.PersistException(identifier, e);
    } finally {
        releaseLock(lock);
    }
}", ,"// PBEKey requires an ascii password, so base64 encode it
","// PBEKey requires an ascii password, so base64 encode it",325,348,[0],0,[0],0,[0],0,0,0,0,"persistSecret(SecretIdentifier, byte[])",org.logstash.secret.store.backend.JavaKeyStore,"persistSecret/2[org.logstash.secret.SecretIdentifier,byte[]]",False,326,6,20,11,9,2,13,25,0,4,2,13,3,1,0,0,2,0,2,0,4,0,2,0,0,0,22,1,1,False
525,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\backend\JavaKeyStore.java,org.logstash.secret.store.backend.JavaKeyStore,byte[] retrieveSecret(SecretIdentifier),"@Override
public byte[] retrieveSecret(SecretIdentifier identifier) {
    if (identifier != null && identifier.getKey() != null && !identifier.getKey().isEmpty()) {
        try {
            lock.lock();
            loadKeyStore();
            SecretKeyFactory factory = SecretKeyFactory.getInstance(""PBE"");
            KeyStore.SecretKeyEntry secretKeyEntry = (KeyStore.SecretKeyEntry) keyStore.getEntry(identifier.toExternalForm(), protectionParameter);
            // not found
            if (secretKeyEntry == null) {
                LOGGER.debug(""requested secret {} not found"", identifier.toExternalForm());
                return null;
            }
            PBEKeySpec passwordBasedKeySpec = (PBEKeySpec) factory.getKeySpec(secretKeyEntry.getSecretKey(), PBEKeySpec.class);
            // base64 encoded char[]
            char[] base64secret = passwordBasedKeySpec.getPassword();
            byte[] secret = SecretStoreUtil.base64Decode(base64secret);
            passwordBasedKeySpec.clearPassword();
            LOGGER.debug(""retrieved secret {}"", identifier.toExternalForm());
            return secret;
        } catch (Exception e) {
            throw new SecretStoreException.RetrievalException(identifier, e);
        } finally {
            releaseLock(lock);
        }
    }
    return null;
}", ,"// not found
[[SEP]]// base64 encoded char[]
",// not found[[SEP]]// base64 encoded char[],382,409,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,retrieveSecret(SecretIdentifier),org.logstash.secret.store.backend.JavaKeyStore,retrieveSecret/1[org.logstash.secret.SecretIdentifier],False,383,7,23,16,7,6,14,27,3,5,1,14,2,1,0,3,1,0,3,0,5,0,3,0,0,0,25,1,2,False
526,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\backend\JavaKeyStore.java,org.logstash.secret.store.backend.JavaKeyStore,void saveKeyStore(),"/**
 * Saves the keystore with some extra meta data if needed. Note - need two output streams here to allow checking the with the append flag, and the other without an append.
 */
private void saveKeyStore() throws IOException, CertificateException, NoSuchAlgorithmException, KeyStoreException {
    FileLock fileLock = null;
    try (final FileOutputStream appendOs = new FileOutputStream(keyStorePath.toFile(), true)) {
        // The keystore.store method on Windows checks for the file lock and does not allow _any_ interaction with the keystore if it is locked.
        if (!IS_WINDOWS) {
            fileLock = appendOs.getChannel().tryLock();
            if (fileLock == null) {
                throw new IllegalStateException(""Can not save Logstash keystore. Some other process has locked on the file: "" + keyStorePath.toAbsolutePath());
            }
        }
        try (final OutputStream os = Files.newOutputStream(keyStorePath, StandardOpenOption.WRITE)) {
            keyStore.store(os, keyStorePass);
        }
        if (useDefaultPass) {
            byte[] obfuscatedPass = SecretStoreUtil.asciiCharToBytes(SecretStoreUtil.obfuscate(keyStorePass.clone()));
            DataOutputStream dataOutputStream = new DataOutputStream(appendOs);
            appendOs.write(obfuscatedPass);
            // 1 byte integer
            dataOutputStream.write(obfuscatedPass.length);
        }
    } finally {
        if (fileLock != null && fileLock.isValid()) {
            fileLock.release();
        }
    }
}","/**
 * Saves the keystore with some extra meta data if needed. Note - need two output streams here to allow checking the with the append flag, and the other without an append.
 */
","// The keystore.store method on Windows checks for the file lock and does not allow _any_ interaction with the keystore if it is locked.
[[SEP]]// 1 byte integer
","/** * Saves the keystore with some extra meta data if needed. Note - need two output streams here to allow checking the with the append flag, and the other without an append. */[[SEP]]// The keystore.store method on Windows checks for the file lock and does not allow _any_ interaction with the keystore if it is locked.[[SEP]]// 1 byte integer",414,438,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,0,1,saveKeyStore(),org.logstash.secret.store.backend.JavaKeyStore,saveKeyStore/0,False,414,2,5,3,2,6,13,25,0,5,0,13,0,0,0,2,2,0,1,0,6,1,3,0,0,0,55,2,0,True
527,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\secret\store\backend\JavaKeyStore.java,org.logstash.secret.store.backend.JavaKeyStore,boolean valid(char[]),"/**
 * @param chars char[] to check for null or empty
 * @return true if not null, and not empty, false otherwise
 */
private boolean valid(char[] chars) {
    return !(chars == null || chars.length == 0);
}","/**
 * @param chars char[] to check for null or empty
 * @return true if not null, and not empty, false otherwise
 */
", ,"/** * @param chars char[] to check for null or empty * @return true if not null, and not empty, false otherwise */",444,446,[0],0,[0],0,[0],0,0,0,0,valid(char[]),org.logstash.secret.store.backend.JavaKeyStore,valid/1[char[]],False,444,1,2,2,0,3,0,3,1,0,1,0,0,0,0,2,0,1,0,1,0,0,0,0,0,0,12,2,0,True
528,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\util\CATrustedFingerprintTrustStrategy.java,org.logstash.util.CATrustedFingerprintTrustStrategy,"boolean isTrusted(X509Certificate[], String)","@Override
public boolean isTrusted(X509Certificate[] chain, String authType) throws CertificateException {
    if (chain.length == 0 || this.trustedFingerprints.isEmpty()) {
        return false;
    }
    final MessageDigest sha256Digest = sha256();
    // traverse up the chain until we find one whose fingerprint matches
    for (int i = 0; i < chain.length; i++) {
        final X509Certificate currentCandidate = chain[i];
        final byte[] derEncoding = currentCandidate.getEncoded();
        Fingerprint candidateFingerprint = new Fingerprint(sha256Digest.digest(derEncoding));
        if (this.trustedFingerprints.contains(candidateFingerprint)) {
            final Date currentDate = dateSupplier.get();
            currentCandidate.checkValidity(currentDate);
            // zip back down the chain and make sure everything is valid
            for (; i > 0; i--) {
                final X509Certificate signer = chain[i];
                final X509Certificate signed = chain[i - 1];
                verifyAndValidate(signed, signer, currentDate);
            }
            return true;
        }
    }
    return false;
}", ,"// traverse up the chain until we find one whose fingerprint matches
[[SEP]]// zip back down the chain and make sure everything is valid
",// traverse up the chain until we find one whose fingerprint matches[[SEP]]// zip back down the chain and make sure everything is valid,59,85,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,"isTrusted(X509Certificate[], String)",org.logstash.util.CATrustedFingerprintTrustStrategy,"isTrusted/2[java.security.cert.X509Certificate[],java.lang.String]",False,60,3,3,0,3,6,8,22,3,8,2,8,2,1,2,1,0,0,0,4,8,1,3,0,0,0,25,1,0,False
529,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\util\JavaVersion.java,org.logstash.util.JavaVersion,"int compare(JavaVersion, JavaVersion)","private static int compare(final JavaVersion leftVersion, final JavaVersion rightVersion) {
    List<Integer> left = leftVersion.version;
    List<Integer> right = rightVersion.version;
    // lexicographically compare two lists, treating missing entries as zeros
    final int len = Math.max(left.size(), right.size());
    for (int i = 0; i < len; i++) {
        final int l = (i < left.size()) ? left.get(i) : 0;
        final int r = (i < right.size()) ? right.get(i) : 0;
        if (l < r) {
            return -1;
        }
        if (r < l) {
            return 1;
        }
    }
    return 0;
}", ,"// lexicographically compare two lists, treating missing entries as zeros
","// lexicographically compare two lists, treating missing entries as zeros",62,78,[0],0,[0],0,[0],0,0,0,0,"compare(JavaVersion, JavaVersion)",org.logstash.util.JavaVersion,"compare/2[org.logstash.util.JavaVersion,org.logstash.util.JavaVersion]",False,62,1,1,1,0,6,3,16,3,6,2,3,0,0,1,0,0,2,0,6,6,0,2,0,0,0,9,10,0,False
530,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\util\ModulesSettingArray.java,org.logstash.util.ModulesSettingArray,"Map<String, Object> wrapPasswordsInSettings(Map<String, Object>)","private static Map<String, Object> wrapPasswordsInSettings(Map<String, Object> settings) {
    // Insertion order is important. The Map object passed into is usually a org.jruby.RubyHash, which preserves
    // the insertion order, during the scan. Here we need to keep the same order, because tests on modules
    // expects a precise order of keys. It's important to have stable tests.
    final Map<String, Object> acc = new LinkedHashMap<>();
    for (Map.Entry<String, Object> entry : settings.entrySet()) {
        if (entry.getKey().endsWith(""password"") && !(entry.getValue() instanceof Password)) {
            acc.put(entry.getKey(), new Password((String) entry.getValue()));
        } else {
            acc.put(entry.getKey(), entry.getValue());
        }
    }
    return acc;
}", ,"// Insertion order is important. The Map object passed into is usually a org.jruby.RubyHash, which preserves
[[SEP]]// the insertion order, during the scan. Here we need to keep the same order, because tests on modules
[[SEP]]// expects a precise order of keys. It's important to have stable tests.
","// Insertion order is important. The Map object passed into is usually a org.jruby.RubyHash, which preserves// the insertion order, during the scan. Here we need to keep the same order, because tests on modules// expects a precise order of keys. It's important to have stable tests.",50,63,[0],0,"[0, 0, 0]",0,[0],0,0,0,0,"wrapPasswordsInSettings(Map<String, Object>)",org.logstash.util.ModulesSettingArray,"wrapPasswordsInSettings/1[java.util.Map<java.lang.String,java.lang.Object>]",False,50,1,1,0,1,4,5,12,1,1,1,5,0,0,1,0,0,1,1,0,1,0,2,0,0,0,8,10,0,False
531,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\util\TimeValue.java,org.logstash.util.TimeValue,String getTimeUnit(),"public String getTimeUnit() {
    final String value = timeUnit.toString();
    // remove last ""s""
    return value.substring(0, value.length() - 1);
}", ,"// remove last ""s""
","// remove last ""s""",95,98,[0],0,[0],0,[0],0,0,0,0,getTimeUnit(),org.logstash.util.TimeValue,getTimeUnit/0,False,95,0,0,0,0,1,3,4,1,1,0,3,0,0,0,0,0,0,0,2,1,1,0,0,0,0,4,1,0,False
532,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\util\UtilExt.java,org.logstash.util.UtilExt,"IRubyObject get_thread_id(ThreadContext, IRubyObject, IRubyObject)","@JRubyMethod(module = true)
public static IRubyObject get_thread_id(final ThreadContext context, IRubyObject self, IRubyObject thread) {
    if (!(thread instanceof RubyThread)) {
        throw context.runtime.newTypeError(thread, context.runtime.getThread());
    }
    // weak-reference
    final Thread javaThread = ((RubyThread) thread).getNativeThread();
    // even if thread is dead the RubyThread instance might stick around while the Java thread
    // instance already could have been garbage collected - let's return nil for dead meat :
    return javaThread == null ? context.nil : context.runtime.newFixnum(javaThread.getId());
}", ,"// even if thread is dead the RubyThread instance might stick around while the Java thread
[[SEP]]// weak-reference
[[SEP]]// instance already could have been garbage collected - let's return nil for dead meat :
",// weak-reference[[SEP]]// even if thread is dead the RubyThread instance might stick around while the Java thread// instance already could have been garbage collected - let's return nil for dead meat :,36,45,[0],0,"[0, 0, 0]",0,"[0, 0]",0,0,0,0,"get_thread_id(ThreadContext, IRubyObject, IRubyObject)",org.logstash.util.UtilExt,"get_thread_id/3[org.logstash.util.ThreadContext,org.logstash.util.IRubyObject,org.logstash.util.IRubyObject]",False,37,4,0,0,0,3,5,7,1,1,3,5,0,0,0,1,0,2,0,0,1,0,1,0,0,0,12,9,0,False
533,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\util\UtilExt.java,org.logstash.util.UtilExt,"IRubyObject get_thread_name(ThreadContext, IRubyObject, IRubyObject)","@JRubyMethod(module = true)
public static IRubyObject get_thread_name(final ThreadContext context, IRubyObject self, IRubyObject thread) {
    if (!(thread instanceof RubyThread)) {
        throw context.runtime.newTypeError(thread, context.runtime.getThread());
    }
    // weak-reference
    final Thread javaThread = ((RubyThread) thread).getNativeThread();
    // even if thread is dead the RubyThread instance might stick around while the Java thread
    // instance already could have been garbage collected - let's return nil for dead meat :
    return javaThread == null ? context.nil : context.runtime.newString(javaThread.getName());
}", ,"// even if thread is dead the RubyThread instance might stick around while the Java thread
[[SEP]]// weak-reference
[[SEP]]// instance already could have been garbage collected - let's return nil for dead meat :
",// weak-reference[[SEP]]// even if thread is dead the RubyThread instance might stick around while the Java thread// instance already could have been garbage collected - let's return nil for dead meat :,47,56,[0],0,"[0, 0, 0]",0,"[0, 0]",0,0,0,0,"get_thread_name(ThreadContext, IRubyObject, IRubyObject)",org.logstash.util.UtilExt,"get_thread_name/3[org.logstash.util.ThreadContext,org.logstash.util.IRubyObject,org.logstash.util.IRubyObject]",False,48,4,0,0,0,3,5,7,1,1,3,5,0,0,0,1,0,2,0,0,1,0,1,0,0,0,12,9,0,False
534,..\projects\logstash-8.5.2\logstash-core\src\main\java\org\logstash\util\UtilExt.java,org.logstash.util.UtilExt,"IRubyObject synchronize(ThreadContext, IRubyObject, IRubyObject, Block)","// JRuby.reference(target).synchronized { ... }
@JRubyMethod(module = true)
public static IRubyObject synchronize(final ThreadContext context, IRubyObject self, IRubyObject target, Block block) {
    synchronized (target) {
        return block.yieldSpecific(context);
    }
}", ,"// JRuby.reference(target).synchronized { ... }
",// JRuby.reference(target).synchronized { ... },58,63,[0],0,[0],0,[0],0,0,0,0,"synchronize(ThreadContext, IRubyObject, IRubyObject, Block)",org.logstash.util.UtilExt,"synchronize/4[org.logstash.util.ThreadContext,org.logstash.util.IRubyObject,org.logstash.util.IRubyObject,org.logstash.util.Block]",False,59,4,0,0,0,1,1,5,1,0,4,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,11,9,0,False
535,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\AccessorsTest.java,org.logstash.AccessorsTest,void testInvalidIdList(),"/*
     * Check if accessors are able to recovery from
     * failure to convert the key (string) to integer,
     * when it is a non-numeric value, which is not
     * expected.
     */
@Test
public void testInvalidIdList() throws Exception {
    final ConvertedMap data = new ConvertedMap(1);
    List<Object> inner = new ConvertedList(2);
    data.put(""map1"", inner);
    inner.add(""obj1"");
    inner.add(""obj2"");
    String reference = ""[map1][IdNonNumeric]"";
    assertNull(get(data, reference));
    assertNull(set(data, reference, ""obj3""));
    assertFalse(includes(data, reference));
    assertNull(del(data, reference));
}","/*
     * Check if accessors are able to recovery from
     * failure to convert the key (string) to integer,
     * when it is a non-numeric value, which is not
     * expected.
     */
", ,"/*     * Check if accessors are able to recovery from     * failure to convert the key (string) to integer,     * when it is a non-numeric value, which is not     * expected.     */",114,128,[0],0,[0],0,[0],0,0,0,0,testInvalidIdList(),org.logstash.AccessorsTest,testInvalidIdList/0,False,115,4,7,0,7,1,8,12,0,3,0,8,4,1,0,0,0,0,5,2,3,0,0,0,0,0,17,1,0,False
536,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ClonerTest.java,org.logstash.ClonerTest,void testRubyStringCloning(),"@Test
public void testRubyStringCloning() {
    String javaString = ""fooBar"";
    RubyString original = RubyString.newString(RubyUtil.RUBY, javaString);
    RubyString result = Cloner.deep(original);
    // Check object identity
    assertNotSame(original, result);
    // Check string equality
    assertEquals(original, result);
    assertEquals(javaString, result.asJavaString());
}", ,"// Check object identity
[[SEP]]// Check string equality
",// Check object identity[[SEP]]// Check string equality,32,44,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testRubyStringCloning(),org.logstash.ClonerTest,testRubyStringCloning/0,False,33,3,1,0,1,1,5,8,0,3,0,5,0,0,0,0,0,0,1,0,3,0,0,0,0,0,11,1,0,False
537,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ClonerTest.java,org.logstash.ClonerTest,void testRubyStringCloningAndChangeOriginal(),"@Test
public void testRubyStringCloningAndChangeOriginal() {
    String javaString = ""fooBar"";
    RubyString original = RubyString.newString(RubyUtil.RUBY, javaString);
    RubyString result = Cloner.deep(original);
    ThreadContext context = RubyUtil.RUBY.getCurrentContext();
    IRubyObject index = RubyUtil.RUBY.newFixnum(5);
    // original[5] = 'z'
    original.op_aset(context, index, RubyUtil.RUBY.newString(""z""));
    assertNotEquals(result, original);
    assertTrue(result.op_equal(context, RubyString.newString(RubyUtil.RUBY, javaString)).isTrue());
    assertEquals(javaString, result.asJavaString());
    assertEquals(""fooBaz"", original.asJavaString());
}", ,"// original[5] = 'z'
",// original[5] = 'z',62,78,[0],0,[0],0,[0],0,0,0,0,testRubyStringCloningAndChangeOriginal(),org.logstash.ClonerTest,testRubyStringCloningAndChangeOriginal/0,False,63,5,1,0,1,1,11,12,0,5,0,11,0,0,0,0,0,0,3,1,5,0,0,0,0,0,20,1,0,False
538,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ClonerTest.java,org.logstash.ClonerTest,void testRubyStringCloningMemoryOptimization(),"// @Tag(""Performance Optimization"")
@Test
public void testRubyStringCloningMemoryOptimization() {
    ByteList bytes = ByteList.create(""0123456789"");
    RubyString original = RubyString.newString(RubyUtil.RUBY, bytes);
    RubyString result = Cloner.deep(original);
    assertNotSame(original, result);
    assertSame(bytes, original.getByteList());
    // NOTE: this is an implementation detail or the underlying sharing :
    // bytes-list shared
    assertSame(bytes, result.getByteList());
    // but when string is modified it will stop using the same byte container
    result.concat(RubyUtil.RUBY.getCurrentContext(), RubyUtil.RUBY.newString("" ""));
    // byte-list copied on write
    assertNotSame(bytes, result.getByteList());
}", ,"// @Tag(""Performance Optimization"")
[[SEP]]// NOTE: this is an implementation detail or the underlying sharing :
[[SEP]]// bytes-list shared
[[SEP]]// but when string is modified it will stop using the same byte container
[[SEP]]// byte-list copied on write
","// @Tag(""Performance Optimization"")[[SEP]]// NOTE: this is an implementation detail or the underlying sharing :// bytes-list shared[[SEP]]// but when string is modified it will stop using the same byte container[[SEP]]// byte-list copied on write",80,95,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,testRubyStringCloningMemoryOptimization(),org.logstash.ClonerTest,testRubyStringCloningMemoryOptimization/0,False,81,4,1,0,1,1,8,10,0,3,0,8,0,0,0,0,0,0,2,0,3,0,0,0,0,0,14,1,0,False
539,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\EventTest.java,org.logstash.EventTest,void bigNumsBinaryRoundtrip(),"/**
 * Test for proper BigInteger and BigDecimal serialization
 * related to Jackson/CBOR issue https://github.com/elastic/logstash/issues/8379
 */
@Test
public void bigNumsBinaryRoundtrip() throws Exception {
    final Event e = new Event();
    final BigInteger bi = new BigInteger(""9223372036854776000"");
    final BigDecimal bd = new BigDecimal(""9223372036854776001.99"");
    e.setField(""bi"", bi);
    e.setField(""bd"", bd);
    final Event deserialized = Event.deserialize(e.serialize());
    assertEquals(bi, deserialized.getField(""bi""));
    assertEquals(bd, deserialized.getField(""bd""));
}","/**
 * Test for proper BigInteger and BigDecimal serialization
 * related to Jackson/CBOR issue https://github.com/elastic/logstash/issues/8379
 */
", ,/** * Test for proper BigInteger and BigDecimal serialization * related to Jackson/CBOR issue https://github.com/elastic/logstash/issues/8379 */,133,143,[0],0,[0],0,[0],0,0,0,0,bigNumsBinaryRoundtrip(),org.logstash.EventTest,bigNumsBinaryRoundtrip/0,False,134,2,5,0,5,1,5,10,0,4,0,5,0,0,0,0,0,0,6,0,4,0,0,0,0,0,30,1,0,True
540,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\FieldReferenceTest.java,org.logstash.FieldReferenceTest.Base,void restoreGlobalEscapeMode(),"@After
public void restoreGlobalEscapeMode() {
    // Default value for `config.field_reference.escape_style`
    FieldReference.setEscapeStyle(""none"");
}", ,"// Default value for `config.field_reference.escape_style`
",// Default value for `config.field_reference.escape_style`,53,57,[0],0,[0],0,[0],0,0,0,0,restoreGlobalEscapeMode(),org.logstash.FieldReferenceTest$Base,restoreGlobalEscapeMode/0,False,54,2,1,0,1,1,1,3,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,4,1,0,False
541,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\FileLockFactoryMain.java,org.logstash.FileLockFactoryMain,void main(String[]),"public static void main(String[] args) {
    try {
        FileLockFactory.obtainLock(Paths.get(args[0]), args[1]);
        System.out.println(""File locked"");
        // Sleep enough time until this process is killed.
        Thread.sleep(Long.MAX_VALUE);
    } catch (InterruptedException e) {
        // This process is killed. Do nothing.
    } catch (IOException e) {
        // Failed to obtain the lock.
        System.exit(1);
    }
}", ,"// Sleep enough time until this process is killed.
[[SEP]]// This process is killed. Do nothing.
[[SEP]]// Failed to obtain the lock.
",// Sleep enough time until this process is killed.[[SEP]]// This process is killed. Do nothing.[[SEP]]// Failed to obtain the lock.,31,43,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,main(String[]),org.logstash.FileLockFactoryMain,main/1[java.lang.String[]],False,31,1,1,0,1,3,5,12,0,0,1,5,0,0,0,0,1,0,1,3,0,0,1,0,0,0,7,9,0,False
542,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\FileLockFactoryTest.java,org.logstash.FileLockFactoryTest,void ObtainLockOnNonLocked(),"@Test
public void ObtainLockOnNonLocked() throws IOException {
    // empty to just test the lone @Before lockFirst() test
}", ,"// empty to just test the lone @Before lockFirst() test
",// empty to just test the lone @Before lockFirst() test,77,80,[0],0,[0],0,[0],0,0,0,0,ObtainLockOnNonLocked(),org.logstash.FileLockFactoryTest,ObtainLockOnNonLocked/0,False,78,1,0,0,0,1,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,1,0,False
543,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\FileLockFactoryTest.java,org.logstash.FileLockFactoryTest,void crossJvmObtainLockOnLocked(),"@Test
public void crossJvmObtainLockOnLocked() throws Exception {
    Process p = null;
    String lockFile = "".testCrossJvm"";
    FileLock lock = null;
    // Build the command to spawn a children JVM.
    String[] cmd = { Paths.get(System.getProperty(""java.home""), ""bin"", ""java"").toString(), ""-cp"", System.getProperty(""java.class.path""), Class.forName(""org.logstash.FileLockFactoryMain"").getName(), lockDir.toString(), lockFile };
    try {
        // Start the children program that will lock the file.
        p = new ProcessBuilder(cmd).start();
        InputStream is = p.getInputStream();
        /* Wait the children program write to stdout, meaning the file
             * is locked. Set a timeout to ensure it returns.
             */
        Future<Integer> future = executor.submit(() -> {
            return is.read();
        });
        assertTrue(future.get(30, TimeUnit.SECONDS) > -1);
        // Check the children process is still running.
        assertThat(p.isAlive(), is(equalTo(true)));
        try {
            // Try to obtain the lock held by the children process.
            FileLockFactory.obtainLock(lockDir, lockFile);
            fail(""Should have threw an exception"");
        } catch (LockException e) {
            // Expected exception as the file is already locked.
        }
    } finally {
        if (p != null) {
            p.destroy();
        }
    }
}", ,"// Build the command to spawn a children JVM.
[[SEP]]// Start the children program that will lock the file.
[[SEP]]/* Wait the children program write to stdout, meaning the file
             * is locked. Set a timeout to ensure it returns.
             */
[[SEP]]// Check the children process is still running.
[[SEP]]// Try to obtain the lock held by the children process.
[[SEP]]// Expected exception as the file is already locked.
","// Build the command to spawn a children JVM.[[SEP]]// Start the children program that will lock the file.[[SEP]]/* Wait the children program write to stdout, meaning the file             * is locked. Set a timeout to ensure it returns.             */[[SEP]]// Check the children process is still running.[[SEP]]// Try to obtain the lock held by the children process.[[SEP]]// Expected exception as the file is already locked.",130,169,[0],0,"[0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0]",0,0,0,0,crossJvmObtainLockOnLocked(),org.logstash.FileLockFactoryTest,crossJvmObtainLockOnLocked/0,False,131,2,1,0,1,4,18,27,1,6,0,18,0,0,0,1,2,0,8,2,7,0,2,0,0,1,29,1,0,False
544,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\RubyfierTest.java,org.logstash.RubyfierTest,void testDeepMapWithString(),"@Test
public void testDeepMapWithString() throws Exception {
    Map<String, String> data = new HashMap<>();
    data.put(""foo"", ""bar"");
    RubyHash rubyHash = (RubyHash) Rubyfier.deep(RubyUtil.RUBY, data);
    // Hack to be able to retrieve the original, unconverted Ruby object from Map
    // it seems the only method providing this is internalGet but it is declared protected.
    // I know this is bad practice but I think this is practically acceptable.
    Method internalGet = RubyHash.class.getDeclaredMethod(""internalGet"", IRubyObject.class);
    internalGet.setAccessible(true);
    Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(RubyUtil.RUBY, ""foo""));
    assertEquals(RubyString.class, result.getClass());
    assertEquals(""bar"", result.toString());
}", ,"// Hack to be able to retrieve the original, unconverted Ruby object from Map
[[SEP]]// it seems the only method providing this is internalGet but it is declared protected.
[[SEP]]// I know this is bad practice but I think this is practically acceptable.
","// Hack to be able to retrieve the original, unconverted Ruby object from Map// it seems the only method providing this is internalGet but it is declared protected.// I know this is bad practice but I think this is practically acceptable.",53,68,[0],0,"[1, 0, 1]",1,[1],1,1,1,1,testDeepMapWithString(),org.logstash.RubyfierTest,testDeepMapWithString/0,False,54,5,1,0,1,1,9,10,0,4,0,9,0,0,0,0,0,0,5,0,4,0,0,0,0,0,18,1,0,False
545,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\RubyfierTest.java,org.logstash.RubyfierTest,void testDeepListWithString(),"@Test
public void testDeepListWithString() throws Exception {
    List<String> data = new ArrayList<>();
    data.add(""foo"");
    @SuppressWarnings(""rawtypes"")
    RubyArray rubyArray = (RubyArray) Rubyfier.deep(RubyUtil.RUBY, data);
    // toJavaArray does not newFromRubyArray inner elements to Java types \o/
    assertEquals(RubyString.class, rubyArray.toJavaArray()[0].getClass());
    assertEquals(""foo"", rubyArray.toJavaArray()[0].toString());
}", ,"// toJavaArray does not newFromRubyArray inner elements to Java types \o/
",// toJavaArray does not newFromRubyArray inner elements to Java types \o/,70,81,[0],0,[0],0,[0],0,0,0,0,testDeepListWithString(),org.logstash.RubyfierTest,testDeepListWithString/0,False,71,4,1,0,1,1,6,7,0,2,0,6,0,0,0,0,0,0,3,2,2,0,0,0,0,0,12,1,0,False
546,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\RubyfierTest.java,org.logstash.RubyfierTest,void testDeepMapWithInteger(),"@Test
public void testDeepMapWithInteger() throws Exception {
    Map<String, Integer> data = new HashMap<>();
    data.put(""foo"", 1);
    RubyHash rubyHash = (RubyHash) Rubyfier.deep(RubyUtil.RUBY, data);
    // Hack to be able to retrieve the original, unconverted Ruby object from Map
    // it seems the only method providing this is internalGet but it is declared protected.
    // I know this is bad practice but I think this is practically acceptable.
    Method internalGet = RubyHash.class.getDeclaredMethod(""internalGet"", IRubyObject.class);
    internalGet.setAccessible(true);
    Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(RubyUtil.RUBY, ""foo""));
    assertEquals(RubyFixnum.class, result.getClass());
    assertEquals(1L, ((RubyFixnum) result).getLongValue());
}", ,"// Hack to be able to retrieve the original, unconverted Ruby object from Map
[[SEP]]// it seems the only method providing this is internalGet but it is declared protected.
[[SEP]]// I know this is bad practice but I think this is practically acceptable.
","// Hack to be able to retrieve the original, unconverted Ruby object from Map// it seems the only method providing this is internalGet but it is declared protected.// I know this is bad practice but I think this is practically acceptable.",90,105,[0],0,"[1, 0, 1]",1,[1],1,1,1,1,testDeepMapWithInteger(),org.logstash.RubyfierTest,testDeepMapWithInteger/0,False,91,5,1,0,1,1,9,10,0,4,0,9,0,0,0,0,0,1,3,2,4,0,0,0,0,0,19,1,0,False
547,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\RubyfierTest.java,org.logstash.RubyfierTest,void testDeepListWithInteger(),"@Test
public void testDeepListWithInteger() throws Exception {
    List<Integer> data = new ArrayList<>();
    data.add(1);
    @SuppressWarnings(""rawtypes"")
    RubyArray rubyArray = (RubyArray) Rubyfier.deep(RubyUtil.RUBY, data);
    // toJavaArray does not newFromRubyArray inner elements to Java types \o/
    assertEquals(RubyFixnum.class, rubyArray.toJavaArray()[0].getClass());
    assertEquals(1L, ((RubyFixnum) rubyArray.toJavaArray()[0]).getLongValue());
}", ,"// toJavaArray does not newFromRubyArray inner elements to Java types \o/
",// toJavaArray does not newFromRubyArray inner elements to Java types \o/,107,118,[0],0,[0],0,[0],0,0,0,0,testDeepListWithInteger(),org.logstash.RubyfierTest,testDeepListWithInteger/0,False,108,4,1,0,1,1,6,7,0,2,0,6,0,0,0,0,0,1,1,4,2,0,0,0,0,0,13,1,0,False
548,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\RubyfierTest.java,org.logstash.RubyfierTest,void testDeepMapWithFloat(),"@Test
public void testDeepMapWithFloat() throws Exception {
    Map<String, Float> data = new HashMap<>();
    data.put(""foo"", 1.0F);
    RubyHash rubyHash = (RubyHash) Rubyfier.deep(RubyUtil.RUBY, data);
    // Hack to be able to retrieve the original, unconverted Ruby object from Map
    // it seems the only method providing this is internalGet but it is declared protected.
    // I know this is bad practice but I think this is practically acceptable.
    Method internalGet = RubyHash.class.getDeclaredMethod(""internalGet"", IRubyObject.class);
    internalGet.setAccessible(true);
    Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(RubyUtil.RUBY, ""foo""));
    assertEquals(RubyFloat.class, result.getClass());
    assertEquals(1.0D, ((RubyFloat) result).getDoubleValue(), 0);
}", ,"// Hack to be able to retrieve the original, unconverted Ruby object from Map
[[SEP]]// it seems the only method providing this is internalGet but it is declared protected.
[[SEP]]// I know this is bad practice but I think this is practically acceptable.
","// Hack to be able to retrieve the original, unconverted Ruby object from Map// it seems the only method providing this is internalGet but it is declared protected.// I know this is bad practice but I think this is practically acceptable.",127,142,[0],0,"[1, 0, 1]",1,[1],1,1,1,1,testDeepMapWithFloat(),org.logstash.RubyfierTest,testDeepMapWithFloat/0,False,128,5,1,0,1,1,9,10,0,4,0,9,0,0,0,0,0,1,3,3,4,0,0,0,0,0,18,1,0,False
549,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\RubyfierTest.java,org.logstash.RubyfierTest,void testDeepListWithFloat(),"@Test
public void testDeepListWithFloat() throws Exception {
    List<Float> data = new ArrayList<>();
    data.add(1.0F);
    @SuppressWarnings(""rawtypes"")
    RubyArray rubyArray = (RubyArray) Rubyfier.deep(RubyUtil.RUBY, data);
    // toJavaArray does not newFromRubyArray inner elements to Java types \o/
    assertEquals(RubyFloat.class, rubyArray.toJavaArray()[0].getClass());
    assertEquals(1.0D, ((RubyFloat) rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
}", ,"// toJavaArray does not newFromRubyArray inner elements to Java types \o/
",// toJavaArray does not newFromRubyArray inner elements to Java types \o/,144,155,[0],0,[0],0,[0],0,0,0,0,testDeepListWithFloat(),org.logstash.RubyfierTest,testDeepListWithFloat/0,False,145,4,1,0,1,1,6,7,0,2,0,6,0,0,0,0,0,1,1,5,2,0,0,0,0,0,12,1,0,False
550,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\RubyfierTest.java,org.logstash.RubyfierTest,void testDeepMapWithDouble(),"@Test
public void testDeepMapWithDouble() throws Exception {
    Map<String, Double> data = new HashMap<>();
    data.put(""foo"", 1.0D);
    RubyHash rubyHash = (RubyHash) Rubyfier.deep(RubyUtil.RUBY, data);
    // Hack to be able to retrieve the original, unconverted Ruby object from Map
    // it seems the only method providing this is internalGet but it is declared protected.
    // I know this is bad practice but I think this is practically acceptable.
    Method internalGet = RubyHash.class.getDeclaredMethod(""internalGet"", IRubyObject.class);
    internalGet.setAccessible(true);
    Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(RubyUtil.RUBY, ""foo""));
    assertEquals(RubyFloat.class, result.getClass());
    assertEquals(1.0D, ((RubyFloat) result).getDoubleValue(), 0);
}", ,"// Hack to be able to retrieve the original, unconverted Ruby object from Map
[[SEP]]// it seems the only method providing this is internalGet but it is declared protected.
[[SEP]]// I know this is bad practice but I think this is practically acceptable.
","// Hack to be able to retrieve the original, unconverted Ruby object from Map// it seems the only method providing this is internalGet but it is declared protected.// I know this is bad practice but I think this is practically acceptable.",164,179,[0],0,"[1, 0, 1]",1,[1],1,1,1,1,testDeepMapWithDouble(),org.logstash.RubyfierTest,testDeepMapWithDouble/0,False,165,5,1,0,1,1,9,10,0,4,0,9,0,0,0,0,0,1,3,3,4,0,0,0,0,0,19,1,0,False
551,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\RubyfierTest.java,org.logstash.RubyfierTest,void testDeepListWithDouble(),"@Test
public void testDeepListWithDouble() throws Exception {
    List<Double> data = new ArrayList<>();
    data.add(1.0D);
    @SuppressWarnings(""rawtypes"")
    RubyArray rubyArray = (RubyArray) Rubyfier.deep(RubyUtil.RUBY, data);
    // toJavaArray does not newFromRubyArray inner elements to Java types \o/
    assertEquals(RubyFloat.class, rubyArray.toJavaArray()[0].getClass());
    assertEquals(1.0D, ((RubyFloat) rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
}", ,"// toJavaArray does not newFromRubyArray inner elements to Java types \o/
",// toJavaArray does not newFromRubyArray inner elements to Java types \o/,181,192,[0],0,[0],0,[0],0,0,0,0,testDeepListWithDouble(),org.logstash.RubyfierTest,testDeepListWithDouble/0,False,182,4,1,0,1,1,6,7,0,2,0,6,0,0,0,0,0,1,1,5,2,0,0,0,0,0,13,1,0,False
552,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\RubyfierTest.java,org.logstash.RubyfierTest,void testDeepMapWithBigDecimal(),"@Test
public void testDeepMapWithBigDecimal() throws Exception {
    Map<String, BigDecimal> data = new HashMap<>();
    data.put(""foo"", new BigDecimal(1));
    RubyHash rubyHash = (RubyHash) Rubyfier.deep(RubyUtil.RUBY, data);
    // Hack to be able to retrieve the original, unconverted Ruby object from Map
    // it seems the only method providing this is internalGet but it is declared protected.
    // I know this is bad practice but I think this is practically acceptable.
    Method internalGet = RubyHash.class.getDeclaredMethod(""internalGet"", IRubyObject.class);
    internalGet.setAccessible(true);
    Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(RubyUtil.RUBY, ""foo""));
    assertEquals(RubyBigDecimal.class, result.getClass());
    assertEquals(1.0D, ((RubyBigDecimal) result).getDoubleValue(), 0);
}", ,"// Hack to be able to retrieve the original, unconverted Ruby object from Map
[[SEP]]// it seems the only method providing this is internalGet but it is declared protected.
[[SEP]]// I know this is bad practice but I think this is practically acceptable.
","// Hack to be able to retrieve the original, unconverted Ruby object from Map// it seems the only method providing this is internalGet but it is declared protected.// I know this is bad practice but I think this is practically acceptable.",201,217,[0],0,"[1, 0, 1]",1,[1],1,1,1,1,testDeepMapWithBigDecimal(),org.logstash.RubyfierTest,testDeepMapWithBigDecimal/0,False,202,5,1,0,1,1,9,10,0,4,0,9,0,0,0,0,0,1,3,3,4,0,0,0,0,0,19,1,0,False
553,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\RubyfierTest.java,org.logstash.RubyfierTest,void testDeepListWithBigDecimal(),"@Test
public void testDeepListWithBigDecimal() throws Exception {
    List<BigDecimal> data = new ArrayList<>();
    data.add(new BigDecimal(1));
    @SuppressWarnings(""rawtypes"")
    RubyArray rubyArray = (RubyArray) Rubyfier.deep(RubyUtil.RUBY, data);
    // toJavaArray does not newFromRubyArray inner elements to Java types \o/
    assertEquals(RubyBigDecimal.class, rubyArray.toJavaArray()[0].getClass());
    assertEquals(1.0D, ((RubyBigDecimal) rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
}", ,"// toJavaArray does not newFromRubyArray inner elements to Java types \o/
",// toJavaArray does not newFromRubyArray inner elements to Java types \o/,219,230,[0],0,[0],0,[0],0,0,0,0,testDeepListWithBigDecimal(),org.logstash.RubyfierTest,testDeepListWithBigDecimal/0,False,220,4,1,0,1,1,6,7,0,2,0,6,0,0,0,0,0,1,1,5,2,0,0,0,0,0,13,1,0,False
554,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\StringInterpolationTest.java,org.logstash.StringInterpolationTest,void TestEpochSeconds(),"@Test
public void TestEpochSeconds() throws IOException {
    Event event = getTestEvent();
    String path = ""%{+%ss}"";
    // `+%ss` bypasses the EPOCH syntax and instead matches the JODA syntax.
    // which produces the literal `%` followed by a two-s seconds value `00`
    assertEquals(""%00"", StringInterpolation.evaluate(event, path));
}", ,"// `+%ss` bypasses the EPOCH syntax and instead matches the JODA syntax.
[[SEP]]// which produces the literal `%` followed by a two-s seconds value `00`
",// `+%ss` bypasses the EPOCH syntax and instead matches the JODA syntax.// which produces the literal `%` followed by a two-s seconds value `00`,115,122,[0],0,"[0, 0]",0,[0],0,0,0,0,TestEpochSeconds(),org.logstash.StringInterpolationTest,TestEpochSeconds/0,False,116,4,2,0,2,1,3,5,0,2,0,3,1,1,0,0,0,0,2,0,2,0,0,0,0,0,12,1,0,False
555,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\TimestampTest.java,org.logstash.TimestampTest,void testCircularIso8601(),"@Test
@SuppressWarnings({ ""deprecation"" })
public void testCircularIso8601() throws Exception {
    Timestamp t1 = new Timestamp();
    Timestamp t2 = new Timestamp(t1.toString());
    // noinspection deprecation
    assertEquals(t1.getTime(), t2.getTime());
    assertEquals(t1.toInstant(), t2.toInstant());
}", ,"// noinspection deprecation
",// noinspection deprecation,41,49,[0],0,[0],0,[0],0,0,0,0,testCircularIso8601(),org.logstash.TimestampTest,testCircularIso8601/0,False,43,2,4,0,4,1,4,6,0,2,0,4,0,0,0,0,0,0,1,0,2,0,0,0,0,0,9,1,0,False
556,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\TimestampTest.java,org.logstash.TimestampTest,void testParsingDateTimeWithCommaDecimalStyleLocale(),"@Test
public void testParsingDateTimeWithCommaDecimalStyleLocale() throws Exception {
    final Locale germanLocale = Locale.GERMANY;
    // DST doesn't matter
    final Clock germanClock = Clock.systemUTC().withZone(ZoneId.of(""+02:00""));
    // comma-decimal
    final Timestamp t1 = new Timestamp(""2014-09-23T13:49:52,987654321Z"", germanClock, germanLocale);
    assertEquals(""2014-09-23T13:49:52.987654321Z"", t1.toString());
    // fallback to stop-decimal
    final Timestamp t2 = new Timestamp(""2014-09-23T13:49:52.987654321Z"", germanClock, germanLocale);
    assertEquals(""2014-09-23T13:49:52.987654321Z"", t2.toString());
}", ,"// DST doesn't matter
[[SEP]]// comma-decimal
[[SEP]]// fallback to stop-decimal
",// DST doesn't matter[[SEP]]// comma-decimal[[SEP]]// fallback to stop-decimal,86,98,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,testParsingDateTimeWithCommaDecimalStyleLocale(),org.logstash.TimestampTest,testParsingDateTimeWithCommaDecimalStyleLocale/0,False,87,2,2,0,2,1,5,8,0,4,0,5,0,0,0,0,0,0,5,0,4,0,0,0,0,0,17,1,0,False
557,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\TimestampTest.java,org.logstash.TimestampTest,void testUTC(),"// Timestamp should always be in a UTC representation
// TODO: remove spec, since `Instant` is UTC by default.
@Test
@SuppressWarnings({ ""deprecation"" })
public void testUTC() throws Exception {
    Timestamp t;
    t = new Timestamp();
    // noinspection deprecation
    assertEquals(DateTimeZone.UTC, t.getTime().getZone());
    t = new Timestamp(""2014-09-23T00:00:00-0800"");
    // noinspection deprecation
    assertEquals(DateTimeZone.UTC, t.getTime().getZone());
    t = new Timestamp(""2014-09-23T08:00:00.000Z"");
    // noinspection deprecation
    assertEquals(DateTimeZone.UTC, t.getTime().getZone());
    long ms = DateTime.now(DateTimeZone.forID(""EST"")).getMillis();
    t = new Timestamp(ms);
    // noinspection deprecation
    assertEquals(DateTimeZone.UTC, t.getTime().getZone());
}","// TODO: remove spec, since `Instant` is UTC by default.
","// noinspection deprecation
[[SEP]]// noinspection deprecation
[[SEP]]// noinspection deprecation
[[SEP]]// noinspection deprecation
","// Timestamp should always be in a UTC representation// TODO: remove spec, since `Instant` is UTC by default.[[SEP]]// noinspection deprecation[[SEP]]// noinspection deprecation[[SEP]]// noinspection deprecation[[SEP]]// noinspection deprecation",102,123,[1],1,"[0, 0, 0, 0]",0,"[1, 0, 0, 0, 0]",1,1,1,1,testUTC(),org.logstash.TimestampTest,testUTC/0,False,104,2,4,0,4,1,6,12,0,2,0,6,0,0,0,0,0,0,4,0,5,0,0,0,0,0,10,1,0,False
558,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\TimestampTest.java,org.logstash.TimestampTest,void testMicroseconds(),"@Test
public void testMicroseconds() {
    Instant i = Instant.now();
    Timestamp t1 = new Timestamp(i.toEpochMilli());
    long usec = t1.usec();
    // since our Timestamp was created with epoch millis, it cannot be more precise.
    Assert.assertEquals(i.getNano() / 1_000_000, usec / 1_000);
}", ,"// since our Timestamp was created with epoch millis, it cannot be more precise.
","// since our Timestamp was created with epoch millis, it cannot be more precise.",125,133,[0],0,[0],0,[0],0,0,0,0,testMicroseconds(),org.logstash.TimestampTest,testMicroseconds/0,False,126,2,2,0,2,1,5,6,0,3,0,5,0,0,0,0,0,0,0,2,3,2,0,0,0,0,9,1,0,False
559,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\HeadPageTest.java,org.logstash.ackedqueue.HeadPageTest,void newHeadPage(),"@Test
public void newHeadPage() throws IOException {
    Settings s = TestSettings.persistedQueueSettings(100, dataPath);
    // Close method on Page requires an instance of Queue that has already been opened.
    try (Queue q = new Queue(s)) {
        q.open();
        PageIO pageIO = new MmapPageIOV2(0, 100, Paths.get(dataPath));
        pageIO.create();
        try (final Page p = PageFactory.newHeadPage(0, q, pageIO)) {
            assertThat(p.getPageNum(), is(equalTo(0)));
            assertThat(p.isFullyRead(), is(true));
            assertThat(p.isFullyAcked(), is(false));
            assertThat(p.hasSpace(10), is(true));
            assertThat(p.hasSpace(100), is(false));
        }
    }
}", ,"// Close method on Page requires an instance of Queue that has already been opened.
",// Close method on Page requires an instance of Queue that has already been opened.,50,66,[0],0,[0],0,[0],0,0,0,0,newHeadPage(),org.logstash.ackedqueue.HeadPageTest,newHeadPage/0,False,51,8,10,0,10,1,12,15,0,4,0,12,0,0,0,0,2,0,0,7,4,0,2,0,0,0,22,1,0,False
560,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void writeToFullyAckedHeadpage(),"/**
 * This test guards against issue https://github.com/elastic/logstash/pull/8186 by ensuring
 * that repeated writes to an already fully acknowledged headpage do not corrupt the queue's
 * internal bytesize counter.
 * @throws IOException On Failure
 */
@Test(timeout = 5000)
public void writeToFullyAckedHeadpage() throws IOException {
    final Queueable element = new StringElement(""foobarbaz"");
    final int page = element.serialize().length * 2 + MmapPageIOV2.MIN_CAPACITY;
    // Queue that can only hold one element per page.
    try (Queue q = new Queue(TestSettings.persistedQueueSettings(page, page * 2 - 1, dataPath))) {
        q.open();
        for (int i = 0; i < 5; ++i) {
            q.write(element);
            try (Batch b = q.readBatch(1, 500L)) {
                assertThat(b.getElements().size(), is(1));
                assertThat(b.getElements().get(0).toString(), is(element.toString()));
            }
        }
        assertThat(q.nonBlockReadBatch(1), nullValue());
    }
}","/**
 * This test guards against issue https://github.com/elastic/logstash/pull/8186 by ensuring
 * that repeated writes to an already fully acknowledged headpage do not corrupt the queue's
 * internal bytesize counter.
 * @throws IOException On Failure
 */
","// Queue that can only hold one element per page.
",/** * This test guards against issue https://github.com/elastic/logstash/pull/8186 by ensuring * that repeated writes to an already fully acknowledged headpage do not corrupt the queue's * internal bytesize counter. * @throws IOException On Failure */[[SEP]]// Queue that can only hold one element per page.,117,134,[0],0,[0],0,"[0, 0]",0,0,0,0,writeToFullyAckedHeadpage(),org.logstash.ackedqueue.QueueTest,writeToFullyAckedHeadpage/0,False,118,6,9,0,9,2,13,15,0,5,0,13,0,0,1,0,2,0,1,11,5,4,3,0,0,0,47,1,0,True
561,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void writeWhenPageEqualsQueueSize(),"/**
 * This test ensures that the {@link Queue} functions properly when pagesize is equal to overall
 * queue size (i.e. there is only a single page).
 * @throws IOException On Failure
 */
@Test(timeout = 5000)
public void writeWhenPageEqualsQueueSize() throws IOException {
    final Queueable element = new StringElement(""foobarbaz"");
    // Queue that can only hold one element per page.
    try (Queue q = new Queue(TestSettings.persistedQueueSettings(1024, 1024L, dataPath))) {
        q.open();
        for (int i = 0; i < 3; ++i) {
            q.write(element);
            try (Batch b = q.readBatch(1, 500L)) {
                assertThat(b.getElements().size(), is(1));
                assertThat(b.getElements().get(0).toString(), is(element.toString()));
            }
        }
        assertThat(q.nonBlockReadBatch(1), nullValue());
    }
}","/**
 * This test ensures that the {@link Queue} functions properly when pagesize is equal to overall
 * queue size (i.e. there is only a single page).
 * @throws IOException On Failure
 */
","// Queue that can only hold one element per page.
",/** * This test ensures that the {@link Queue} functions properly when pagesize is equal to overall * queue size (i.e. there is only a single page). * @throws IOException On Failure */[[SEP]]// Queue that can only hold one element per page.,153,169,[0],0,[0],0,"[0, 0]",0,0,0,0,writeWhenPageEqualsQueueSize(),org.logstash.ackedqueue.QueueTest,writeWhenPageEqualsQueueSize/0,False,154,6,8,0,8,2,12,14,0,4,0,12,0,0,1,0,2,0,1,10,4,0,3,0,0,0,41,1,0,True
562,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void writeMultiPage(),"@Test
public void writeMultiPage() throws IOException {
    List<Queueable> elements = Arrays.asList(new StringElement(""foobarbaz1""), new StringElement(""foobarbaz2""), new StringElement(""foobarbaz3""), new StringElement(""foobarbaz4""));
    try (Queue q = new Queue(TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(elements.get(0), 2), dataPath))) {
        q.open();
        for (Queueable e : elements) {
            q.write(e);
        }
        // total of 2 pages: 1 head and 1 tail
        assertThat(q.tailPages.size(), is(1));
        assertThat(q.tailPages.get(0).isFullyRead(), is(false));
        assertThat(q.tailPages.get(0).isFullyAcked(), is(false));
        assertThat(q.headPage.isFullyRead(), is(false));
        assertThat(q.headPage.isFullyAcked(), is(false));
        Batch b = q.nonBlockReadBatch(10);
        assertThat(b.getElements().size(), is(2));
        assertThat(q.tailPages.size(), is(1));
        assertThat(q.tailPages.get(0).isFullyRead(), is(true));
        assertThat(q.tailPages.get(0).isFullyAcked(), is(false));
        assertThat(q.headPage.isFullyRead(), is(false));
        assertThat(q.headPage.isFullyAcked(), is(false));
        b = q.nonBlockReadBatch(10);
        assertThat(b.getElements().size(), is(2));
        assertThat(q.tailPages.get(0).isFullyRead(), is(true));
        assertThat(q.tailPages.get(0).isFullyAcked(), is(false));
        assertThat(q.headPage.isFullyRead(), is(true));
        assertThat(q.headPage.isFullyAcked(), is(false));
        b = q.nonBlockReadBatch(10);
        assertThat(b, nullValue());
    }
}", ,"// total of 2 pages: 1 head and 1 tail
",// total of 2 pages: 1 head and 1 tail,212,252,[0],0,[0],0,[0],0,0,0,0,writeMultiPage(),org.logstash.ackedqueue.QueueTest,writeMultiPage/0,False,213,8,10,0,10,2,16,29,0,3,0,16,0,0,1,0,1,0,4,15,5,0,2,0,0,0,23,1,0,False
563,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void writeMultiPageWithInOrderAcking(),"@Test
public void writeMultiPageWithInOrderAcking() throws IOException {
    List<Queueable> elements = Arrays.asList(new StringElement(""foobarbaz1""), new StringElement(""foobarbaz2""), new StringElement(""foobarbaz3""), new StringElement(""foobarbaz4""));
    try (Queue q = new Queue(TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(elements.get(0), 2), dataPath))) {
        q.open();
        for (Queueable e : elements) {
            q.write(e);
        }
        Batch b = q.nonBlockReadBatch(10);
        assertThat(b.getElements().size(), is(2));
        assertThat(q.tailPages.size(), is(1));
        // lets keep a ref to that tail page before acking
        Page tailPage = q.tailPages.get(0);
        assertThat(tailPage.isFullyRead(), is(true));
        // ack first batch which includes all elements from tailPages
        b.close();
        assertThat(q.tailPages.size(), is(0));
        assertThat(tailPage.isFullyRead(), is(true));
        assertThat(tailPage.isFullyAcked(), is(true));
        b = q.nonBlockReadBatch(10);
        assertThat(b.getElements().size(), is(2));
        assertThat(q.headPage.isFullyRead(), is(true));
        assertThat(q.headPage.isFullyAcked(), is(false));
        b.close();
        assertThat(q.headPage.isFullyAcked(), is(true));
    }
}", ,"// lets keep a ref to that tail page before acking
[[SEP]]// ack first batch which includes all elements from tailPages
",// lets keep a ref to that tail page before acking[[SEP]]// ack first batch which includes all elements from tailPages,255,293,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,writeMultiPageWithInOrderAcking(),org.logstash.ackedqueue.QueueTest,writeMultiPageWithInOrderAcking/0,False,256,8,11,0,11,2,16,24,0,4,0,16,0,0,1,0,1,0,4,9,5,0,2,0,0,0,28,1,0,False
564,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void writeMultiPageWithInOrderAckingCheckpoints(),"@Test
public void writeMultiPageWithInOrderAckingCheckpoints() throws IOException {
    List<Queueable> elements1 = Arrays.asList(new StringElement(""foobarbaz1""), new StringElement(""foobarbaz2""));
    List<Queueable> elements2 = Arrays.asList(new StringElement(""foobarbaz3""), new StringElement(""foobarbaz4""));
    Settings settings = SettingsImpl.builder(TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(elements1.get(0), 2), dataPath)).checkpointMaxWrites(// arbitrary high enough threshold so that it's not reached (default for TestSettings is 1)
    1024).build();
    try (Queue q = new Queue(settings)) {
        q.open();
        assertThat(q.headPage.getPageNum(), is(0));
        Checkpoint c = q.getCheckpointIO().read(""checkpoint.head"");
        assertThat(c.getPageNum(), is(0));
        assertThat(c.getElementCount(), is(0));
        assertThat(c.getMinSeqNum(), is(0L));
        assertThat(c.getFirstUnackedSeqNum(), is(0L));
        assertThat(c.getFirstUnackedPageNum(), is(0));
        for (Queueable e : elements1) {
            q.write(e);
        }
        c = q.getCheckpointIO().read(""checkpoint.head"");
        assertThat(c.getPageNum(), is(0));
        assertThat(c.getElementCount(), is(0));
        assertThat(c.getMinSeqNum(), is(0L));
        assertThat(c.getFirstUnackedSeqNum(), is(0L));
        assertThat(c.getFirstUnackedPageNum(), is(0));
        // assertThat(elements1.get(1).getSeqNum(), is(2L));
        q.ensurePersistedUpto(2);
        c = q.getCheckpointIO().read(""checkpoint.head"");
        assertThat(c.getPageNum(), is(0));
        assertThat(c.getElementCount(), is(2));
        assertThat(c.getMinSeqNum(), is(1L));
        assertThat(c.getFirstUnackedSeqNum(), is(1L));
        assertThat(c.getFirstUnackedPageNum(), is(0));
        for (Queueable e : elements2) {
            q.write(e);
        }
        c = q.getCheckpointIO().read(""checkpoint.head"");
        assertThat(c.getPageNum(), is(1));
        assertThat(c.getElementCount(), is(0));
        assertThat(c.getMinSeqNum(), is(0L));
        assertThat(c.getFirstUnackedSeqNum(), is(0L));
        assertThat(c.getFirstUnackedPageNum(), is(0));
        c = q.getCheckpointIO().read(""checkpoint.0"");
        assertThat(c.getPageNum(), is(0));
        assertThat(c.getElementCount(), is(2));
        assertThat(c.getMinSeqNum(), is(1L));
        assertThat(c.getFirstUnackedSeqNum(), is(1L));
        Batch b = q.nonBlockReadBatch(10);
        b.close();
        try {
            q.getCheckpointIO().read(""checkpoint.0"");
            fail(""expected NoSuchFileException thrown"");
        } catch (NoSuchFileException e) {
            // nothing
        }
        c = q.getCheckpointIO().read(""checkpoint.head"");
        assertThat(c.getPageNum(), is(1));
        assertThat(c.getElementCount(), is(2));
        assertThat(c.getMinSeqNum(), is(3L));
        assertThat(c.getFirstUnackedSeqNum(), is(3L));
        assertThat(c.getFirstUnackedPageNum(), is(1));
        b = q.nonBlockReadBatch(10);
        b.close();
        c = q.getCheckpointIO().read(""checkpoint.head"");
        assertThat(c.getPageNum(), is(1));
        assertThat(c.getElementCount(), is(2));
        assertThat(c.getMinSeqNum(), is(3L));
        assertThat(c.getFirstUnackedSeqNum(), is(5L));
        assertThat(c.getFirstUnackedPageNum(), is(1));
    }
}", ,"// arbitrary high enough threshold so that it's not reached (default for TestSettings is 1)
[[SEP]]// assertThat(elements1.get(1).getSeqNum(), is(2L));
[[SEP]]// nothing
","// arbitrary high enough threshold so that it's not reached (default for TestSettings is 1)[[SEP]]// assertThat(elements1.get(1).getSeqNum(), is(2L));[[SEP]]// nothing",295,380,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,writeMultiPageWithInOrderAckingCheckpoints(),org.logstash.ackedqueue.QueueTest,writeMultiPageWithInOrderAckingCheckpoints/0,False,296,13,20,0,20,4,23,67,0,6,0,23,0,0,2,0,2,0,13,41,13,0,2,0,0,0,40,1,0,False
565,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void randomAcking(),"@Test
public void randomAcking() throws IOException {
    Random random = new Random();
    // 10 tests of random queue sizes
    for (int loop = 0; loop < 10; loop++) {
        int page_count = random.nextInt(100) + 1;
        // String format call below needs to at least print one digit
        final int digits = Math.max((int) Math.ceil(Math.log10(page_count)), 1);
        // create a queue with a single element per page
        List<Queueable> elements = new ArrayList<>();
        for (int i = 0; i < page_count; i++) {
            elements.add(new StringElement(String.format(""%0"" + digits + ""d"", i)));
        }
        try (Queue q = new Queue(TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(elements.get(0)), dataPath))) {
            q.open();
            for (Queueable e : elements) {
                q.write(e);
            }
            assertThat(q.tailPages.size(), is(page_count - 1));
            // first read all elements
            List<Batch> batches = new ArrayList<>();
            for (Batch b = q.nonBlockReadBatch(1); b != null; b = q.nonBlockReadBatch(1)) {
                batches.add(b);
            }
            assertThat(batches.size(), is(page_count));
            // then ack randomly
            Collections.shuffle(batches);
            for (Batch b : batches) {
                b.close();
            }
            assertThat(q.tailPages.size(), is(0));
        }
    }
}", ,"// 10 tests of random queue sizes
[[SEP]]// String format call below needs to at least print one digit
[[SEP]]// create a queue with a single element per page
[[SEP]]// first read all elements
[[SEP]]// then ack randomly
",// 10 tests of random queue sizes[[SEP]]// String format call below needs to at least print one digit[[SEP]]// create a queue with a single element per page[[SEP]]// first read all elements[[SEP]]// then ack randomly,382,424,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,randomAcking(),org.logstash.ackedqueue.QueueTest,randomAcking/0,False,383,7,8,0,8,6,19,28,0,9,0,19,0,0,5,1,1,0,2,11,10,3,3,0,0,0,29,1,0,False
566,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void reachMaxUnread(),"@Test(timeout = 300_000)
public void reachMaxUnread() throws IOException, InterruptedException, ExecutionException {
    Queueable element = new StringElement(""foobarbaz"");
    int singleElementCapacity = computeCapacityForMmapPageIO(element);
    Settings settings = SettingsImpl.builder(TestSettings.persistedQueueSettings(singleElementCapacity, dataPath)).maxUnread(// 2 so we know the first write should not block and the second should
    2).build();
    try (Queue q = new Queue(settings)) {
        q.open();
        long seqNum = q.write(element);
        assertThat(seqNum, is(1L));
        assertThat(q.isFull(), is(false));
        int ELEMENT_COUNT = 1000;
        for (int i = 0; i < ELEMENT_COUNT; i++) {
            // we expect the next write call to block so let's wrap it in a Future
            Future<Long> future = executor.submit(() -> q.write(element));
            while (!q.isFull()) {
                // spin wait until data is written and write blocks
                Thread.sleep(1);
            }
            assertThat(q.unreadCount, is(2L));
            assertThat(future.isDone(), is(false));
            // read one element, which will unblock the last write
            Batch b = q.nonBlockReadBatch(1);
            assertThat(b.getElements().size(), is(1));
            // future result is the blocked write seqNum for the second element
            assertThat(future.get(), is(2L + i));
            assertThat(q.isFull(), is(false));
        }
        // since we did not ack and pages hold a single item
        assertThat(q.tailPages.size(), is(ELEMENT_COUNT));
    }
}", ,"// 2 so we know the first write should not block and the second should
[[SEP]]// we expect the next write call to block so let's wrap it in a Future
[[SEP]]// spin wait until data is written and write blocks
[[SEP]]// read one element, which will unblock the last write
[[SEP]]// future result is the blocked write seqNum for the second element
[[SEP]]// since we did not ack and pages hold a single item
","// 2 so we know the first write should not block and the second should[[SEP]]// we expect the next write call to block so let's wrap it in a Future[[SEP]]// spin wait until data is written and write blocks[[SEP]]// read one element, which will unblock the last write[[SEP]]// future result is the blocked write seqNum for the second element[[SEP]]// since we did not ack and pages hold a single item",426,467,[0],0,"[0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0]",0,0,0,0,reachMaxUnread(),org.logstash.ackedqueue.QueueTest,reachMaxUnread/0,False,427,10,12,0,12,3,18,25,0,9,0,18,0,0,2,0,1,0,1,10,9,1,3,0,0,1,43,1,0,False
567,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void reachMaxUnreadWithAcking(),"@Test
public void reachMaxUnreadWithAcking() throws IOException, InterruptedException, ExecutionException {
    Queueable element = new StringElement(""foobarbaz"");
    // TODO: add randomized testing on the page size (but must be > single element size)
    Settings settings = SettingsImpl.builder(// 256 is arbitrary, large enough to hold a few elements
    TestSettings.persistedQueueSettings(256, dataPath)).maxUnread(2).// 2 so we know the first write should not block and the second should
    build();
    try (Queue q = new Queue(settings)) {
        q.open();
        // perform first non-blocking write
        long seqNum = q.write(element);
        assertThat(seqNum, is(1L));
        assertThat(q.isFull(), is(false));
        int ELEMENT_COUNT = 1000;
        for (int i = 0; i < ELEMENT_COUNT; i++) {
            // we expect this next write call to block so let's wrap it in a Future
            Future<Long> future = executor.submit(() -> q.write(element));
            // spin wait until data is written and write blocks
            while (!q.isFull()) {
                Thread.sleep(1);
            }
            // read one element, which will unblock the last write
            Batch b = q.nonBlockReadBatch(1);
            assertThat(b, notNullValue());
            assertThat(b.getElements().size(), is(1));
            b.close();
            // future result is the blocked write seqNum for the second element
            assertThat(future.get(), is(2L + i));
            assertThat(q.isFull(), is(false));
        }
        // all batches are acked, no tail pages should exist
        assertThat(q.tailPages.size(), is(0));
        // the last read unblocked the last write so some elements (1 unread and maybe some acked) should be in the head page
        assertThat(q.headPage.getElementCount() > 0L, is(true));
        assertThat(q.headPage.unreadCount(), is(1L));
        assertThat(q.unreadCount, is(1L));
    }
}", ,"// TODO: add randomized testing on the page size (but must be > single element size)
[[SEP]]// 256 is arbitrary, large enough to hold a few elements
[[SEP]]// 2 so we know the first write should not block and the second should
[[SEP]]// perform first non-blocking write
[[SEP]]// we expect this next write call to block so let's wrap it in a Future
[[SEP]]// spin wait until data is written and write blocks
[[SEP]]// read one element, which will unblock the last write
[[SEP]]// future result is the blocked write seqNum for the second element
[[SEP]]// all batches are acked, no tail pages should exist
[[SEP]]// the last read unblocked the last write so some elements (1 unread and maybe some acked) should be in the head page
","// TODO: add randomized testing on the page size (but must be > single element size)[[SEP]]// 256 is arbitrary, large enough to hold a few elements[[SEP]]// 2 so we know the first write should not block and the second should[[SEP]]// perform first non-blocking write[[SEP]]// we expect this next write call to block so let's wrap it in a Future[[SEP]]// spin wait until data is written and write blocks[[SEP]]// read one element, which will unblock the last write[[SEP]]// future result is the blocked write seqNum for the second element[[SEP]]// all batches are acked, no tail pages should exist[[SEP]]// the last read unblocked the last write so some elements (1 unread and maybe some acked) should be in the head page",469,516,[0],0,"[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",1,"[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",1,1,1,1,reachMaxUnreadWithAcking(),org.logstash.ackedqueue.QueueTest,reachMaxUnreadWithAcking/0,False,470,10,14,0,14,4,20,27,0,8,0,20,0,0,2,0,1,0,1,13,8,1,3,0,0,1,36,1,0,False
568,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void reachMaxSizeTest(),"@Test(timeout = 50_000)
public void reachMaxSizeTest() throws IOException, InterruptedException {
    // 10 bytes
    Queueable element = new StringElement(""0123456789"");
    // allow 10 elements per page but only 100 events in total
    Settings settings = TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(element, 10), computeCapacityForMmapPageIO(element, 100), dataPath);
    try (Queue q = new Queue(settings)) {
        q.open();
        // should be able to write 99 events before getting full
        int elementCount = 99;
        for (int i = 0; i < elementCount; i++) {
            q.write(element);
        }
        assertThat(q.isFull(), is(false));
        // we expect this next write call to block so let's wrap it in a Future
        executor.submit(() -> q.write(element));
        while (!q.isFull()) {
            Thread.sleep(10);
        }
        assertThat(q.isFull(), is(true));
    }
}", ,"// 10 bytes
[[SEP]]// allow 10 elements per page but only 100 events in total
[[SEP]]// should be able to write 99 events before getting full
[[SEP]]// we expect this next write call to block so let's wrap it in a Future
",// 10 bytes[[SEP]]// allow 10 elements per page but only 100 events in total[[SEP]]// should be able to write 99 events before getting full[[SEP]]// we expect this next write call to block so let's wrap it in a Future,518,543,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,reachMaxSizeTest(),org.logstash.ackedqueue.QueueTest,reachMaxSizeTest/0,False,519,7,7,0,7,3,9,17,0,5,0,9,0,0,2,0,1,0,1,6,5,0,2,0,0,1,30,1,0,False
569,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void ackingMakesQueueNotFullAgainTest(),"@Test(timeout = 50_000)
public void ackingMakesQueueNotFullAgainTest() throws IOException, InterruptedException, ExecutionException {
    // 10 bytes
    Queueable element = new StringElement(""0123456789"");
    // allow 10 elements per page but only 100 events in total
    Settings settings = TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(element, 10), computeCapacityForMmapPageIO(element, 100), dataPath);
    try (Queue q = new Queue(settings)) {
        q.open();
        // should be able to write 90 + 9 events (9 pages + 1 head-page) before getting full
        final long elementCount = 99;
        for (int i = 0; i < elementCount; i++) {
            q.write(element);
        }
        assertThat(q.isFull(), is(false));
        // we expect this next write call to block so let's wrap it in a Future
        Future<Long> future = executor.submit(() -> q.write(element));
        assertThat(future.isDone(), is(false));
        while (!q.isFull()) {
            Thread.sleep(10);
        }
        assertThat(q.isFull(), is(true));
        // read 1 page (10 events)
        Batch b = q.readBatch(10, TimeUnit.SECONDS.toMillis(1));
        // purge 1 page
        b.close();
        while (q.isFull()) {
            Thread.sleep(10);
        }
        assertThat(q.isFull(), is(false));
        assertThat(future.get(), is(elementCount + 1));
    }
}", ,"// 10 bytes
[[SEP]]// allow 10 elements per page but only 100 events in total
[[SEP]]// should be able to write 90 + 9 events (9 pages + 1 head-page) before getting full
[[SEP]]// we expect this next write call to block so let's wrap it in a Future
[[SEP]]// read 1 page (10 events)
[[SEP]]// purge 1 page
",// 10 bytes[[SEP]]// allow 10 elements per page but only 100 events in total[[SEP]]// should be able to write 90 + 9 events (9 pages + 1 head-page) before getting full[[SEP]]// we expect this next write call to block so let's wrap it in a Future[[SEP]]// read 1 page (10 events)[[SEP]]// purge 1 page,545,580,[0],0,"[0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0]",0,0,0,0,ackingMakesQueueNotFullAgainTest(),org.logstash.ackedqueue.QueueTest,ackingMakesQueueNotFullAgainTest/0,False,546,8,9,0,9,4,14,25,0,7,0,14,0,0,3,0,1,0,1,10,7,1,2,0,0,1,35,1,0,False
570,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void resumeWriteOnNoLongerFullQueueTest(),"@Test(timeout = 50_000)
public void resumeWriteOnNoLongerFullQueueTest() throws IOException, InterruptedException, ExecutionException {
    // 10 bytes
    Queueable element = new StringElement(""0123456789"");
    // allow 10 elements per page but only 100 events in total
    Settings settings = TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(element, 10), computeCapacityForMmapPageIO(element, 100), dataPath);
    try (Queue q = new Queue(settings)) {
        q.open();
        // should be able to write 90 + 9 events (9 pages + 1 head-page) before getting full
        int elementCount = 99;
        for (int i = 0; i < elementCount; i++) {
            q.write(element);
        }
        assertThat(q.isFull(), is(false));
        // read 1 page (10 events) here while not full yet so that the read will not signal the not full state
        // we want the batch closing below to signal the not full state
        Batch b = q.readBatch(10, TimeUnit.SECONDS.toMillis(1));
        // we expect this next write call to block so let's wrap it in a Future
        Future<Long> future = executor.submit(() -> q.write(element));
        assertThat(future.isDone(), is(false));
        while (!q.isFull()) {
            Thread.sleep(10);
        }
        assertThat(q.isFull(), is(true));
        assertThat(future.isDone(), is(false));
        // purge 1 page
        b.close();
        assertThat(future.get(), is(elementCount + 1L));
    }
}", ,"// 10 bytes
[[SEP]]// allow 10 elements per page but only 100 events in total
[[SEP]]// read 1 page (10 events) here while not full yet so that the read will not signal the not full state
[[SEP]]// should be able to write 90 + 9 events (9 pages + 1 head-page) before getting full
[[SEP]]// we want the batch closing below to signal the not full state
[[SEP]]// we expect this next write call to block so let's wrap it in a Future
[[SEP]]// purge 1 page
",// 10 bytes[[SEP]]// allow 10 elements per page but only 100 events in total[[SEP]]// should be able to write 90 + 9 events (9 pages + 1 head-page) before getting full[[SEP]]// read 1 page (10 events) here while not full yet so that the read will not signal the not full state// we want the batch closing below to signal the not full state[[SEP]]// we expect this next write call to block so let's wrap it in a Future[[SEP]]// purge 1 page,582,617,[0],0,"[0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0]",0,0,0,0,resumeWriteOnNoLongerFullQueueTest(),org.logstash.ackedqueue.QueueTest,resumeWriteOnNoLongerFullQueueTest/0,False,583,8,9,0,9,3,14,22,0,7,0,14,0,0,2,0,1,0,1,9,7,1,2,0,0,1,38,1,0,False
571,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void queueStillFullAfterPartialPageAckTest(),"@Test(timeout = 50_000)
public void queueStillFullAfterPartialPageAckTest() throws IOException, InterruptedException {
    // 10 bytes
    Queueable element = new StringElement(""0123456789"");
    // allow 10 elements per page but only 100 events in total
    Settings settings = TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(element, 10), computeCapacityForMmapPageIO(element, 100), dataPath);
    try (Queue q = new Queue(settings)) {
        q.open();
        // should be able to write 99 events before getting full
        int ELEMENT_COUNT = 99;
        for (int i = 0; i < ELEMENT_COUNT; i++) {
            q.write(element);
        }
        assertThat(q.isFull(), is(false));
        // we expect this next write call to block so let's wrap it in a Future
        executor.submit(() -> q.write(element));
        while (!q.isFull()) {
            Thread.sleep(10);
        }
        assertThat(q.isFull(), is(true));
        // read 90% of page (9 events)
        Batch b = q.readBatch(9, TimeUnit.SECONDS.toMillis(1));
        // this should not purge a page
        b.close();
        // queue should still be full
        assertThat(q.isFull(), is(true));
    }
}", ,"// 10 bytes
[[SEP]]// allow 10 elements per page but only 100 events in total
[[SEP]]// should be able to write 99 events before getting full
[[SEP]]// we expect this next write call to block so let's wrap it in a Future
[[SEP]]// read 90% of page (9 events)
[[SEP]]// this should not purge a page
[[SEP]]// queue should still be full
",// 10 bytes[[SEP]]// allow 10 elements per page but only 100 events in total[[SEP]]// should be able to write 99 events before getting full[[SEP]]// we expect this next write call to block so let's wrap it in a Future[[SEP]]// read 90% of page (9 events)[[SEP]]// this should not purge a page[[SEP]]// queue should still be full,619,650,[0],0,"[0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0, 0]",0,0,0,0,queueStillFullAfterPartialPageAckTest(),org.logstash.ackedqueue.QueueTest,queueStillFullAfterPartialPageAckTest/0,False,620,8,9,0,9,3,12,20,0,6,0,12,0,0,2,0,1,0,1,8,6,0,2,0,0,1,42,1,0,False
572,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void concurrentWritesTest(),"@Test(timeout = 300_000)
public void concurrentWritesTest() throws IOException, InterruptedException, ExecutionException {
    final int WRITER_COUNT = 5;
    final ExecutorService executorService = Executors.newFixedThreadPool(WRITER_COUNT);
    // very small pages to maximize page creation
    Settings settings = TestSettings.persistedQueueSettings(100, dataPath);
    try (Queue q = new Queue(settings)) {
        q.open();
        int ELEMENT_COUNT = 1000;
        AtomicInteger element_num = new AtomicInteger(0);
        // we expect this next write call to block so let's wrap it in a Future
        Callable<Integer> writer = () -> {
            for (int i = 0; i < ELEMENT_COUNT; i++) {
                int n = element_num.getAndIncrement();
                q.write(new StringElement("""" + n));
            }
            return ELEMENT_COUNT;
        };
        List<Future<Integer>> futures = new ArrayList<>();
        for (int i = 0; i < WRITER_COUNT; i++) {
            futures.add(executorService.submit(writer));
        }
        int BATCH_SIZE = 10;
        int read_count = 0;
        while (read_count < ELEMENT_COUNT * WRITER_COUNT) {
            Batch b = q.readBatch(BATCH_SIZE, TimeUnit.SECONDS.toMillis(1));
            read_count += b.size();
            b.close();
        }
        for (Future<Integer> future : futures) {
            int result = future.get();
            assertThat(result, is(ELEMENT_COUNT));
        }
        assertThat(q.tailPages.isEmpty(), is(true));
        assertThat(q.isFullyAcked(), is(true));
    } finally {
        executorService.shutdownNow();
        executorService.awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);
    }
}", ,"// very small pages to maximize page creation
[[SEP]]// we expect this next write call to block so let's wrap it in a Future
",// very small pages to maximize page creation[[SEP]]// we expect this next write call to block so let's wrap it in a Future,715,763,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,concurrentWritesTest(),org.logstash.ackedqueue.QueueTest,concurrentWritesTest/0,False,716,6,9,0,9,5,18,39,1,15,0,18,0,0,4,0,1,0,1,10,16,2,3,0,0,1,51,1,0,False
573,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void fullyAckedHeadPageBeheadingTest(),"@Test
public void fullyAckedHeadPageBeheadingTest() throws IOException {
    Queueable element = new StringElement(""foobarbaz1"");
    try (Queue q = new Queue(TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(element, 2), dataPath))) {
        q.open();
        Batch b;
        q.write(element);
        b = q.nonBlockReadBatch(1);
        assertThat(b.getElements().size(), is(1));
        b.close();
        q.write(element);
        b = q.nonBlockReadBatch(1);
        assertThat(b.getElements().size(), is(1));
        b.close();
        // head page should be full and fully acked
        assertThat(q.headPage.isFullyAcked(), is(true));
        assertThat(q.headPage.hasSpace(element.serialize().length), is(false));
        assertThat(q.isFullyAcked(), is(true));
        // write extra element to trigger beheading
        q.write(element);
        // since head page was fully acked it should not have created a new tail page
        assertThat(q.tailPages.isEmpty(), is(true));
        assertThat(q.headPage.getPageNum(), is(1));
        assertThat(q.firstUnackedPageNum(), is(1));
        assertThat(q.isFullyAcked(), is(false));
    }
}", ,"// since head page was fully acked it should not have created a new tail page
[[SEP]]// head page should be full and fully acked
[[SEP]]// write extra element to trigger beheading
",// head page should be full and fully acked[[SEP]]// write extra element to trigger beheading[[SEP]]// since head page was fully acked it should not have created a new tail page,765,798,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,fullyAckedHeadPageBeheadingTest(),org.logstash.ackedqueue.QueueTest,fullyAckedHeadPageBeheadingTest/0,False,766,8,15,0,15,1,17,23,0,3,0,17,0,0,0,0,1,0,1,7,4,0,1,0,0,0,25,1,0,False
574,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void getsPersistedByteSizeCorrectlyForFullyAckedDeletedTailPages(),"@SuppressWarnings(""try"")
@Test
public void getsPersistedByteSizeCorrectlyForFullyAckedDeletedTailPages() throws Exception {
    // 10 bytes
    final Queueable element = new StringElement(""0123456789"");
    final Settings settings = TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(element), dataPath);
    try (Queue q = new Queue(settings)) {
        q.open();
        q.write(element);
        Batch b1 = q.readBatch(2, TimeUnit.SECONDS.toMillis(1));
        q.write(element);
        Batch b2 = q.readBatch(2, TimeUnit.SECONDS.toMillis(1));
        q.write(element);
        Batch b3 = q.readBatch(2, TimeUnit.SECONDS.toMillis(1));
        q.write(element);
        Batch b4 = q.readBatch(2, TimeUnit.SECONDS.toMillis(1));
        assertThat(q.tailPages.size(), is(3));
        assertThat(q.getPersistedByteSize() > 0, is(true));
        // fully ack middle page and head page
        b2.close();
        b4.close();
        assertThat(q.tailPages.size(), is(2));
        assertThat(q.getPersistedByteSize() > 0, is(true));
        q.close();
        q.open();
        assertThat(q.tailPages.size(), is(2));
        assertThat(q.getPersistedByteSize() > 0, is(true));
    }
}", ,"// 10 bytes
[[SEP]]// fully ack middle page and head page
",// 10 bytes[[SEP]]// fully ack middle page and head page,822,856,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,getsPersistedByteSizeCorrectlyForFullyAckedDeletedTailPages(),org.logstash.ackedqueue.QueueTest,getsPersistedByteSizeCorrectlyForFullyAckedDeletedTailPages/0,False,824,8,10,0,10,4,12,25,0,7,0,12,0,0,0,0,1,0,2,14,7,0,1,0,0,0,34,1,0,False
575,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void testZeroByteFullyAckedPageOnOpen(),"@Test
public void testZeroByteFullyAckedPageOnOpen() throws IOException {
    // 10 bytes
    Queueable element = new StringElement(""0123456789"");
    Settings settings = TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(element), dataPath);
    // the goal here is to recreate a condition where the queue has a tail page of size zero with
    // a checkpoint that indicates it is full acknowledged
    // see issue #7809
    try (Queue q = new Queue(settings)) {
        q.open();
        Queueable element1 = new StringElement(""0123456789"");
        Queueable element2 = new StringElement(""9876543210"");
        // write 2 elements to force a new page.
        final long firstSeq = q.write(element1);
        q.write(element2);
        assertThat(q.tailPages.size(), is(1));
        // work directly on the tail page and not the queue to avoid having the queue purge the page
        // but make sure the tail page checkpoint marks it as fully acked
        Page tp = q.tailPages.get(0);
        Batch b = new Batch(tp.read(1), q);
        assertThat(b.getElements().get(0), is(element1));
        tp.ack(firstSeq, 1, 1);
        assertThat(tp.isFullyAcked(), is(true));
    }
    // now we have a queue state where page 0 is fully acked but not purged
    // manually truncate page 0 to zero byte.
    // TODO page.0 file name is hard coded here because we did not expose the page file naming.
    FileChannel c = new FileOutputStream(Paths.get(dataPath, ""page.0"").toFile(), true).getChannel();
    c.truncate(0);
    c.close();
    try (Queue q = new Queue(settings)) {
        // here q.open used to crash with:
        // java.io.IOException: Page file size 0 different to configured page capacity 27 for ...
        // because page.0 ended up as a zero byte file but its checkpoint says it's fully acked
        q.open();
        assertThat(q.getUnackedCount(), is(1L));
        assertThat(q.tailPages.size(), is(1));
        assertThat(q.tailPages.get(0).isFullyAcked(), is(false));
        assertThat(q.tailPages.get(0).elementCount, is(1));
        assertThat(q.headPage.elementCount, is(0));
    }
}", ,"// the goal here is to recreate a condition where the queue has a tail page of size zero with
[[SEP]]// a checkpoint that indicates it is full acknowledged
[[SEP]]// see issue #7809
[[SEP]]// now we have a queue state where page 0 is fully acked but not purged
[[SEP]]// manually truncate page 0 to zero byte.
[[SEP]]// 10 bytes
[[SEP]]// work directly on the tail page and not the queue to avoid having the queue purge the page
[[SEP]]// write 2 elements to force a new page.
[[SEP]]// but make sure the tail page checkpoint marks it as fully acked
[[SEP]]// TODO page.0 file name is hard coded here because we did not expose the page file naming.
[[SEP]]// here q.open used to crash with:
[[SEP]]// java.io.IOException: Page file size 0 different to configured page capacity 27 for ...
[[SEP]]// because page.0 ended up as a zero byte file but its checkpoint says it's fully acked
",// 10 bytes[[SEP]]// the goal here is to recreate a condition where the queue has a tail page of size zero with// a checkpoint that indicates it is full acknowledged// see issue #7809[[SEP]]// write 2 elements to force a new page.[[SEP]]// work directly on the tail page and not the queue to avoid having the queue purge the page// but make sure the tail page checkpoint marks it as fully acked[[SEP]]// now we have a queue state where page 0 is fully acked but not purged// manually truncate page 0 to zero byte.// TODO page.0 file name is hard coded here because we did not expose the page file naming.[[SEP]]// here q.open used to crash with:// java.io.IOException: Page file size 0 different to configured page capacity 27 for ...// because page.0 ended up as a zero byte file but its checkpoint says it's fully acked,928,976,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",1,"[0, 0, 0, 0, 1, 0]",1,1,1,1,testZeroByteFullyAckedPageOnOpen(),org.logstash.ackedqueue.QueueTest,testZeroByteFullyAckedPageOnOpen/0,False,929,9,12,0,12,1,19,28,0,10,0,19,0,0,0,0,2,0,4,13,10,0,1,0,0,0,41,1,0,False
576,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void testZeroByteFullyAckedHeadPageOnOpen(),"@Test
public void testZeroByteFullyAckedHeadPageOnOpen() throws IOException {
    // 10 bytes
    Queueable element = new StringElement(""0123456789"");
    Settings settings = TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(element), dataPath);
    // the goal here is to recreate a condition where the queue has a head page of size zero with
    // a checkpoint that indicates it is full acknowledged
    // see issue #10855
    try (Queue q = new Queue(settings)) {
        q.open();
        q.write(element);
        Batch batch = q.readBatch(1, TimeUnit.SECONDS.toMillis(1));
        batch.close();
        assertThat(batch.size(), is(1));
        assertThat(q.isFullyAcked(), is(true));
    }
    // now we have a queue state where page 0 is fully acked but not purged
    // manually truncate page 0 to zero byte to mock corrupted page
    FileChannel c = new FileOutputStream(Paths.get(dataPath, ""page.0"").toFile(), true).getChannel();
    c.truncate(0);
    c.close();
    try (Queue q = new Queue(settings)) {
        // here q.open used to crash with:
        // java.io.IOException: Page file size is too small to hold elements
        // because head page recover() check integrity of file size
        q.open();
        // recreated head page and checkpoint
        File page1 = Paths.get(dataPath, ""page.1"").toFile();
        assertThat(page1.exists(), is(true));
        assertThat(page1.length(), is(greaterThan(0L)));
    }
}", ,"// the goal here is to recreate a condition where the queue has a head page of size zero with
[[SEP]]// a checkpoint that indicates it is full acknowledged
[[SEP]]// see issue #10855
[[SEP]]// now we have a queue state where page 0 is fully acked but not purged
[[SEP]]// 10 bytes
[[SEP]]// manually truncate page 0 to zero byte to mock corrupted page
[[SEP]]// here q.open used to crash with:
[[SEP]]// java.io.IOException: Page file size is too small to hold elements
[[SEP]]// because head page recover() check integrity of file size
[[SEP]]// recreated head page and checkpoint
",// 10 bytes[[SEP]]// the goal here is to recreate a condition where the queue has a head page of size zero with// a checkpoint that indicates it is full acknowledged// see issue #10855[[SEP]]// now we have a queue state where page 0 is fully acked but not purged// manually truncate page 0 to zero byte to mock corrupted page[[SEP]]// here q.open used to crash with:// java.io.IOException: Page file size is too small to hold elements// because head page recover() check integrity of file size[[SEP]]// recreated head page and checkpoint,978,1014,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,testZeroByteFullyAckedHeadPageOnOpen(),org.logstash.ackedqueue.QueueTest,testZeroByteFullyAckedHeadPageOnOpen/0,False,979,8,10,0,10,1,19,21,0,7,0,19,0,0,0,0,2,0,3,5,7,0,1,0,0,0,39,1,0,False
577,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void pageCapacityChangeOnExistingQueue(),"@Test
public void pageCapacityChangeOnExistingQueue() throws IOException {
    final Queueable element = new StringElement(""foobarbaz1"");
    final int ORIGINAL_CAPACITY = computeCapacityForMmapPageIO(element, 2);
    final int NEW_CAPACITY = computeCapacityForMmapPageIO(element, 10);
    try (Queue q = new Queue(TestSettings.persistedQueueSettings(ORIGINAL_CAPACITY, dataPath))) {
        q.open();
        q.write(element);
    }
    try (Queue q = new Queue(TestSettings.persistedQueueSettings(NEW_CAPACITY, dataPath))) {
        q.open();
        assertThat(q.tailPages.get(0).getPageIO().getCapacity(), is(ORIGINAL_CAPACITY));
        assertThat(q.headPage.getPageIO().getCapacity(), is(NEW_CAPACITY));
        q.write(element);
    }
    try (Queue q = new Queue(TestSettings.persistedQueueSettings(NEW_CAPACITY, dataPath))) {
        q.open();
        assertThat(q.tailPages.get(0).getPageIO().getCapacity(), is(ORIGINAL_CAPACITY));
        assertThat(q.tailPages.get(1).getPageIO().getCapacity(), is(NEW_CAPACITY));
        assertThat(q.headPage.getPageIO().getCapacity(), is(NEW_CAPACITY));
        // will read only within a page boundary
        Batch b1 = q.readBatch(10, TimeUnit.SECONDS.toMillis(1));
        assertThat(b1.size(), is(1));
        b1.close();
        // will read only within a page boundary
        Batch b2 = q.readBatch(10, TimeUnit.SECONDS.toMillis(1));
        assertThat(b2.size(), is(1));
        b2.close();
        assertThat(q.tailPages.size(), is(0));
    }
}", ,"// will read only within a page boundary
[[SEP]]// will read only within a page boundary
",// will read only within a page boundary[[SEP]]// will read only within a page boundary,1016,1052,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,pageCapacityChangeOnExistingQueue(),org.logstash.ackedqueue.QueueTest,pageCapacityChangeOnExistingQueue/0,False,1017,9,11,0,11,1,14,28,0,8,0,14,0,0,0,0,3,0,1,12,8,0,1,0,0,0,35,1,0,False
578,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void maximizeBatch(),"@Test(timeout = 5000)
public void maximizeBatch() throws IOException {
    // very small pages to maximize page creation
    Settings settings = TestSettings.persistedQueueSettings(1000, dataPath);
    try (Queue q = new Queue(settings)) {
        q.open();
        Callable<Void> writer = () -> {
            // sleep 500 ms
            Thread.sleep(500);
            q.write(new StringElement(""E2""));
            return null;
        };
        // write one element now and schedule the 2nd write in 500ms
        q.write(new StringElement(""E1""));
        executor.submit(writer);
        // issue a batch read with a 1s timeout, the batch should contain both elements since
        // the timeout is greater than the 2nd write delay
        Batch b = q.readBatch(10, TimeUnit.SECONDS.toMillis(1));
        assertThat(b.size(), is(2));
    }
}", ,"// very small pages to maximize page creation
[[SEP]]// issue a batch read with a 1s timeout, the batch should contain both elements since
[[SEP]]// sleep 500 ms
[[SEP]]// write one element now and schedule the 2nd write in 500ms
[[SEP]]// the timeout is greater than the 2nd write delay
","// very small pages to maximize page creation[[SEP]]// sleep 500 ms[[SEP]]// write one element now and schedule the 2nd write in 500ms[[SEP]]// issue a batch read with a 1s timeout, the batch should contain both elements since// the timeout is greater than the 2nd write delay",1055,1079,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,maximizeBatch(),org.logstash.ackedqueue.QueueTest,maximizeBatch/0,False,1056,6,7,0,7,1,10,16,1,4,0,10,0,0,0,0,1,0,2,6,4,0,2,0,0,1,17,1,0,False
579,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void lockIsReleasedUponOpenException(),"@Test
public void lockIsReleasedUponOpenException() throws Exception {
    Settings settings = SettingsImpl.builder(TestSettings.persistedQueueSettings(100, dataPath)).queueMaxBytes(Long.MAX_VALUE).build();
    try {
        Queue queue = new Queue(settings);
        queue.open();
        fail(""expected queue.open() to throws when not enough disk free"");
    } catch (IOException e) {
        assertThat(e.getMessage(), CoreMatchers.containsString(""Unable to allocate""));
    }
    // at this point the Queue lock should be released and Queue.open should not throw a LockException
    try (Queue queue = new Queue(TestSettings.persistedQueueSettings(10, dataPath))) {
        queue.open();
    }
}", ,"// at this point the Queue lock should be released and Queue.open should not throw a LockException
",// at this point the Queue lock should be released and Queue.open should not throw a LockException,1091,1108,[0],0,[0],0,[0],0,0,0,0,lockIsReleasedUponOpenException(),org.logstash.ackedqueue.QueueTest,lockIsReleasedUponOpenException/0,False,1092,6,6,0,6,2,9,14,0,3,0,9,0,0,0,0,2,0,2,2,3,0,1,0,0,0,21,1,0,False
580,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTest.java,org.logstash.ackedqueue.QueueTest,void firstUnackedPagePointToFullyAckedPurgedPage(),"@Test
public void firstUnackedPagePointToFullyAckedPurgedPage() throws Exception {
    // 10 bytes
    Queueable element = new StringElement(""0123456789"");
    Settings settings = TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(element), dataPath);
    // simulate a scenario that a tail page fail to complete fully ack, crash in the middle of purge
    // normal purge: write fully acked checkpoint -> delete tail page -> (boom!) delete checkpoint -> write head page
    // the queue head page, firstUnackedPageNum, points to a removed page which is fully acked
    // the queue should be able to open and remove dangling checkpoint
    try (Queue q = new Queue(settings)) {
        q.open();
        // create two pages
        q.write(element);
        q.write(element);
    }
    try (Queue q = new Queue(settings)) {
        // now we have head checkpoint pointing to page.0
        // manually delete page.0
        Paths.get(dataPath, ""page.0"").toFile().delete();
        // create a fully acked checkpoint.0 to mock a partial acked action
        // which purges the tail page and the checkpoint file remains
        Checkpoint cp = q.getCheckpointIO().read(""checkpoint.0"");
        Paths.get(dataPath, ""checkpoint.0"").toFile().delete();
        Checkpoint mockAckedCp = new Checkpoint(cp.getPageNum(), cp.getFirstUnackedPageNum(), cp.getFirstUnackedSeqNum() + 1, cp.getMinSeqNum(), cp.getElementCount());
        q.getCheckpointIO().write(""checkpoint.0"", mockAckedCp);
        // here q.open used to crash with:
        // java.io.IOException: Page file size is too small to hold elements
        // because checkpoint has outdated state saying it is not fully acked
        q.open();
        // dangling checkpoint should be deleted
        File cp0 = Paths.get(dataPath, ""checkpoint.0"").toFile();
        assertFalse(""Dangling page's checkpoint file should be removed"", cp0.exists());
    }
}", ,"// simulate a scenario that a tail page fail to complete fully ack, crash in the middle of purge
[[SEP]]// normal purge: write fully acked checkpoint -> delete tail page -> (boom!) delete checkpoint -> write head page
[[SEP]]// the queue head page, firstUnackedPageNum, points to a removed page which is fully acked
[[SEP]]// the queue should be able to open and remove dangling checkpoint
[[SEP]]// 10 bytes
[[SEP]]// create two pages
[[SEP]]// now we have head checkpoint pointing to page.0
[[SEP]]// create a fully acked checkpoint.0 to mock a partial acked action
[[SEP]]// here q.open used to crash with:
[[SEP]]// java.io.IOException: Page file size is too small to hold elements
[[SEP]]// manually delete page.0
[[SEP]]// which purges the tail page and the checkpoint file remains
[[SEP]]// because checkpoint has outdated state saying it is not fully acked
[[SEP]]// dangling checkpoint should be deleted
","// 10 bytes[[SEP]]// simulate a scenario that a tail page fail to complete fully ack, crash in the middle of purge// normal purge: write fully acked checkpoint -> delete tail page -> (boom!) delete checkpoint -> write head page// the queue head page, firstUnackedPageNum, points to a removed page which is fully acked// the queue should be able to open and remove dangling checkpoint[[SEP]]// create two pages[[SEP]]// now we have head checkpoint pointing to page.0// manually delete page.0[[SEP]]// create a fully acked checkpoint.0 to mock a partial acked action// which purges the tail page and the checkpoint file remains[[SEP]]// here q.open used to crash with:// java.io.IOException: Page file size is too small to hold elements// because checkpoint has outdated state saying it is not fully acked[[SEP]]// dangling checkpoint should be deleted",1110,1147,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0, 0]",0,0,0,0,firstUnackedPagePointToFullyAckedPurgedPage(),org.logstash.ackedqueue.QueueTest,firstUnackedPagePointToFullyAckedPurgedPage/0,False,1111,9,15,0,15,1,17,19,0,7,0,17,0,0,0,0,2,0,7,1,7,1,1,0,0,0,35,1,0,False
581,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTestHelpers.java,org.logstash.ackedqueue.QueueTestHelpers,int computeCapacityForMmapPageIO(Queueable),"/**
 * Returns the {@link MmapPageIOV2} capacity required for the supplied element
 * @param element
 * @return int - capacity required for the supplied element
 * @throws IOException Throws if a serialization error occurs
 */
public static int computeCapacityForMmapPageIO(final Queueable element) throws IOException {
    return computeCapacityForMmapPageIO(element, 1);
}","/**
 * Returns the {@link MmapPageIOV2} capacity required for the supplied element
 * @param element
 * @return int - capacity required for the supplied element
 * @throws IOException Throws if a serialization error occurs
 */
", ,/** * Returns the {@link MmapPageIOV2} capacity required for the supplied element * @param element * @return int - capacity required for the supplied element * @throws IOException Throws if a serialization error occurs */,37,39,[0],0,[0],0,[0],0,0,0,0,computeCapacityForMmapPageIO(Queueable),org.logstash.ackedqueue.QueueTestHelpers,computeCapacityForMmapPageIO/1[org.logstash.ackedqueue.Queueable],False,37,2,10,9,1,1,1,3,1,0,1,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,21,9,0,True
582,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\ackedqueue\QueueTestHelpers.java,org.logstash.ackedqueue.QueueTestHelpers,"int computeCapacityForMmapPageIO(Queueable, int)","/**
 * Returns the {@link org.logstash.ackedqueue.io.MmapPageI} capacity require to hold a multiple elements including all headers and other metadata.
 * @param element
 * @return int - capacity required for the supplied number of elements
 * @throws IOException Throws if a serialization error occurs
 */
public static int computeCapacityForMmapPageIO(final Queueable element, int count) throws IOException {
    return MmapPageIOV2.HEADER_SIZE + (count * (MmapPageIOV2.SEQNUM_SIZE + MmapPageIOV2.LENGTH_SIZE + element.serialize().length + MmapPageIOV2.CHECKSUM_SIZE));
}","/**
 * Returns the {@link org.logstash.ackedqueue.io.MmapPageI} capacity require to hold a multiple elements including all headers and other metadata.
 * @param element
 * @return int - capacity required for the supplied number of elements
 * @throws IOException Throws if a serialization error occurs
 */
", ,/** * Returns the {@link org.logstash.ackedqueue.io.MmapPageI} capacity require to hold a multiple elements including all headers and other metadata. * @param element * @return int - capacity required for the supplied number of elements * @throws IOException Throws if a serialization error occurs */,47,49,[0],0,[0],0,[0],0,0,0,0,"computeCapacityForMmapPageIO(Queueable, int)",org.logstash.ackedqueue.QueueTestHelpers,"computeCapacityForMmapPageIO/2[org.logstash.ackedqueue.Queueable,int]",False,47,1,12,11,1,1,1,3,1,0,2,1,0,0,0,0,0,2,0,0,0,3,0,0,0,0,33,9,0,True
583,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\ConfigVariableExpanderTest.java,org.logstash.common.ConfigVariableExpanderTest,"ConfigVariableExpander getFakeCve(Map<String, Object>, Map<String, String>)","// used by tests IfVertexTest, EventConditionTest
public static ConfigVariableExpander getFakeCve(final Map<String, Object> ssValues, final Map<String, String> envVarValues) {
    MemoryStore ms = new MemoryStore();
    for (Map.Entry<String, Object> e : ssValues.entrySet()) {
        if (e.getValue() instanceof String) {
            ms.persistSecret(new SecretIdentifier(e.getKey()), ((String) e.getValue()).getBytes(StandardCharsets.UTF_8));
        }
    }
    return new ConfigVariableExpander(ms, envVarValues::get);
}","// used by tests IfVertexTest, EventConditionTest
", ,"// used by tests IfVertexTest, EventConditionTest",131,142,[0],0,[0],0,[0],0,0,0,0,"getFakeCve(Map<String, Object>, Map<String, String>)",org.logstash.common.ConfigVariableExpanderTest,"getFakeCve/2[java.util.Map<java.lang.String,java.lang.Object>,java.util.Map<java.lang.String,java.lang.String>]",False,132,3,12,10,2,3,5,9,1,1,2,5,0,0,1,0,0,1,0,0,1,0,2,0,0,0,15,9,0,False
584,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\FsUtilTest.java,org.logstash.common.FsUtilTest,void trueIfEnoughSpace(),"/**
 * {@link FsUtil#hasFreeSpace(java.nio.file.Path, long)} should return true when asked for 1kb of free
 * space in a subfolder of the system's TEMP location.
 */
@Test
public void trueIfEnoughSpace() throws Exception {
    MatcherAssert.assertThat(FsUtil.hasFreeSpace(temp.newFolder().toPath().toAbsolutePath(), 1024L), CoreMatchers.is(true));
}","/**
 * {@link FsUtil#hasFreeSpace(java.nio.file.Path, long)} should return true when asked for 1kb of free
 * space in a subfolder of the system's TEMP location.
 */
", ,"/** * {@link FsUtil#hasFreeSpace(java.nio.file.Path, long)} should return true when asked for 1kb of free * space in a subfolder of the system's TEMP location. */",41,47,[0],0,[0],0,[0],0,0,0,0,trueIfEnoughSpace(),org.logstash.common.FsUtilTest,trueIfEnoughSpace/0,False,42,2,1,0,1,1,6,3,0,0,0,6,0,0,0,0,0,0,0,1,0,0,0,0,0,0,19,1,0,True
585,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\FsUtilTest.java,org.logstash.common.FsUtilTest,void falseIfNotEnoughSpace(),"/**
 * {@link FsUtil#hasFreeSpace(java.nio.file.Path, long)} should return false when asked for
 * {@link Long#MAX_VALUE} of free space in a subfolder of the system's TEMP location.
 */
@Test
public void falseIfNotEnoughSpace() throws Exception {
    MatcherAssert.assertThat(FsUtil.hasFreeSpace(temp.newFolder().toPath().toAbsolutePath(), Long.MAX_VALUE), CoreMatchers.is(false));
}","/**
 * {@link FsUtil#hasFreeSpace(java.nio.file.Path, long)} should return false when asked for
 * {@link Long#MAX_VALUE} of free space in a subfolder of the system's TEMP location.
 */
", ,"/** * {@link FsUtil#hasFreeSpace(java.nio.file.Path, long)} should return false when asked for * {@link Long#MAX_VALUE} of free space in a subfolder of the system's TEMP location. */",53,59,[0],0,[0],0,[0],0,0,0,0,falseIfNotEnoughSpace(),org.logstash.common.FsUtilTest,falseIfNotEnoughSpace/0,False,54,2,1,0,1,1,6,3,0,0,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,20,1,0,True
586,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\SourceWithMetadataTest.java,org.logstash.common.SourceWithMetadataTest,void itShouldInstantiateCleanlyWhenParamsAreGood(),"// So, this really isn't parameterized, but it isn't worth making a separate class for this
@Test
public void itShouldInstantiateCleanlyWhenParamsAreGood() throws IncompleteSourceWithMetadataException {
    new SourceWithMetadata(""proto"", ""path"", 1, 1, ""text"");
}","// So, this really isn't parameterized, but it isn't worth making a separate class for this
", ,"// So, this really isn't parameterized, but it isn't worth making a separate class for this",69,72,[0],0,[0],0,[0],0,0,0,0,itShouldInstantiateCleanlyWhenParamsAreGood(),org.logstash.common.SourceWithMetadataTest,itShouldInstantiateCleanlyWhenParamsAreGood/0,False,70,2,1,0,1,1,0,3,0,0,0,0,0,0,0,0,0,0,3,2,0,0,0,0,0,0,13,1,0,False
587,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testRereadFinalBlock(),"// This test checks that polling after a block has been mostly filled with an event is handled correctly.
@Test
public void testRereadFinalBlock() throws Exception {
    Event event = createEventWithConstantSerializationOverhead(Collections.emptyMap());
    // Fill event with not quite enough characters to fill block. Fill event with valid RecordType characters - this
    // was the cause of https://github.com/elastic/logstash/issues/7868
    event.setField(""message"", generateMessageContent(32495));
    long startTime = System.currentTimeMillis();
    int messageSize = 0;
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1)).build()) {
        for (int i = 0; i < 2; i++) {
            DLQEntry entry = new DLQEntry(event, """", """", String.valueOf(i), constantSerializationLengthTimestamp(startTime++));
            final int serializationLength = entry.serialize().length;
            assertThat(""setup: serialized entry size..."", serializationLength, is(lessThan(BLOCK_SIZE)));
            messageSize += serializationLength;
            writeManager.writeEntry(entry);
        }
        assertThat(messageSize, is(greaterThan(BLOCK_SIZE)));
    }
    try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
        for (int i = 0; i < 2; i++) {
            final DLQEntry dlqEntry = readManager.pollEntry(100);
            assertThat(String.format(""read index `%s`"", i), dlqEntry, is(notNullValue()));
            assertThat("""", dlqEntry.getReason(), is(String.valueOf(i)));
        }
        final DLQEntry entryBeyondEnd = readManager.pollEntry(100);
        assertThat(""read beyond end"", entryBeyondEnd, is(nullValue()));
    }
}","// This test checks that polling after a block has been mostly filled with an event is handled correctly.
","// Fill event with not quite enough characters to fill block. Fill event with valid RecordType characters - this
[[SEP]]// was the cause of https://github.com/elastic/logstash/issues/7868
",// This test checks that polling after a block has been mostly filled with an event is handled correctly.[[SEP]]// Fill event with not quite enough characters to fill block. Fill event with valid RecordType characters - this// was the cause of https://github.com/elastic/logstash/issues/7868,149,179,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testRereadFinalBlock(),org.logstash.common.io.DeadLetterQueueReaderTest,testRereadFinalBlock/0,False,150,7,12,0,12,3,21,25,0,11,0,21,3,3,2,0,2,0,7,12,12,1,2,0,0,0,57,1,0,False
588,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testBlockBoundary(),"// Notes on these tests:
// These tests are designed to test specific edge cases where events end at block boundaries, hence the specific
// sizes of the char arrays being used to pad the events
// This test tests for a single event that ends on a block boundary
@Test
public void testBlockBoundary() throws Exception {
    final int PAD_FOR_BLOCK_SIZE_EVENT = 32490;
    Event event = createEventWithConstantSerializationOverhead();
    char[] field = new char[PAD_FOR_BLOCK_SIZE_EVENT];
    Arrays.fill(field, 'e');
    event.setField(""T"", new String(field));
    Timestamp timestamp = constantSerializationLengthTimestamp();
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1)).build()) {
        for (int i = 0; i < 2; i++) {
            DLQEntry entry = new DLQEntry(event, """", """", """", timestamp);
            assertThat(entry.serialize().length + RecordIOWriter.RECORD_HEADER_SIZE, is(BLOCK_SIZE));
            writeManager.writeEntry(entry);
        }
    }
    try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
        for (int i = 0; i < 2; i++) {
            readManager.pollEntry(100);
        }
    }
}","// This test tests for a single event that ends on a block boundary
", ,"// Notes on these tests:// These tests are designed to test specific edge cases where events end at block boundaries, hence the specific// sizes of the char arrays being used to pad the events// This test tests for a single event that ends on a block boundary",240,263,[0],0,[0],0,[0],0,0,0,0,testBlockBoundary(),org.logstash.common.io.DeadLetterQueueReaderTest,testBlockBoundary/0,False,241,9,10,0,10,3,12,20,0,9,0,12,2,5,2,0,2,0,4,10,9,2,2,0,0,0,49,1,0,False
589,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testBlockBoundaryMultiple(),"// This test has multiple messages, with a message ending on a block boundary
@Test
public void testBlockBoundaryMultiple() throws Exception {
    Event event = createEventWithConstantSerializationOverhead();
    char[] field = new char[7929];
    Arrays.fill(field, 'x');
    event.setField(""message"", new String(field));
    long startTime = System.currentTimeMillis();
    int messageSize = 0;
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1)).build()) {
        for (int i = 1; i <= 5; i++) {
            DLQEntry entry = new DLQEntry(event, """", """", """", constantSerializationLengthTimestamp(startTime++));
            messageSize += entry.serialize().length;
            writeManager.writeEntry(entry);
            if (i == 4) {
                assertThat(messageSize + (RecordIOWriter.RECORD_HEADER_SIZE * 4), is(BLOCK_SIZE));
            }
        }
    }
    try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
        for (int i = 0; i < 5; i++) {
            readManager.pollEntry(100);
        }
    }
}","// This test has multiple messages, with a message ending on a block boundary
", ,"// This test has multiple messages, with a message ending on a block boundary",266,291,[0],0,[0],0,[0],0,0,0,0,testBlockBoundaryMultiple(),org.logstash.common.io.DeadLetterQueueReaderTest,testBlockBoundaryMultiple/0,False,267,8,10,0,10,4,13,23,0,9,0,13,2,4,2,1,2,1,4,13,10,3,3,0,0,0,43,1,0,False
590,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testMultiFlushAfterSegmentComplete(),"@Test
public void testMultiFlushAfterSegmentComplete() throws Exception {
    Event event = new Event();
    final int eventsInSegment = randomBetween(1, 32);
    // Write enough events to not quite complete a second segment.
    final int totalEventsToWrite = (2 * eventsInSegment) - 1;
    event.setField(""T"", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
    Timestamp timestamp = new Timestamp();
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, BLOCK_SIZE * eventsInSegment, defaultDlqSize, Duration.ofHours(1)).build()) {
        for (int i = 1; i < totalEventsToWrite; i++) {
            DLQEntry entry = new DLQEntry(event, """", """", Integer.toString(i), timestamp);
            writeManager.writeEntry(entry);
        }
        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
            for (int i = 1; i < eventsInSegment; i++) {
                DLQEntry entry = readManager.pollEntry(100);
                assertThat(entry.getReason(), is(String.valueOf(i)));
            }
            for (int i = eventsInSegment + 1; i < totalEventsToWrite; i++) {
                DLQEntry entry = readManager.pollEntry(100);
                assertThat(entry, is(nullValue()));
            }
        }
        writeManager.writeEntry(new DLQEntry(event, """", """", ""flush event"", timestamp));
        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
            for (int i = 1; i < totalEventsToWrite; i++) {
                DLQEntry entry = readManager.pollEntry(100);
                assertThat(entry.getReason(), is(String.valueOf(i)));
            }
        }
    }
}", ,"// Write enough events to not quite complete a second segment.
",// Write enough events to not quite complete a second segment.,347,387,[0],0,[0],0,[0],0,0,0,0,testMultiFlushAfterSegmentComplete(),org.logstash.common.io.DeadLetterQueueReaderTest,testMultiFlushAfterSegmentComplete/0,False,348,8,12,0,12,5,14,30,0,15,0,14,2,1,4,0,3,1,6,12,15,4,3,0,0,0,54,1,0,False
591,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testBlockAndSegmentBoundary(),"// This test tests for a single event that ends on a block and segment boundary
@Test
public void testBlockAndSegmentBoundary() throws Exception {
    Event event = createEventWithConstantSerializationOverhead();
    event.setField(""T"", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
    Timestamp timestamp = constantSerializationLengthTimestamp();
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, BLOCK_SIZE, defaultDlqSize, Duration.ofSeconds(1)).build()) {
        for (int i = 0; i < 2; i++) {
            DLQEntry entry = new DLQEntry(event, """", """", """", timestamp);
            assertThat(entry.serialize().length + RecordIOWriter.RECORD_HEADER_SIZE, is(BLOCK_SIZE));
            writeManager.writeEntry(entry);
        }
    }
    try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
        for (int i = 0; i < 2; i++) {
            readManager.pollEntry(100);
        }
    }
}","// This test tests for a single event that ends on a block and segment boundary
", ,// This test tests for a single event that ends on a block and segment boundary,427,447,[0],0,[0],0,[0],0,0,0,0,testBlockAndSegmentBoundary(),org.logstash.common.io.DeadLetterQueueReaderTest,testBlockAndSegmentBoundary/0,False,428,8,11,0,11,3,12,17,0,7,0,12,3,5,2,0,2,0,4,6,7,1,2,0,0,0,50,1,0,False
592,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testWriteReadRandomEventSize(),"@Test
public void testWriteReadRandomEventSize() throws Exception {
    Event event = new Event(Collections.emptyMap());
    // 64kb
    int maxEventSize = BLOCK_SIZE * 2;
    // max = 1000 * 64kb = 64mb
    int eventCount = 1024;
    long startTime = System.currentTimeMillis();
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1)).build()) {
        for (int i = 0; i < eventCount; i++) {
            event.setField(""message"", generateMessageContent((int) (Math.random() * (maxEventSize))));
            DLQEntry entry = new DLQEntry(event, """", """", String.valueOf(i), new Timestamp(startTime++));
            writeManager.writeEntry(entry);
        }
    }
    try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
        for (int i = 0; i < eventCount; i++) {
            DLQEntry entry = readManager.pollEntry(100);
            assertThat(entry.getReason(), is(String.valueOf(i)));
        }
    }
}", ,"// 64kb
[[SEP]]// max = 1000 * 64kb = 64mb
",// 64kb[[SEP]]// max = 1000 * 64kb = 64mb,449,471,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testWriteReadRandomEventSize(),org.logstash.common.io.DeadLetterQueueReaderTest,testWriteReadRandomEventSize/0,False,450,8,11,0,11,3,14,19,0,10,0,14,1,1,2,0,2,2,3,9,10,3,2,0,0,0,39,1,0,False
593,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testSeekByTimestampMoveAfterDeletedSegment(),"@Test
public void testSeekByTimestampMoveAfterDeletedSegment() throws IOException, InterruptedException {
    final long startTime = 1646296760000L;
    final int eventsPerSegment = prepareFilledSegmentFiles(2, startTime);
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
        // remove the first segment
        Files.delete(dir.resolve(segmentFileName(1)));
        // Exercise, seek in the middle of first segment
        final Timestamp seekTarget = new Timestamp(startTime + (eventsPerSegment / 2));
        reader.seekToNextEvent(seekTarget);
        // Verify, hit the first event of the second segment
        DLQEntry readEntry = reader.pollEntry(100);
        assertEquals(""Must load first event of next available segment"", readEntry.getReason(), String.format(""%05d"", eventsPerSegment));
        final Timestamp firstEventSecondSegmentTimestamp = new Timestamp(startTime + eventsPerSegment);
        assertEquals(firstEventSecondSegmentTimestamp, readEntry.getEntryTime());
    }
}", ,"// remove the first segment
[[SEP]]// Exercise, seek in the middle of first segment
[[SEP]]// Verify, hit the first event of the second segment
","// remove the first segment[[SEP]]// Exercise, seek in the middle of first segment[[SEP]]// Verify, hit the first event of the second segment",505,524,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,testSeekByTimestampMoveAfterDeletedSegment(),org.logstash.common.io.DeadLetterQueueReaderTest,testSeekByTimestampMoveAfterDeletedSegment/0,False,506,5,8,0,8,1,10,13,0,6,0,10,2,4,0,0,1,1,2,5,6,3,1,0,0,0,44,1,0,False
594,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testSeekByTimestampWhenAllSegmentsAreDeleted(),"@Test
public void testSeekByTimestampWhenAllSegmentsAreDeleted() throws IOException, InterruptedException {
    final long startTime = System.currentTimeMillis();
    final int eventsPerSegment = prepareFilledSegmentFiles(2, startTime);
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
        // remove the first segment
        Files.delete(dir.resolve(segmentFileName(1)));
        Files.delete(dir.resolve(segmentFileName(2)));
        // Exercise, seek in the middle of first segment
        final Timestamp seekTarget = new Timestamp(startTime + (eventsPerSegment / 2));
        reader.seekToNextEvent(seekTarget);
        // Verify, hit the first event of the second segment
        DLQEntry readEntry = reader.pollEntry(100);
        assertNull(""No entry is available after all segments are deleted"", readEntry);
    }
}", ,"// remove the first segment
[[SEP]]// Exercise, seek in the middle of first segment
[[SEP]]// Verify, hit the first event of the second segment
","// remove the first segment[[SEP]]// Exercise, seek in the middle of first segment[[SEP]]// Verify, hit the first event of the second segment",526,544,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,testSeekByTimestampWhenAllSegmentsAreDeleted(),org.logstash.common.io.DeadLetterQueueReaderTest,testSeekByTimestampWhenAllSegmentsAreDeleted/0,False,527,5,6,0,6,1,8,12,0,5,0,8,2,4,0,0,1,1,1,5,5,2,1,0,0,0,44,1,0,False
595,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testSeekByTimestampWhenSegmentIs1Byte(),"@Test
public void testSeekByTimestampWhenSegmentIs1Byte() throws IOException, InterruptedException {
    final long startTime = System.currentTimeMillis();
    Files.write(dir.resolve(""1.log""), ""1"".getBytes());
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
        // Exercise
        final Timestamp seekTarget = new Timestamp(startTime);
        reader.seekToNextEvent(seekTarget);
        // Verify, no entry is available, reader should seek without exception
        DLQEntry readEntry = reader.pollEntry(100);
        assertNull(""No entry is available after all segments are deleted"", readEntry);
    }
}", ,"// Exercise
[[SEP]]// Verify, no entry is available, reader should seek without exception
","// Exercise[[SEP]]// Verify, no entry is available, reader should seek without exception",546,561,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testSeekByTimestampWhenSegmentIs1Byte(),org.logstash.common.io.DeadLetterQueueReaderTest,testSeekByTimestampWhenSegmentIs1Byte/0,False,547,4,4,0,4,1,7,10,0,4,0,7,0,0,0,0,1,0,3,1,4,0,1,0,0,0,34,1,0,False
596,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testConcurrentWriteReadRandomEventSize(),"/**
 * Tests concurrently reading and writing from the DLQ.
 * @throws Exception On Failure
 */
@Test
public void testConcurrentWriteReadRandomEventSize() throws Exception {
    final ExecutorService exec = Executors.newSingleThreadExecutor();
    try {
        final int maxEventSize = BLOCK_SIZE * 2;
        final int eventCount = 300;
        exec.submit(() -> {
            final Event event = new Event();
            long startTime = System.currentTimeMillis();
            try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(10)).build()) {
                for (int i = 0; i < eventCount; i++) {
                    event.setField(""message"", generateMessageContent((int) (Math.random() * (maxEventSize))));
                    writeManager.writeEntry(new DLQEntry(event, """", """", String.valueOf(i), new Timestamp(startTime++)));
                }
            } catch (final IOException ex) {
                throw new IllegalStateException(ex);
            }
        });
        int i = 0;
        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
            while (i < eventCount) {
                DLQEntry entry = readManager.pollEntry(10_000L);
                if (entry != null) {
                    assertThat(entry.getReason(), is(String.valueOf(i)));
                    i++;
                }
            }
        } catch (Exception e) {
            throw new IllegalArgumentException(""Failed to process entry number"" + i, e);
        }
    } finally {
        exec.shutdown();
        if (!exec.awaitTermination(2L, TimeUnit.MINUTES)) {
            Assert.fail(""Failed to shut down record writer"");
        }
    }
}","/**
 * Tests concurrently reading and writing from the DLQ.
 * @throws Exception On Failure
 */
", ,/** * Tests concurrently reading and writing from the DLQ. * @throws Exception On Failure */,567,614,[0],0,[0],0,[0],0,0,0,0,testConcurrentWriteReadRandomEventSize(),org.logstash.common.io.DeadLetterQueueReaderTest,testConcurrentWriteReadRandomEventSize/0,False,568,8,11,0,11,7,18,40,0,10,0,18,1,1,2,1,3,2,5,10,10,4,4,0,0,1,65,1,0,True
597,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testReaderFindSegmentHoleAfterSimulatingRetentionPolicyClean(),"@Test
public void testReaderFindSegmentHoleAfterSimulatingRetentionPolicyClean() throws IOException, InterruptedException {
    final int eventsPerSegment = prepareFilledSegmentFiles(3);
    assertEquals(319, eventsPerSegment);
    int remainingEventsInSegment = eventsPerSegment;
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
        // read the first event to initialize reader structures
        final DLQEntry dlqEntry = reader.pollEntry(1_000);
        assertEquals(""00000"", dlqEntry.getReason());
        remainingEventsInSegment--;
        // simulate a storage policy clean, drop the middle segment file
        final List<Path> allSegments = listSegmentsSorted(dir);
        assertThat(allSegments.size(), greaterThanOrEqualTo(2));
        // tail segment
        Files.delete(allSegments.remove(0));
        // the segment after
        Files.delete(allSegments.remove(0));
        // consume the first segment
        for (int i = 0; i < remainingEventsInSegment; i++) {
            reader.pollEntry(1_000);
        }
        // Exercise
        // consume the first event after the hole
        final DLQEntry entryAfterHole = reader.pollEntry(1_000);
        assertEquals(String.format(""%05d"", eventsPerSegment * 2), entryAfterHole.getReason());
    }
}", ,"// Exercise
[[SEP]]// read the first event to initialize reader structures
[[SEP]]// simulate a storage policy clean, drop the middle segment file
[[SEP]]// tail segment
[[SEP]]// the segment after
[[SEP]]// consume the first segment
[[SEP]]// consume the first event after the hole
","// read the first event to initialize reader structures[[SEP]]// simulate a storage policy clean, drop the middle segment file[[SEP]]// tail segment[[SEP]]// the segment after[[SEP]]// consume the first segment[[SEP]]// Exercise// consume the first event after the hole",616,646,[0],0,"[0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0]",0,0,0,0,testReaderFindSegmentHoleAfterSimulatingRetentionPolicyClean(),org.logstash.common.io.DeadLetterQueueReaderTest,testReaderFindSegmentHoleAfterSimulatingRetentionPolicyClean/0,False,617,4,5,0,5,2,11,19,0,7,0,11,2,5,1,0,1,0,2,10,7,1,2,0,0,0,41,1,0,False
598,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testReaderWhenAllRemaningSegmentsAreRemoved(),"@Test
public void testReaderWhenAllRemaningSegmentsAreRemoved() throws IOException, InterruptedException {
    int remainingEventsInSegment = prepareFilledSegmentFiles(3);
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
        // read the first event to initialize reader structures
        final DLQEntry dlqEntry = reader.pollEntry(1_000);
        assertEquals(""00000"", dlqEntry.getReason());
        remainingEventsInSegment--;
        // simulate a retention policy clean, that drops the remaining segments
        Files.list(dir).sorted().skip(1).forEach(DeadLetterQueueReaderTest::deleteSegment);
        // consume the first segment
        for (int i = 0; i < remainingEventsInSegment; i++) {
            reader.pollEntry(1_000);
        }
        // Exercise
        // consume the first event after the hole
        final DLQEntry entryAfterHole = reader.pollEntry(1_000);
        assertNull(entryAfterHole);
    }
}", ,"// Exercise
[[SEP]]// read the first event to initialize reader structures
[[SEP]]// simulate a retention policy clean, that drops the remaining segments
[[SEP]]// consume the first segment
[[SEP]]// consume the first event after the hole
","// read the first event to initialize reader structures[[SEP]]// simulate a retention policy clean, that drops the remaining segments[[SEP]]// consume the first segment[[SEP]]// Exercise// consume the first event after the hole",648,674,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,testReaderWhenAllRemaningSegmentsAreRemoved(),org.logstash.common.io.DeadLetterQueueReaderTest,testReaderWhenAllRemaningSegmentsAreRemoved/0,False,649,4,4,0,4,2,9,14,0,5,0,9,1,5,1,0,1,0,1,6,5,0,2,0,0,0,36,1,0,False
599,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testSeekToMiddleWhileTheLogIsRemoved(),"@Test
public void testSeekToMiddleWhileTheLogIsRemoved() throws IOException, InterruptedException {
    writeSegmentSizeEntries(3);
    try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
        // removes 2 segments simulating a retention policy action
        Files.delete(dir.resolve(""1.log""));
        Files.delete(dir.resolve(""2.log""));
        readManager.setCurrentReaderAndPosition(dir.resolve(""1.log""), 1);
        DLQEntry readEntry = readManager.pollEntry(100);
        assertThat(readEntry.getReason(), equalTo(String.valueOf(3)));
    }
}", ,"// removes 2 segments simulating a retention policy action
",// removes 2 segments simulating a retention policy action,684,699,[0],0,[0],0,[0],0,0,0,0,testSeekToMiddleWhileTheLogIsRemoved(),org.logstash.common.io.DeadLetterQueueReaderTest,testSeekToMiddleWhileTheLogIsRemoved/0,False,685,4,5,0,5,1,9,10,0,2,0,9,1,5,0,0,1,0,3,4,2,0,1,0,0,0,28,1,0,False
600,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testStoreReaderPositionAndRestart(),"@Test
public void testStoreReaderPositionAndRestart() throws IOException, InterruptedException {
    // write some data into a segment file
    Path segmentPath = dir.resolve(segmentFileName(0));
    RecordIOWriter writer = new RecordIOWriter(segmentPath);
    for (int j = 0; j < 10; j++) {
        writer.writeEvent((new StringElement("""" + j)).serialize());
    }
    writer.close();
    // read the first event and save read position
    Path currentSegment;
    long currentPosition;
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
        byte[] rawStr = reader.pollEntryBytes();
        assertNotNull(rawStr);
        assertEquals(""0"", new String(rawStr, StandardCharsets.UTF_8));
        currentSegment = reader.getCurrentSegment();
        currentPosition = reader.getCurrentPosition();
    }
    // reopen the reader from the last saved position and read next element
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
        reader.setCurrentReaderAndPosition(currentSegment, currentPosition);
        byte[] rawStr = reader.pollEntryBytes();
        assertNotNull(rawStr);
        assertEquals(""1"", new String(rawStr, StandardCharsets.UTF_8));
    }
}", ,"// write some data into a segment file
[[SEP]]// read the first event and save read position
[[SEP]]// reopen the reader from the last saved position and read next element
",// write some data into a segment file[[SEP]]// read the first event and save read position[[SEP]]// reopen the reader from the last saved position and read next element,701,730,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,testStoreReaderPositionAndRestart(),org.logstash.common.io.DeadLetterQueueReaderTest,testStoreReaderPositionAndRestart/0,False,702,6,11,0,11,2,11,23,0,9,0,11,1,1,1,0,2,1,3,3,9,1,1,0,0,0,33,1,0,False
601,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testStoreReaderPositionWithBlocksWithInternalFragmentation(),"@Test
public void testStoreReaderPositionWithBlocksWithInternalFragmentation() throws IOException, InterruptedException {
    writeSegmentWithFirstBlockContainingInternalFragmentation();
    // read the first event and save read position
    Path currentSegment;
    long currentPosition;
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
        byte[] rawStr = reader.pollEntryBytes();
        assertNotNull(rawStr);
        assertEquals(stringOf(INTERNAL_FRAG_PAYLOAD_SIZE, 'A'), new String(rawStr, StandardCharsets.UTF_8));
        currentSegment = reader.getCurrentSegment();
        currentPosition = reader.getCurrentPosition();
    }
    // reopen the reader from the last saved position and read next element
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
        reader.setCurrentReaderAndPosition(currentSegment, currentPosition);
        byte[] rawStr = reader.pollEntryBytes();
        assertNotNull(rawStr);
        assertEquals(""BBBBBBBBBB"", new String(rawStr, StandardCharsets.UTF_8));
    }
}", ,"// read the first event and save read position
[[SEP]]// reopen the reader from the last saved position and read next element
",// read the first event and save read position[[SEP]]// reopen the reader from the last saved position and read next element,755,778,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testStoreReaderPositionWithBlocksWithInternalFragmentation(),org.logstash.common.io.DeadLetterQueueReaderTest,testStoreReaderPositionWithBlocksWithInternalFragmentation/0,False,756,4,7,0,7,1,8,18,0,6,0,8,2,2,0,0,2,0,1,0,6,0,1,0,0,0,30,1,0,False
602,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testStoreReaderPositionWithBlocksWithInternalFragmentationOnceMessageIsBiggerThenBlock(),"@Test
public void testStoreReaderPositionWithBlocksWithInternalFragmentationOnceMessageIsBiggerThenBlock() throws IOException, InterruptedException {
    final int payloadSize = INTERNAL_FRAG_PAYLOAD_SIZE + BLOCK_SIZE;
    byte[] almostFullBlockPayload = new byte[payloadSize];
    Arrays.fill(almostFullBlockPayload, (byte) 'A');
    Path segmentPath = dir.resolve(segmentFileName(0));
    RecordIOWriter writer = new RecordIOWriter(segmentPath);
    writer.writeEvent(almostFullBlockPayload);
    // write a second segment with small payload
    byte[] smallPayload = new byte[10];
    Arrays.fill(smallPayload, (byte) 'B');
    writer.writeEvent(smallPayload);
    writer.close();
    // read the first event and save read position
    Path currentSegment;
    long currentPosition;
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
        byte[] rawStr = reader.pollEntryBytes();
        assertNotNull(rawStr);
        assertEquals(stringOf(payloadSize, 'A'), new String(rawStr, StandardCharsets.UTF_8));
        currentSegment = reader.getCurrentSegment();
        currentPosition = reader.getCurrentPosition();
    }
    // reopen the reader from the last saved position and read next element
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
        reader.setCurrentReaderAndPosition(currentSegment, currentPosition);
        byte[] rawStr = reader.pollEntryBytes();
        assertNotNull(rawStr);
        assertEquals(""BBBBBBBBBB"", new String(rawStr, StandardCharsets.UTF_8));
    }
}", ,"// write a second segment with small payload
[[SEP]]// read the first event and save read position
[[SEP]]// reopen the reader from the last saved position and read next element
",// write a second segment with small payload[[SEP]]// read the first event and save read position[[SEP]]// reopen the reader from the last saved position and read next element,780,815,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,testStoreReaderPositionWithBlocksWithInternalFragmentationOnceMessageIsBiggerThenBlock(),org.logstash.common.io.DeadLetterQueueReaderTest,testStoreReaderPositionWithBlocksWithInternalFragmentationOnceMessageIsBiggerThenBlock/0,False,781,5,10,0,10,1,12,27,0,11,0,12,2,1,0,0,2,0,1,2,11,1,1,0,0,0,63,1,0,False
603,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void writeSegmentWithFirstBlockContainingInternalFragmentation(),"private void writeSegmentWithFirstBlockContainingInternalFragmentation() throws IOException {
    byte[] almostFullBlockPayload = new byte[INTERNAL_FRAG_PAYLOAD_SIZE];
    Arrays.fill(almostFullBlockPayload, (byte) 'A');
    Path segmentPath = dir.resolve(segmentFileName(0));
    RecordIOWriter writer = new RecordIOWriter(segmentPath);
    writer.writeEvent(almostFullBlockPayload);
    // write a second segment with small payload
    byte[] smallPayload = new byte[10];
    Arrays.fill(smallPayload, (byte) 'B');
    writer.writeEvent(smallPayload);
    writer.close();
}", ,"// write a second segment with small payload
",// write a second segment with small payload,817,830,[0],0,[0],0,[0],0,0,0,0,writeSegmentWithFirstBlockContainingInternalFragmentation(),org.logstash.common.io.DeadLetterQueueReaderTest,writeSegmentWithFirstBlockContainingInternalFragmentation/0,False,817,3,6,2,4,1,5,11,0,4,0,5,1,1,0,0,0,0,0,2,4,0,0,0,0,0,22,2,0,False
604,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,Timestamp constantSerializationLengthTimestamp(long),"/**
 * Produces a {@link Timestamp} whose epoch milliseconds is _near_ the provided value
 * such that the result will have a constant serialization length of 24 bytes.
 *
 * If the provided epoch millis is exactly a whole second with no remainder, one millisecond
 * is added to the value to ensure that there are remainder millis.
 *
 * @param millis
 * @return
 */
static Timestamp constantSerializationLengthTimestamp(long millis) {
    if (millis % 1000 == 0) {
        millis += 1;
    }
    final Timestamp timestamp = new Timestamp(millis);
    assertThat(String.format(""pre-validation: expected timestamp to serialize to exactly 24 bytes, got `%s`"", timestamp), timestamp.serialize().length, is(24));
    return new Timestamp(millis);
}","/**
 * Produces a {@link Timestamp} whose epoch milliseconds is _near_ the provided value
 * such that the result will have a constant serialization length of 24 bytes.
 *
 * If the provided epoch millis is exactly a whole second with no remainder, one millisecond
 * is added to the value to ensure that there are remainder millis.
 *
 * @param millis
 * @return
 */
", ,"/** * Produces a {@link Timestamp} whose epoch milliseconds is _near_ the provided value * such that the result will have a constant serialization length of 24 bytes. * * If the provided epoch millis is exactly a whole second with no remainder, one millisecond * is added to the value to ensure that there are remainder millis. * * @param millis * @return */",842,849,[0],0,[0],0,[0],0,0,0,0,constantSerializationLengthTimestamp(long),org.logstash.common.io.DeadLetterQueueReaderTest,constantSerializationLengthTimestamp/1[long],False,842,1,14,12,2,2,4,8,1,1,1,4,0,0,0,1,0,0,1,4,2,1,1,0,0,0,44,8,0,True
605,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,"Event createEventWithConstantSerializationOverhead(Map<String, Object>)","/**
 * Because many of the tests here rely on _exact_ alignment of serialization byte size,
 * and the {@link Timestamp} has a variable-sized serialization length, we need a way to
 * generated events whose serialization length will not vary depending on the millisecond
 * in which the test was run.
 *
 * This method uses the normal method of creating an event, and ensures that the value of
 * the timestamp field will serialize to a constant length, truncating precision and
 * possibly shifting the value to ensure that there is sub-second remainder millis.
 *
 * @param data
 * @return
 */
static Event createEventWithConstantSerializationOverhead(final Map<String, Object> data) {
    final Event event = new Event(data);
    final Timestamp existingTimestamp = event.getTimestamp();
    if (existingTimestamp != null) {
        event.setTimestamp(constantSerializationLengthTimestamp(existingTimestamp));
    }
    return event;
}","/**
 * Because many of the tests here rely on _exact_ alignment of serialization byte size,
 * and the {@link Timestamp} has a variable-sized serialization length, we need a way to
 * generated events whose serialization length will not vary depending on the millisecond
 * in which the test was run.
 *
 * This method uses the normal method of creating an event, and ensures that the value of
 * the timestamp field will serialize to a constant length, truncating precision and
 * possibly shifting the value to ensure that there is sub-second remainder millis.
 *
 * @param data
 * @return
 */
", ,"/** * Because many of the tests here rely on _exact_ alignment of serialization byte size, * and the {@link Timestamp} has a variable-sized serialization length, we need a way to * generated events whose serialization length will not vary depending on the millisecond * in which the test was run. * * This method uses the normal method of creating an event, and ensures that the value of * the timestamp field will serialize to a constant length, truncating precision and * possibly shifting the value to ensure that there is sub-second remainder millis. * * @param data * @return */",872,881,[0],0,[0],0,[0],0,0,0,0,"createEventWithConstantSerializationOverhead(Map<String, Object>)",org.logstash.common.io.DeadLetterQueueReaderTest,"createEventWithConstantSerializationOverhead/1[java.util.Map<java.lang.String,java.lang.Object>]",False,872,3,12,8,4,2,3,8,1,2,1,3,1,2,0,1,0,0,0,0,2,0,1,0,0,0,69,8,0,True
606,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testReaderWithCleanConsumedIsEnabledDeleteFullyConsumedSegmentsAfterBeingAcknowledged(),"@Test
public void testReaderWithCleanConsumedIsEnabledDeleteFullyConsumedSegmentsAfterBeingAcknowledged() throws IOException, InterruptedException {
    final int eventsPerSegment = prepareFilledSegmentFiles(2);
    MockSegmentListener listener = new MockSegmentListener();
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir, true, listener)) {
        // to reach endOfStream on the first segment, a read more than the size has to be done.
        for (int i = 0; i < eventsPerSegment + 1; i++) {
            reader.pollEntry(1_000);
            reader.markForDelete();
        }
        // Verify
        Set<String> segments = DeadLetterQueueUtils.listSegmentPaths(dir).map(Path::getFileName).map(Path::toString).collect(Collectors.toSet());
        assertEquals(""Only head segment file MUST be present"", Set.of(""2.log""), segments);
        assertTrue(""Reader's client must be notified of the segment deletion"", listener.notified);
        assertEquals(""Must report the deletion of 1 segment"", 1, listener.segments);
        assertEquals(""Must report the correct number of deleted events"", eventsPerSegment, listener.events);
    }
}", ,"// to reach endOfStream on the first segment, a read more than the size has to be done.
[[SEP]]// Verify
","// to reach endOfStream on the first segment, a read more than the size has to be done.[[SEP]]// Verify",987,1010,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testReaderWithCleanConsumedIsEnabledDeleteFullyConsumedSegmentsAfterBeingAcknowledged(),org.logstash.common.io.DeadLetterQueueReaderTest,testReaderWithCleanConsumedIsEnabledDeleteFullyConsumedSegmentsAfterBeingAcknowledged/0,False,988,5,6,0,6,2,11,15,0,5,0,11,1,5,1,0,1,0,5,5,5,1,2,0,0,0,54,1,0,False
607,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testReaderWithCleanConsumedIsEnabledWhenSetCurrentPositionThenCleanupTrashedSegments(),"@Test
public void testReaderWithCleanConsumedIsEnabledWhenSetCurrentPositionThenCleanupTrashedSegments() throws IOException {
    prepareFilledSegmentFiles(2);
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir, true, new MockSegmentListener())) {
        final List<Path> allSegments = listSegmentsSorted(dir);
        verifySegmentFiles(allSegments, ""1.log"", ""2.log"");
        Path lastSegmentPath = allSegments.get(1);
        reader.setCurrentReaderAndPosition(lastSegmentPath, VERSION_SIZE);
        // verify
        Set<Path> segmentFiles = DeadLetterQueueUtils.listSegmentPaths(dir).collect(Collectors.toSet());
        assertEquals(Set.of(lastSegmentPath), segmentFiles);
        assertEquals(""Just the 1.log segment should be marked as consumed"", 1, reader.getConsumedSegments());
    }
}", ,"// verify
",// verify,1012,1028,[0],0,[0],0,[0],0,0,0,0,testReaderWithCleanConsumedIsEnabledWhenSetCurrentPositionThenCleanupTrashedSegments(),org.logstash.common.io.DeadLetterQueueReaderTest,testReaderWithCleanConsumedIsEnabledWhenSetCurrentPositionThenCleanupTrashedSegments/0,False,1013,5,8,0,8,1,11,12,0,4,0,11,3,5,0,0,1,0,3,3,4,0,1,0,0,0,43,1,0,False
608,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testReaderCleanMultipleConsumedSegmentsAfterMarkForDeleteAndDontTouchLockOrWriterHeadFiles(),"@Test
public void testReaderCleanMultipleConsumedSegmentsAfterMarkForDeleteAndDontTouchLockOrWriterHeadFiles() throws IOException, InterruptedException {
    int eventsPerSegment = prepareFilledSegmentFiles(3);
    // insert also a .lock file, must be the oldest one
    Path lockFile = Files.createFile(dir.resolve("".lock""));
    FileTime oneSecondAgo = FileTime.from(Instant.now().minusMillis(1_000));
    // this attribute is used in segments sorting
    Files.setAttribute(lockFile, ""basic:lastModifiedTime"", oneSecondAgo);
    // simulate a writer's segment head
    Files.createFile(dir.resolve(""4.log.tmp""));
    MockSegmentListener listener = new MockSegmentListener();
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir, true, listener)) {
        verifySegmentFiles(listSegmentsSorted(dir), ""1.log"", ""2.log"", ""3.log"");
        // consume fully two segments plus one more event to trigger the endOfStream on the second segment
        for (int i = 0; i < (2 * eventsPerSegment) + 1; i++) {
            reader.pollEntry(100L);
        }
        verifySegmentFiles(listSegmentsSorted(dir), ""1.log"", ""2.log"", ""3.log"");
        reader.markForDelete();
        verifySegmentFiles(listSegmentsSorted(dir), ""3.log"");
        assertEquals(""Must report the deletion of 2 segments"", 2, listener.segments);
        assertEquals(""Must report the correct number of deleted events"", eventsPerSegment * listener.segments, listener.events);
        // verify no other files are removed
        try (Stream<Path> stream = Files.list(dir)) {
            Set<String> files = stream.map(Path::getFileName).map(Path::toString).collect(Collectors.toSet());
            assertTrue(""No segments file remain untouched"", files.containsAll(Arrays.asList("".lock"", ""4.log.tmp"")));
        }
    }
}", ,"// insert also a .lock file, must be the oldest one
[[SEP]]// this attribute is used in segments sorting
[[SEP]]// simulate a writer's segment head
[[SEP]]// consume fully two segments plus one more event to trigger the endOfStream on the second segment
[[SEP]]// verify no other files are removed
","// insert also a .lock file, must be the oldest one[[SEP]]// this attribute is used in segments sorting[[SEP]]// simulate a writer's segment head[[SEP]]// consume fully two segments plus one more event to trigger the endOfStream on the second segment[[SEP]]// verify no other files are removed",1045,1082,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,testReaderCleanMultipleConsumedSegmentsAfterMarkForDeleteAndDontTouchLockOrWriterHeadFiles(),org.logstash.common.io.DeadLetterQueueReaderTest,testReaderCleanMultipleConsumedSegmentsAfterMarkForDeleteAndDontTouchLockOrWriterHeadFiles/0,False,1046,4,7,0,7,2,20,23,0,8,0,20,3,5,1,0,2,1,15,7,8,3,2,0,0,0,63,1,0,False
609,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testReaderDoesntIncrementStatisticsOnDeletionError(),"@Test
public void testReaderDoesntIncrementStatisticsOnDeletionError() throws IOException, InterruptedException {
    int eventsPerSegment = prepareFilledSegmentFiles(3);
    MockSegmentListener listener = new MockSegmentListener();
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir, true, listener)) {
        verifySegmentFiles(listSegmentsSorted(dir), ""1.log"", ""2.log"", ""3.log"");
        // consume fully two segments plus one more event to trigger the endOfStream on the second segment
        for (int i = 0; i < (2 * eventsPerSegment) + 1; i++) {
            reader.pollEntry(100L);
        }
        verifySegmentFiles(listSegmentsSorted(dir), ""1.log"", ""2.log"", ""3.log"");
        // simulate an error in last consumed segment (2.log)
        Files.delete(dir.resolve(""2.log""));
        reader.markForDelete();
        verifySegmentFiles(listSegmentsSorted(dir), ""3.log"");
        assertEquals(""Must report the deletion of 1 segment"", 1, listener.segments);
        assertEquals(""Must report the correct number of deleted events"", eventsPerSegment * listener.segments, listener.events);
    }
}", ,"// consume fully two segments plus one more event to trigger the endOfStream on the second segment
[[SEP]]// simulate an error in last consumed segment (2.log)
",// consume fully two segments plus one more event to trigger the endOfStream on the second segment[[SEP]]// simulate an error in last consumed segment (2.log),1084,1109,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testReaderDoesntIncrementStatisticsOnDeletionError(),org.logstash.common.io.DeadLetterQueueReaderTest,testReaderDoesntIncrementStatisticsOnDeletionError/0,False,1085,4,7,0,7,2,8,16,0,4,0,8,3,5,1,0,1,1,10,6,4,3,2,0,0,0,40,1,0,False
610,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueReaderTest.java,org.logstash.common.io.DeadLetterQueueReaderTest,void testReaderLockProhibitMultipleInstances(),"@Test
@SuppressWarnings(""try"")
public void testReaderLockProhibitMultipleInstances() throws IOException {
    try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir, true, new MockSegmentListener())) {
        try (DeadLetterQueueReader secondReader = new DeadLetterQueueReader(dir, true, new MockSegmentListener())) {
        } catch (LockException lockException) {
            // ok it's expected to happen here
            assertThat(lockException.getMessage(), startsWith(""Existing `dlg_reader.lock` file""));
        }
    }
}", ,"// ok it's expected to happen here
",// ok it's expected to happen here,1111,1121,[0],0,[0],0,[0],0,0,0,0,testReaderLockProhibitMultipleInstances(),org.logstash.common.io.DeadLetterQueueReaderTest,testReaderLockProhibitMultipleInstances/0,False,1113,3,2,0,2,2,3,9,0,2,0,3,0,0,0,0,2,0,2,0,2,0,2,0,0,0,20,1,0,False
611,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueWriterAgeRetentionTest.java,org.logstash.common.io.DeadLetterQueueWriterAgeRetentionTest,void testRemovesOlderSegmentsWhenWriteOnReopenedDLQContainingExpiredSegments(),"@Test
public void testRemovesOlderSegmentsWhenWriteOnReopenedDLQContainingExpiredSegments() throws IOException {
    final Event event = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
    event.setField(""message"", DeadLetterQueueReaderTest.generateMessageContent(32479));
    final Clock pointInTimeFixedClock = Clock.fixed(Instant.parse(""2022-02-22T10:20:30.00Z""), ZoneId.of(""Europe/Rome""));
    final ForwardableClock fakeClock = new ForwardableClock(pointInTimeFixedClock);
    // given DLQ with first segment filled of expired events
    prepareDLQWithFirstSegmentOlderThanRetainPeriod(event, fakeClock, Duration.ofDays(2));
    // Exercise
    final long prevQueueSize;
    final long beheadedQueueSize;
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * MB, 1 * GB, Duration.ofSeconds(1)).retentionTime(Duration.ofDays(2)).clock(fakeClock).build()) {
        prevQueueSize = writeManager.getCurrentQueueSize();
        assertEquals(""Queue size is composed of one just one empty file with version byte"", VERSION_SIZE, prevQueueSize);
        // write new entry that trigger clean of age retained segment
        DLQEntry entry = new DLQEntry(event, """", """", String.format(""%05d"", 320), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(System.currentTimeMillis()));
        // when a new write happens in a reopened queue
        writeManager.writeEntry(entry);
        beheadedQueueSize = writeManager.getCurrentQueueSize();
        assertEquals(""No event is expired after reopen of DLQ"", 0, writeManager.getExpiredEvents());
    }
    // then the age policy must remove the expired segments
    assertEquals(""Write should push off the age expired segments"", VERSION_SIZE + BLOCK_SIZE, beheadedQueueSize);
}", ,"// given DLQ with first segment filled of expired events
[[SEP]]// Exercise
[[SEP]]// write new entry that trigger clean of age retained segment
[[SEP]]// when a new write happens in a reopened queue
[[SEP]]// then the age policy must remove the expired segments
",// given DLQ with first segment filled of expired events[[SEP]]// Exercise[[SEP]]// write new entry that trigger clean of age retained segment[[SEP]]// when a new write happens in a reopened queue[[SEP]]// then the age policy must remove the expired segments,73,104,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,testRemovesOlderSegmentsWhenWriteOnReopenedDLQContainingExpiredSegments(),org.logstash.common.io.DeadLetterQueueWriterAgeRetentionTest,testRemovesOlderSegmentsWhenWriteOnReopenedDLQContainingExpiredSegments/0,False,74,8,14,0,14,1,21,18,0,7,0,21,1,1,0,0,1,0,9,8,7,3,1,0,0,0,63,1,0,False
612,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueWriterAgeRetentionTest.java,org.logstash.common.io.DeadLetterQueueWriterAgeRetentionTest,"void prepareDLQWithFirstSegmentOlderThanRetainPeriod(Event, ForwardableClock, Duration)","private void prepareDLQWithFirstSegmentOlderThanRetainPeriod(Event event, ForwardableClock fakeClock, Duration retainedPeriod) throws IOException {
    final Duration littleMoreThanRetainedPeriod = retainedPeriod.plusMinutes(1);
    long startTime = fakeClock.instant().minus(littleMoreThanRetainedPeriod).toEpochMilli();
    int messageSize = 0;
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * MB, 1 * GB, Duration.ofSeconds(1)).retentionTime(retainedPeriod).clock(fakeClock).build()) {
        // 320 generates 10 Mb of data
        for (int i = 0; i < EVENTS_TO_FILL_A_SEGMENT; i++) {
            DLQEntry entry = new DLQEntry(event, """", """", String.format(""%05d"", i), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(startTime++));
            final int serializationLength = entry.serialize().length;
            assertEquals(""setup: serialized entry size..."", serializationLength + RECORD_HEADER_SIZE, BLOCK_SIZE);
            messageSize += serializationLength;
            writeManager.writeEntry(entry);
        }
        assertThat(messageSize, Matchers.is(greaterThan(BLOCK_SIZE)));
    }
}", ,"// 320 generates 10 Mb of data
",// 320 generates 10 Mb of data,106,126,[0],0,[0],0,[0],0,0,0,0,"prepareDLQWithFirstSegmentOlderThanRetainPeriod(Event, ForwardableClock, Duration)",org.logstash.common.io.DeadLetterQueueWriterAgeRetentionTest,"prepareDLQWithFirstSegmentOlderThanRetainPeriod/3[org.logstash.Event,org.logstash.common.io.DeadLetterQueueWriterAgeRetentionTest.ForwardableClock,java.time.Duration]",False,106,6,10,1,9,2,17,15,0,7,3,17,0,0,1,0,1,0,4,6,8,3,2,0,0,0,56,2,0,False
613,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueWriterAgeRetentionTest.java,org.logstash.common.io.DeadLetterQueueWriterAgeRetentionTest,void testRemovesOlderSegmentsWhenWritesIntoDLQContainingExpiredSegments(),"@Test
public void testRemovesOlderSegmentsWhenWritesIntoDLQContainingExpiredSegments() throws IOException {
    final Event event = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
    event.setField(""message"", DeadLetterQueueReaderTest.generateMessageContent(32479));
    long startTime = fakeClock.instant().toEpochMilli();
    int messageSize = 0;
    final Duration retention = Duration.ofDays(2);
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * MB, 1 * GB, Duration.ofSeconds(1)).retentionTime(retention).clock(fakeClock).build()) {
        // 319 generates 10 Mb of data
        for (int i = 0; i < EVENTS_TO_FILL_A_SEGMENT; i++) {
            DLQEntry entry = new DLQEntry(event, """", """", String.format(""%05d"", i), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(startTime++));
            final int serializationLength = entry.serialize().length;
            assertEquals(""setup: serialized entry size..."", serializationLength + RECORD_HEADER_SIZE, BLOCK_SIZE);
            messageSize += serializationLength;
            writeManager.writeEntry(entry);
        }
        assertThat(messageSize, Matchers.is(greaterThan(BLOCK_SIZE)));
        // Exercise
        // write an event that goes in second segment
        fakeClock.forward(retention.plusSeconds(1));
        final long prevQueueSize = writeManager.getCurrentQueueSize();
        assertEquals(""Queue size is composed of one full segment files"", FULL_SEGMENT_FILE_SIZE, prevQueueSize);
        // write new entry that trigger clean of age retained segment
        DLQEntry entry = new DLQEntry(event, """", """", String.format(""%05d"", 320), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(System.currentTimeMillis()));
        // when a new write happens in the same writer
        writeManager.writeEntry(entry);
        final long beheadedQueueSize = writeManager.getCurrentQueueSize();
        // then the age policy must remove the expired segments
        assertEquals(""Write should push off the age expired segments"", VERSION_SIZE + BLOCK_SIZE, beheadedQueueSize);
        assertEquals(""The number of events removed should count as expired"", EVENTS_TO_FILL_A_SEGMENT, writeManager.getExpiredEvents());
    }
}", ,"// Exercise
[[SEP]]// 319 generates 10 Mb of data
[[SEP]]// write an event that goes in second segment
[[SEP]]// write new entry that trigger clean of age retained segment
[[SEP]]// when a new write happens in the same writer
[[SEP]]// then the age policy must remove the expired segments
",// 319 generates 10 Mb of data[[SEP]]// Exercise// write an event that goes in second segment[[SEP]]// write new entry that trigger clean of age retained segment[[SEP]]// when a new write happens in the same writer[[SEP]]// then the age policy must remove the expired segments,128,169,[0],0,"[0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,testRemovesOlderSegmentsWhenWritesIntoDLQContainingExpiredSegments(),org.logstash.common.io.DeadLetterQueueWriterAgeRetentionTest,testRemovesOlderSegmentsWhenWritesIntoDLQContainingExpiredSegments/0,False,129,7,15,0,15,2,25,25,0,11,0,25,0,0,1,0,1,0,11,9,12,4,2,0,0,0,76,1,0,False
614,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueWriterAgeRetentionTest.java,org.logstash.common.io.DeadLetterQueueWriterAgeRetentionTest,void testRemoveMultipleOldestSegmentsWhenRetainedAgeIsExceeded(),"@Test
public void testRemoveMultipleOldestSegmentsWhenRetainedAgeIsExceeded() throws IOException {
    final Event event = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
    event.setField(""message"", DeadLetterQueueReaderTest.generateMessageContent(32479));
    long startTime = fakeClock.instant().toEpochMilli();
    int messageSize = 0;
    final Duration retention = Duration.ofDays(2);
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * MB, 1 * GB, Duration.ofSeconds(1)).retentionTime(retention).clock(fakeClock).build()) {
        // given DLQ with a couple of segments filled of expired events
        // 319 generates 10 Mb of data
        for (int i = 0; i < EVENTS_TO_FILL_A_SEGMENT * 2; i++) {
            DLQEntry entry = new DLQEntry(event, """", """", String.format(""%05d"", i), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(startTime++));
            final int serializationLength = entry.serialize().length;
            assertEquals(""setup: serialized entry size..."", serializationLength + RECORD_HEADER_SIZE, BLOCK_SIZE);
            messageSize += serializationLength;
            writeManager.writeEntry(entry);
        }
        assertThat(messageSize, Matchers.is(greaterThan(BLOCK_SIZE)));
        // when the age expires the retention and a write is done
        // make the retention age to pass for the first 2 full segments
        fakeClock.forward(retention.plusSeconds(1));
        // Exercise
        // write an event that goes in second segment
        final long prevQueueSize = writeManager.getCurrentQueueSize();
        assertEquals(""Queue size is composed of 2 full segment files"", 2 * FULL_SEGMENT_FILE_SIZE, prevQueueSize);
        // write new entry that trigger clean of age retained segment
        DLQEntry entry = new DLQEntry(event, """", """", String.format(""%05d"", 320), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(System.currentTimeMillis()));
        writeManager.writeEntry(entry);
        final long beheadedQueueSize = writeManager.getCurrentQueueSize();
        // then the age policy must remove the expired segments
        assertEquals(""Write should push off the age expired segments"", VERSION_SIZE + BLOCK_SIZE, beheadedQueueSize);
        assertEquals(""The number of events removed should count as expired"", EVENTS_TO_FILL_A_SEGMENT * 2, writeManager.getExpiredEvents());
    }
}", ,"// given DLQ with a couple of segments filled of expired events
[[SEP]]// when the age expires the retention and a write is done
[[SEP]]// Exercise
[[SEP]]// 319 generates 10 Mb of data
[[SEP]]// make the retention age to pass for the first 2 full segments
[[SEP]]// write an event that goes in second segment
[[SEP]]// write new entry that trigger clean of age retained segment
[[SEP]]// then the age policy must remove the expired segments
",// given DLQ with a couple of segments filled of expired events// 319 generates 10 Mb of data[[SEP]]// when the age expires the retention and a write is done// make the retention age to pass for the first 2 full segments[[SEP]]// Exercise// write an event that goes in second segment[[SEP]]// write new entry that trigger clean of age retained segment[[SEP]]// then the age policy must remove the expired segments,171,215,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,testRemoveMultipleOldestSegmentsWhenRetainedAgeIsExceeded(),org.logstash.common.io.DeadLetterQueueWriterAgeRetentionTest,testRemoveMultipleOldestSegmentsWhenRetainedAgeIsExceeded/0,False,172,7,15,0,15,2,25,25,0,11,0,25,0,0,1,0,1,0,11,12,12,7,2,0,0,0,76,1,0,False
615,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueWriterTest.java,org.logstash.common.io.DeadLetterQueueWriterTest,void testDoesNotWriteBeyondLimit(),"@Test
public void testDoesNotWriteBeyondLimit() throws Exception {
    DLQEntry entry = new DLQEntry(new Event(), ""type"", ""id"", ""reason"");
    int payloadLength = RECORD_HEADER_SIZE + VERSION_SIZE + entry.serialize().length;
    final int MESSAGE_COUNT = 5;
    long MAX_QUEUE_LENGTH = payloadLength * MESSAGE_COUNT;
    try (DeadLetterQueueWriter writer = DeadLetterQueueWriter.newBuilder(dir, payloadLength, MAX_QUEUE_LENGTH, Duration.ofSeconds(1)).build()) {
        for (int i = 0; i < MESSAGE_COUNT; i++) writer.writeEntry(entry);
        // Sleep to allow flush to happen
        sleep(3000);
        assertThat(dlqLength(), is(MAX_QUEUE_LENGTH));
        writer.writeEntry(entry);
        sleep(2000);
        assertThat(dlqLength(), is(MAX_QUEUE_LENGTH));
    }
}", ,"// Sleep to allow flush to happen
",// Sleep to allow flush to happen,149,172,[0],0,[0],0,[0],0,0,0,0,testDoesNotWriteBeyondLimit(),org.logstash.common.io.DeadLetterQueueWriterTest,testDoesNotWriteBeyondLimit/0,False,150,6,8,0,8,2,9,14,0,6,0,9,2,1,1,0,1,0,3,5,6,2,2,0,0,0,42,1,0,False
616,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueWriterTest.java,org.logstash.common.io.DeadLetterQueueWriterTest,void testSlowFlush(),"@Test
public void testSlowFlush() throws Exception {
    try (DeadLetterQueueWriter writer = DeadLetterQueueWriter.newBuilder(dir, 1_000, 1_000_000, Duration.ofSeconds(1)).build()) {
        DLQEntry entry = new DLQEntry(new Event(), ""type"", ""id"", ""1"");
        writer.writeEntry(entry);
        entry = new DLQEntry(new Event(), ""type"", ""id"", ""2"");
        // Sleep to allow flush to happen\
        sleep(3000);
        writer.writeEntry(entry);
        sleep(2000);
        // Do not close here - make sure that the slow write is processed
        try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
            assertThat(reader.pollEntry(100).getReason(), is(""1""));
            assertThat(reader.pollEntry(100).getReason(), is(""2""));
        }
    }
}", ,"// Do not close here - make sure that the slow write is processed
[[SEP]]// Sleep to allow flush to happen\
",// Sleep to allow flush to happen\[[SEP]]// Do not close here - make sure that the slow write is processed,174,193,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testSlowFlush(),org.logstash.common.io.DeadLetterQueueWriterTest,testSlowFlush/0,False,175,7,9,0,9,1,9,14,0,3,0,9,1,1,0,0,2,0,8,7,4,0,2,0,0,0,21,1,0,False
617,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueWriterTest.java,org.logstash.common.io.DeadLetterQueueWriterTest,void testNotFlushed(),"@Test
public void testNotFlushed() throws Exception {
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, BLOCK_SIZE, 1_000_000_000, Duration.ofSeconds(5)).build()) {
        for (int i = 0; i < 4; i++) {
            DLQEntry entry = new DLQEntry(new Event(), ""type"", ""id"", ""1"");
            writeManager.writeEntry(entry);
        }
        // Allow for time for scheduled flush check
        Thread.sleep(1000);
        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
            for (int i = 0; i < 4; i++) {
                DLQEntry entry = readManager.pollEntry(100);
                assertThat(entry, is(CoreMatchers.nullValue()));
            }
        }
    }
}", ,"// Allow for time for scheduled flush check
",// Allow for time for scheduled flush check,196,216,[0],0,[0],0,[0],0,0,0,0,testNotFlushed(),org.logstash.common.io.DeadLetterQueueWriterTest,testNotFlushed/0,False,197,6,7,0,7,3,9,15,0,6,0,9,0,0,2,0,2,0,3,8,6,0,3,0,0,0,22,1,0,False
618,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueWriterTest.java,org.logstash.common.io.DeadLetterQueueWriterTest,void testRemoveOldestSegmentWhenRetainedSizeIsExceededAndDropOlderModeIsEnabled(),"@Test
public void testRemoveOldestSegmentWhenRetainedSizeIsExceededAndDropOlderModeIsEnabled() throws IOException {
    Event event = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
    event.setField(""message"", DeadLetterQueueReaderTest.generateMessageContent(32479));
    long startTime = System.currentTimeMillis();
    int messageSize = 0;
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1)).build()) {
        // 320 generates 10 Mb of data
        for (int i = 0; i < (320 * 2) - 1; i++) {
            DLQEntry entry = new DLQEntry(event, """", """", String.format(""%05d"", i), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(startTime));
            final int serializationLength = entry.serialize().length;
            assertEquals(""Serialized entry fills block payload"", BLOCK_SIZE - RECORD_HEADER_SIZE, serializationLength);
            messageSize += serializationLength;
            writeManager.writeEntry(entry);
        }
        assertThat(messageSize, Matchers.is(greaterThan(BLOCK_SIZE)));
    }
    // but every segment file has 1 byte header, 639 messages of 32Kb generates 3 files
    // 0.log with 319
    // 1.log with 319
    // 2.log with 1
    List<String> segmentFileNames = Files.list(dir).map(Path::getFileName).map(Path::toString).sorted().collect(Collectors.toList());
    assertEquals(3, segmentFileNames.size());
    final String fileToBeRemoved = segmentFileNames.get(0);
    // Exercise
    // with another 32Kb message write we go to write the third file and trigger the 20Mb limit of retained store
    final long prevQueueSize;
    final long beheadedQueueSize;
    long droppedEvent;
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1)).storageType(QueueStorageType.DROP_OLDER).build()) {
        prevQueueSize = writeManager.getCurrentQueueSize();
        final int expectedQueueSize = // number of full segment files
        2 * // size of a segment file
        FULL_SEGMENT_FILE_SIZE + VERSION_SIZE + // the third segment file with just one message
        BLOCK_SIZE + // the header of the head tmp file created in opening
        VERSION_SIZE;
        assertEquals(""Queue size is composed of 2 full segment files plus one with an event plus another with just the header byte"", expectedQueueSize, prevQueueSize);
        DLQEntry entry = new DLQEntry(event, """", """", String.format(""%05d"", (320 * 2) - 1), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(startTime));
        writeManager.writeEntry(entry);
        beheadedQueueSize = writeManager.getCurrentQueueSize();
        droppedEvent = writeManager.getDroppedEvents();
    }
    // 1.log with 319
    // 2.log with 1
    // 3.log with 1, created because the close flushes and beheaded the tail file.
    Set<String> afterBeheadSegmentFileNames = Files.list(dir).map(Path::getFileName).map(Path::toString).collect(Collectors.toSet());
    assertEquals(3, afterBeheadSegmentFileNames.size());
    assertThat(afterBeheadSegmentFileNames, Matchers.not(Matchers.contains(fileToBeRemoved)));
    final long expectedQueueSize = prevQueueSize + // the space of the newly inserted message
    BLOCK_SIZE - // the size of the removed segment file
    FULL_SEGMENT_FILE_SIZE - // the size of a previous head file (n.log.tmp) that doesn't exist anymore.
    VERSION_SIZE;
    assertEquals(""Total queue size must be decremented by the size of the first segment file"", expectedQueueSize, beheadedQueueSize);
    assertEquals(""Last segment removal doesn't increment dropped events counter"", 0, droppedEvent);
}", ,"// but every segment file has 1 byte header, 639 messages of 32Kb generates 3 files
[[SEP]]// 0.log with 319
[[SEP]]// 1.log with 319
[[SEP]]// Exercise
[[SEP]]// 1.log with 319
[[SEP]]// 2.log with 1
[[SEP]]// 320 generates 10 Mb of data
[[SEP]]// 2.log with 1
[[SEP]]// with another 32Kb message write we go to write the third file and trigger the 20Mb limit of retained store
[[SEP]]// number of full segment files
[[SEP]]// size of a segment file
[[SEP]]// the third segment file with just one message
[[SEP]]// the header of the head tmp file created in opening
[[SEP]]// 3.log with 1, created because the close flushes and beheaded the tail file.
[[SEP]]// the space of the newly inserted message
[[SEP]]// the size of the removed segment file
[[SEP]]// the size of a previous head file (n.log.tmp) that doesn't exist anymore.
","// 320 generates 10 Mb of data[[SEP]]// but every segment file has 1 byte header, 639 messages of 32Kb generates 3 files// 0.log with 319// 1.log with 319// 2.log with 1[[SEP]]// Exercise// with another 32Kb message write we go to write the third file and trigger the 20Mb limit of retained store[[SEP]]// number of full segment files[[SEP]]// size of a segment file[[SEP]]// the third segment file with just one message[[SEP]]// the header of the head tmp file created in opening[[SEP]]// 1.log with 319// 2.log with 1// 3.log with 1, created because the close flushes and beheaded the tail file.[[SEP]]// the space of the newly inserted message[[SEP]]// the size of the removed segment file[[SEP]]// the size of a previous head file (n.log.tmp) that doesn't exist anymore.",250,323,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",0,0,0,0,testRemoveOldestSegmentWhenRetainedSizeIsExceededAndDropOlderModeIsEnabled(),org.logstash.common.io.DeadLetterQueueWriterTest,testRemoveOldestSegmentWhenRetainedSizeIsExceededAndDropOlderModeIsEnabled/0,False,251,6,12,0,12,2,32,37,0,17,0,32,0,0,1,0,2,2,11,20,18,16,2,0,0,0,95,1,0,False
619,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueWriterTest.java,org.logstash.common.io.DeadLetterQueueWriterTest,void testRemoveSegmentsOrder(),"@Test
public void testRemoveSegmentsOrder() throws IOException {
    try (DeadLetterQueueWriter sut = DeadLetterQueueWriter.newBuilder(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1)).build()) {
        // create some segments files
        Files.createFile(dir.resolve(""9.log""));
        Files.createFile(dir.resolve(""10.log""));
        // Exercise
        sut.dropTailSegment();
        // Verify
        final Set<String> segments = Files.list(dir).map(Path::getFileName).map(Path::toString).filter(// skip current writer head file 1.log.tmp
        s -> !s.endsWith("".tmp"")).filter(// skip .lock file created by writer
        s -> !"".lock"".equals(s)).collect(Collectors.toSet());
        assertEquals(Collections.singleton(""10.log""), segments);
    }
}", ,"// create some segments files
[[SEP]]// Exercise
[[SEP]]// Verify
[[SEP]]// skip current writer head file 1.log.tmp
[[SEP]]// skip .lock file created by writer
",// create some segments files[[SEP]]// Exercise[[SEP]]// Verify[[SEP]]// skip current writer head file 1.log.tmp[[SEP]]// skip .lock file created by writer,325,346,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,testRemoveSegmentsOrder(),org.logstash.common.io.DeadLetterQueueWriterTest,testRemoveSegmentsOrder/0,False,326,3,3,0,3,1,16,9,0,4,0,16,0,0,0,0,1,0,5,3,2,2,1,0,0,2,17,1,0,False
620,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\DeadLetterQueueWriterTest.java,org.logstash.common.io.DeadLetterQueueWriterTest,void testDropEventCountCorrectlyNotEnqueuedEvents(),"@Test
public void testDropEventCountCorrectlyNotEnqueuedEvents() throws IOException, InterruptedException {
    Event blockAlmostFullEvent = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
    int serializationHeader = 286;
    int notEnoughHeaderSpace = 5;
    blockAlmostFullEvent.setField(""message"", DeadLetterQueueReaderTest.generateMessageContent(BLOCK_SIZE - serializationHeader - RECORD_HEADER_SIZE + notEnoughHeaderSpace));
    Event bigEvent = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
    bigEvent.setField(""message"", DeadLetterQueueReaderTest.generateMessageContent(2 * BLOCK_SIZE));
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1)).build()) {
        // enqueue a record with size smaller than BLOCK_SIZE
        DLQEntry entry = new DLQEntry(blockAlmostFullEvent, """", """", ""00001"", DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(System.currentTimeMillis()));
        assertEquals(""Serialized plus header must not leave enough space for another record header "", entry.serialize().length, BLOCK_SIZE - RECORD_HEADER_SIZE - notEnoughHeaderSpace);
        writeManager.writeEntry(entry);
        // enqueue a record bigger than BLOCK_SIZE
        entry = new DLQEntry(bigEvent, """", """", ""00002"", DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(System.currentTimeMillis()));
        assertThat(""Serialized entry has to split in multiple blocks"", entry.serialize().length, is(greaterThan(2 * BLOCK_SIZE)));
        writeManager.writeEntry(entry);
    }
    // fill the queue to push out the segment with the 2 previous events
    Event event = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
    event.setField(""message"", DeadLetterQueueReaderTest.generateMessageContent(32479));
    try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter.newBuilder(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1)).storageType(QueueStorageType.DROP_NEWER).build()) {
        long startTime = System.currentTimeMillis();
        // 319 events of 32K generates almost 2 segments of 10 Mb of data
        for (int i = 0; i < (320 * 2) - 2; i++) {
            DLQEntry entry = new DLQEntry(event, """", """", String.format(""%05d"", i), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(startTime));
            final int serializationLength = entry.serialize().length;
            assertEquals(""Serialized entry fills block payload"", BLOCK_SIZE - RECORD_HEADER_SIZE, serializationLength);
            if (i == 636) {
                // wait flusher thread flushes the data. When DLQ full condition is reached then the size is checked against
                // the effective file sizes loaded from FS. This is due to writer-reader interaction
                Thread.sleep(2_000);
            }
            writeManager.writeEntry(entry);
        }
        // 1.log with 2 events
        // 2.log with 319
        // 3.log with 319
        assertEquals(2, writeManager.getDroppedEvents());
    }
}", ,"// enqueue a record with size smaller than BLOCK_SIZE
[[SEP]]// enqueue a record bigger than BLOCK_SIZE
[[SEP]]// fill the queue to push out the segment with the 2 previous events
[[SEP]]// 1.log with 2 events
[[SEP]]// 2.log with 319
[[SEP]]// 319 events of 32K generates almost 2 segments of 10 Mb of data
[[SEP]]// wait flusher thread flushes the data. When DLQ full condition is reached then the size is checked against
[[SEP]]// the effective file sizes loaded from FS. This is due to writer-reader interaction
[[SEP]]// 3.log with 319
",// enqueue a record with size smaller than BLOCK_SIZE[[SEP]]// enqueue a record bigger than BLOCK_SIZE[[SEP]]// fill the queue to push out the segment with the 2 previous events[[SEP]]// 319 events of 32K generates almost 2 segments of 10 Mb of data[[SEP]]// wait flusher thread flushes the data. When DLQ full condition is reached then the size is checked against// the effective file sizes loaded from FS. This is due to writer-reader interaction[[SEP]]// 1.log with 2 events// 2.log with 319// 3.log with 319,348,400,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0]",0,0,0,0,testDropEventCountCorrectlyNotEnqueuedEvents(),org.logstash.common.io.DeadLetterQueueWriterTest,testDropEventCountCorrectlyNotEnqueuedEvents/0,False,349,6,11,0,11,3,19,31,0,12,0,19,0,0,1,1,2,1,15,18,13,12,3,0,0,0,68,1,0,False
621,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\common\io\RecordIOReaderTest.java,org.logstash.common.io.RecordIOReaderTest,void testEmptySegment(),"@Test
public void testEmptySegment() throws Exception {
    try (RecordIOWriter writer = new RecordIOWriter(file)) {
        // Do nothing. Creating a new writer is the same behaviour as starting and closing
        // This line avoids a compiler warning.
        writer.toString();
    }
    assertThat(RecordIOReader.getSegmentStatus(file), is(RecordIOReader.SegmentStatus.EMPTY));
}", ,"// Do nothing. Creating a new writer is the same behaviour as starting and closing
[[SEP]]// This line avoids a compiler warning.
",// Do nothing. Creating a new writer is the same behaviour as starting and closing// This line avoids a compiler warning.,195,203,[0],0,"[0, 0]",0,[0],0,0,0,0,testEmptySegment(),org.logstash.common.io.RecordIOReaderTest,testEmptySegment/0,False,196,3,2,0,2,1,4,6,0,1,0,4,0,0,0,0,1,0,0,0,1,0,1,0,0,0,12,1,0,False
622,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\CompiledPipelineTest.java,org.logstash.config.ir.CompiledPipelineTest,void testCacheCompiledClassesWithDifferentId(),"@Test
@SuppressWarnings({ ""unchecked"" })
public void testCacheCompiledClassesWithDifferentId() throws IOException, InvalidIRException {
    final FixedPluginFactory pluginFactory = new FixedPluginFactory(() -> null, () -> IDENTITY_FILTER, mockOutputSupplier());
    final PipelineIR baselinePipeline = ConfigCompiler.configToPipelineIR(IRHelpers.toSourceWithMetadataFromPath(""org/logstash/config/ir/cache/pipeline1.conf""), false, null);
    final CompiledPipeline cBaselinePipeline = new CompiledPipeline(baselinePipeline, pluginFactory);
    final ConfigVariableExpander cve = ConfigVariableExpander.withoutSecret(EnvironmentVariableProvider.defaultProvider());
    final PipelineIR pipelineWithDifferentId = ConfigCompiler.configToPipelineIR(IRHelpers.toSourceWithMetadataFromPath(""org/logstash/config/ir/cache/pipeline2.conf""), false, cve);
    final CompiledPipeline cPipelineWithDifferentId = new CompiledPipeline(pipelineWithDifferentId, pluginFactory);
    // actual test: compiling a pipeline with an extra filter should only create 1 extra class
    ComputeStepSyntaxElement.cleanClassCache();
    cBaselinePipeline.buildExecution();
    final int cachedBefore = ComputeStepSyntaxElement.classCacheSize();
    cPipelineWithDifferentId.buildExecution();
    final int cachedAfter = ComputeStepSyntaxElement.classCacheSize();
    final String message = String.format(""unexpected cache size, cachedAfter: %d, cachedBefore: %d"", cachedAfter, cachedBefore);
    assertEquals(message, 0, cachedAfter - cachedBefore);
}", ,"// actual test: compiling a pipeline with an extra filter should only create 1 extra class
",// actual test: compiling a pipeline with an extra filter should only create 1 extra class,572,601,[0],0,[0],0,[0],0,0,0,0,testCacheCompiledClassesWithDifferentId(),org.logstash.config.ir.CompiledPipelineTest,testCacheCompiledClassesWithDifferentId/0,False,574,10,10,0,10,1,10,15,0,9,0,10,1,1,0,0,0,0,4,1,9,1,0,0,0,2,34,1,0,False
623,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\CompiledPipelineTest.java,org.logstash.config.ir.CompiledPipelineTest,void testReuseCompiledClasses(),"@Test
@SuppressWarnings({ ""unchecked"" })
public void testReuseCompiledClasses() throws IOException, InvalidIRException {
    final FixedPluginFactory pluginFactory = new FixedPluginFactory(() -> null, () -> IDENTITY_FILTER, mockOutputSupplier());
    final ConfigVariableExpander cve = ConfigVariableExpander.withoutSecret(EnvironmentVariableProvider.defaultProvider());
    // this pipeline generates 10 classes
    // - 7 for the filters for the nested and leaf Datasets
    // - 3 for the sequence of outputs with a conditional
    final PipelineIR baselinePipeline = ConfigCompiler.configToPipelineIR(IRHelpers.toSourceWithMetadataFromPath(""org/logstash/config/ir/cache/pipeline_reuse_baseline.conf""), false, cve);
    final CompiledPipeline cBaselinePipeline = new CompiledPipeline(baselinePipeline, pluginFactory);
    // this pipeline is much bigger than the baseline
    // but is carefully crafted to reuse the same classes as the baseline pipeline
    final PipelineIR pipelineTwiceAsBig = ConfigCompiler.configToPipelineIR(IRHelpers.toSourceWithMetadataFromPath(""org/logstash/config/ir/cache/pipeline_reuse_test.conf""), false, cve);
    final CompiledPipeline cPipelineTwiceAsBig = new CompiledPipeline(pipelineTwiceAsBig, pluginFactory);
    // test: compiling a much bigger pipeline and asserting no additional classes are generated
    ComputeStepSyntaxElement.cleanClassCache();
    cBaselinePipeline.buildExecution();
    final int cachedBefore = ComputeStepSyntaxElement.classCacheSize();
    cPipelineTwiceAsBig.buildExecution();
    final int cachedAfter = ComputeStepSyntaxElement.classCacheSize();
    final String message = String.format(""unexpected cache size, cachedAfter: %d, cachedBefore: %d"", cachedAfter, cachedBefore);
    assertEquals(message, 0, cachedAfter - cachedBefore);
}", ,"// this pipeline generates 10 classes
[[SEP]]// - 7 for the filters for the nested and leaf Datasets
[[SEP]]// this pipeline is much bigger than the baseline
[[SEP]]// - 3 for the sequence of outputs with a conditional
[[SEP]]// but is carefully crafted to reuse the same classes as the baseline pipeline
[[SEP]]// test: compiling a much bigger pipeline and asserting no additional classes are generated
",// this pipeline generates 10 classes// - 7 for the filters for the nested and leaf Datasets// - 3 for the sequence of outputs with a conditional[[SEP]]// this pipeline is much bigger than the baseline// but is carefully crafted to reuse the same classes as the baseline pipeline[[SEP]]// test: compiling a much bigger pipeline and asserting no additional classes are generated,603,637,[0],0,"[0, 0, 0, 0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,testReuseCompiledClasses(),org.logstash.config.ir.CompiledPipelineTest,testReuseCompiledClasses/0,False,605,10,10,0,10,1,10,15,0,9,0,10,1,1,0,0,0,0,4,1,9,1,0,0,0,2,34,1,0,False
624,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\CompiledPipelineTest.java,org.logstash.config.ir.CompiledPipelineTest,void compilerBenchmark(),"@Test
@SuppressWarnings({ ""unchecked"", ""rawtypes"" })
public void compilerBenchmark() throws Exception {
    final PipelineIR baselinePipelineIR = createPipelineIR(200);
    final PipelineIR testPipelineIR = createPipelineIR(400);
    final JrubyEventExtLibrary.RubyEvent testEvent = JrubyEventExtLibrary.RubyEvent.newRubyEvent(RubyUtil.RUBY, new Event());
    final FixedPluginFactory pluginFactory = new FixedPluginFactory(() -> null, () -> IDENTITY_FILTER, mockOutputSupplier());
    final CompiledPipeline baselineCompiledPipeline = new CompiledPipeline(baselinePipelineIR, pluginFactory);
    final CompiledPipeline testCompiledPipeline = new CompiledPipeline(testPipelineIR, pluginFactory);
    final long compilationBaseline = time(ChronoUnit.MILLIS, () -> {
        final CompiledPipeline.CompiledExecution compiledExecution = baselineCompiledPipeline.buildExecution();
        compiledExecution.compute(RubyUtil.RUBY.newArray(testEvent), false, false);
    });
    final long compilationTest = time(ChronoUnit.MILLIS, () -> {
        final CompiledPipeline.CompiledExecution compiledExecution = testCompiledPipeline.buildExecution();
        compiledExecution.compute(RubyUtil.RUBY.newArray(testEvent), false, false);
    });
    // sanity checks
    final Collection<JrubyEventExtLibrary.RubyEvent> outputEvents = EVENT_SINKS.get(runId);
    MatcherAssert.assertThat(outputEvents.size(), CoreMatchers.is(2));
    MatcherAssert.assertThat(outputEvents.contains(testEvent), CoreMatchers.is(true));
    // regression check
    final String testMessage = ""regression in pipeline compilation, doubling the filters require more than 5 "" + ""time, baseline: "" + compilationBaseline + "" secs, test: "" + compilationTest + "" secs"";
    assertTrue(testMessage, compilationTest / compilationBaseline <= 5);
}", ,"// sanity checks
[[SEP]]// regression check
",// sanity checks[[SEP]]// regression check,639,675,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,compilerBenchmark(),org.logstash.config.ir.CompiledPipelineTest,compilerBenchmark/0,False,641,8,9,0,9,2,13,23,0,12,0,13,3,3,0,0,0,0,6,4,12,3,1,0,0,4,37,1,0,False
625,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\ConfigCompilerTest.java,org.logstash.config.ir.ConfigCompilerTest,void testConfigDuplicateBlocksToPipelineIR(),"/**
 * Tests that repeatedly parsing the same config (containing a large number of duplicated sections)
 * into a {@link Graph} repeatedly results in a graph with a constant (i.e. deterministic)
 * hash code as returned by {@link Graph#uniqueHash()}.
 *
 * @throws Exception On Failure
 */
@Test
public void testConfigDuplicateBlocksToPipelineIR() throws Exception {
    final String condition = ""if [message] == 'foo' {\nif [message] == 'foo' {drop {}}}\n"";
    final StringBuilder source = new StringBuilder().append(""filter {\n"");
    for (int i = 0; i < 100; ++i) {
        source.append(condition);
    }
    final String config = source.append('}').toString();
    final String first = graphHash(config);
    for (int run = 0; run < 5; ++run) {
        assertThat(graphHash(config), is(first));
    }
}","/**
 * Tests that repeatedly parsing the same config (containing a large number of duplicated sections)
 * into a {@link Graph} repeatedly results in a graph with a constant (i.e. deterministic)
 * hash code as returned by {@link Graph#uniqueHash()}.
 *
 * @throws Exception On Failure
 */
", ,/** * Tests that repeatedly parsing the same config (containing a large number of duplicated sections) * into a {@link Graph} repeatedly results in a graph with a constant (i.e. deterministic) * hash code as returned by {@link Graph#uniqueHash()}. * * @throws Exception On Failure */,63,75,[0],0,[0],0,[0],0,0,0,0,testConfigDuplicateBlocksToPipelineIR(),org.logstash.config.ir.ConfigCompilerTest,testConfigDuplicateBlocksToPipelineIR/0,False,64,2,1,0,1,3,6,12,0,6,0,6,1,1,2,0,0,0,2,4,6,0,1,0,0,0,49,1,0,True
626,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\ConfigCompilerTest.java,org.logstash.config.ir.ConfigCompilerTest,void testComplexConfigToPipelineIR(),"/**
 * Tests that repeatedly parsing the same complex config String into a {@link Graph} repeatedly
 * results in a graph with a constant (i.e. deterministic) hash code as returned by
 * {@link Graph#uniqueHash()}.
 *
 * @throws Exception On Failure
 */
@Test
public void testComplexConfigToPipelineIR() throws Exception {
    final ByteArrayOutputStream baos = new ByteArrayOutputStream();
    try (final InputStream src = getClass().getResourceAsStream(""complex.cfg"")) {
        int read;
        final byte[] buffer = new byte[1024];
        while ((read = src.read(buffer)) >= 0) {
            baos.write(buffer, 0, read);
        }
    }
    final String config = baos.toString(""UTF-8"");
    final String first = graphHash(config);
    assertThat(graphHash(config), is(first));
}","/**
 * Tests that repeatedly parsing the same complex config String into a {@link Graph} repeatedly
 * results in a graph with a constant (i.e. deterministic) hash code as returned by
 * {@link Graph#uniqueHash()}.
 *
 * @throws Exception On Failure
 */
", ,/** * Tests that repeatedly parsing the same complex config String into a {@link Graph} repeatedly * results in a graph with a constant (i.e. deterministic) hash code as returned by * {@link Graph#uniqueHash()}. * * @throws Exception On Failure */,84,97,[0],0,[0],0,[0],0,0,0,0,testComplexConfigToPipelineIR(),org.logstash.config.ir.ConfigCompilerTest,testComplexConfigToPipelineIR/0,False,85,3,1,0,1,2,8,13,0,6,0,8,1,1,1,0,1,1,2,3,6,0,2,0,0,0,47,1,0,True
627,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\EventConditionTest.java,org.logstash.config.ir.EventConditionTest,void testInclusionWithFieldInField(),"@Test
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public void testInclusionWithFieldInField() throws Exception {
    final ConfigVariableExpander cve = ConfigVariableExpander.withoutSecret(EnvironmentVariableProvider.defaultProvider());
    final PipelineIR pipelineIR = ConfigCompiler.configToPipelineIR(IRHelpers.toSourceWithMetadata(""input {mockinput{}} filter { "" + ""mockfilter {} } "" + ""output { "" + ""  if [left] in [right] { "" + ""    mockoutput{}"" + ""  } }""), false, cve);
    // left list values never match
    RubyEvent leftIsList = RubyEvent.newRubyEvent(RubyUtil.RUBY);
    List listValues = Arrays.asList(""foo"", ""bar"", ""baz"");
    leftIsList.getEvent().setField(""left"", listValues);
    leftIsList.getEvent().setField(""right"", listValues);
    // left map values never match
    RubyEvent leftIsMap = RubyEvent.newRubyEvent(RubyUtil.RUBY);
    Map mapValues = Collections.singletonMap(""foo"", ""bar"");
    leftIsMap.getEvent().setField(""left"", mapValues);
    leftIsMap.getEvent().setField(""right"", mapValues);
    // left and right string values match when right.contains(left)
    RubyEvent leftIsString1 = RubyEvent.newRubyEvent(RubyUtil.RUBY);
    leftIsString1.getEvent().setField(""left"", ""foo"");
    leftIsString1.getEvent().setField(""right"", ""zfooz"");
    RubyEvent leftIsString2 = RubyEvent.newRubyEvent(RubyUtil.RUBY);
    leftIsString2.getEvent().setField(""left"", ""foo"");
    leftIsString2.getEvent().setField(""right"", ""zzz"");
    // right list value matches when right.contains(left)
    RubyEvent rightIsList1 = RubyEvent.newRubyEvent(RubyUtil.RUBY);
    rightIsList1.getEvent().setField(""left"", ""bar"");
    rightIsList1.getEvent().setField(""right"", listValues);
    RubyEvent rightIsList2 = RubyEvent.newRubyEvent(RubyUtil.RUBY);
    rightIsList2.getEvent().setField(""left"", ""zzz"");
    rightIsList2.getEvent().setField(""right"", listValues);
    // non-string values match when left == right
    RubyEvent nonStringValue1 = RubyEvent.newRubyEvent(RubyUtil.RUBY);
    nonStringValue1.getEvent().setField(""left"", 42L);
    nonStringValue1.getEvent().setField(""right"", 42L);
    RubyEvent nonStringValue2 = RubyEvent.newRubyEvent(RubyUtil.RUBY);
    nonStringValue2.getEvent().setField(""left"", 42L);
    nonStringValue2.getEvent().setField(""right"", 43L);
    RubyArray inputBatch = RubyUtil.RUBY.newArray(leftIsList, leftIsMap, leftIsString1, leftIsString2, rightIsList1, rightIsList2, nonStringValue1, nonStringValue2);
    new CompiledPipeline(pipelineIR, new CompiledPipelineTest.MockPluginFactory(Collections.singletonMap(""mockinput"", () -> null), Collections.singletonMap(""mockfilter"", () -> IDENTITY_FILTER), Collections.singletonMap(""mockoutput"", mockOutputSupplier()))).buildExecution().compute(inputBatch, false, false);
    final RubyEvent[] outputEvents = EVENT_SINKS.get(runId).toArray(new RubyEvent[0]);
    assertThat(outputEvents.length, is(3));
    assertThat(outputEvents[0], is(leftIsString1));
    assertThat(outputEvents[1], is(rightIsList1));
    assertThat(outputEvents[2], is(nonStringValue1));
}", ,"// left list values never match
[[SEP]]// left map values never match
[[SEP]]// left and right string values match when right.contains(left)
[[SEP]]// right list value matches when right.contains(left)
[[SEP]]// non-string values match when left == right
",// left list values never match[[SEP]]// left map values never match[[SEP]]// left and right string values match when right.contains(left)[[SEP]]// right list value matches when right.contains(left)[[SEP]]// non-string values match when left == right,78,145,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,testInclusionWithFieldInField(),org.logstash.config.ir.EventConditionTest,testInclusionWithFieldInField/0,False,80,13,12,0,12,1,20,37,0,14,0,20,1,1,0,0,0,0,38,9,14,1,0,0,0,2,55,1,0,False
628,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\EventConditionTest.java,org.logstash.config.ir.EventConditionTest,void testConditionWithSecretStoreVariable(),"@Test
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public void testConditionWithSecretStoreVariable() throws InvalidIRException {
    ConfigVariableExpander cve = ConfigVariableExpanderTest.getFakeCve(Collections.singletonMap(""secret_key"", ""s3cr3t""), Collections.emptyMap());
    final PipelineIR pipelineIR = ConfigCompiler.configToPipelineIR(IRHelpers.toSourceWithMetadata(""input {mockinput{}} "" + ""output { "" + ""  if [left] == \""${secret_key}\"" { "" + ""    mockoutput{}"" + ""  } }""), false, cve);
    // left and right string values match when right.contains(left)
    RubyEvent leftIsString1 = RubyEvent.newRubyEvent(RubyUtil.RUBY);
    leftIsString1.getEvent().setField(""left"", ""s3cr3t"");
    RubyArray inputBatch = RubyUtil.RUBY.newArray(leftIsString1);
    new CompiledPipeline(pipelineIR, new CompiledPipelineTest.MockPluginFactory(Collections.singletonMap(""mockinput"", nullInputSupplier()), // no filters
    Collections.emptyMap(), Collections.singletonMap(""mockoutput"", mockOutputSupplier()))).buildExecution().compute(inputBatch, false, false);
    final RubyEvent[] outputEvents = EVENT_SINKS.get(runId).toArray(new RubyEvent[0]);
    assertThat(outputEvents.length, is(1));
    assertThat(outputEvents[0], is(leftIsString1));
}", ,"// left and right string values match when right.contains(left)
[[SEP]]// no filters
",// left and right string values match when right.contains(left)[[SEP]]// no filters,199,232,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testConditionWithSecretStoreVariable(),org.logstash.config.ir.EventConditionTest,testConditionWithSecretStoreVariable/0,False,201,13,12,0,12,1,19,11,0,5,0,19,2,1,0,0,0,0,13,3,5,1,0,0,0,0,34,1,0,False
629,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\IRHelpers.java,org.logstash.config.ir.IRHelpers,SourceWithMetadata randMeta(),"public static SourceWithMetadata randMeta() {
    try {
        return randMeta(new Random());
    } catch (IncompleteSourceWithMetadataException e) {
        // Never happens, or if it does, the whole test suite is broken anyway
        throw new RuntimeException(e);
    }
}", ,"// Never happens, or if it does, the whole test suite is broken anyway
","// Never happens, or if it does, the whole test suite is broken anyway",168,175,[0],0,[0],0,[0],0,0,0,0,randMeta(),org.logstash.config.ir.IRHelpers,randMeta/0,False,168,2,23,22,1,2,1,8,1,0,0,1,1,2,0,0,1,0,0,0,0,0,1,0,0,0,10,9,0,False
630,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\IRHelpers.java,org.logstash.config.ir.IRHelpers,RubyArray toSourceWithMetadataFromPath(String),"/**
 * Load pipeline configuration from a path returning the list of SourceWithMetadata.
 *
 * The path refers to test's resources, if it point to single file that file is loaded, if reference a directory
 * then the full list of contained files is loaded in name order.
 */
@SuppressWarnings(""rawtypes"")
public static RubyArray toSourceWithMetadataFromPath(String configPath) throws IncompleteSourceWithMetadataException, IOException {
    URL url = IRHelpers.class.getClassLoader().getResource(configPath);
    String path = url.getPath();
    final File filePath = new File(path);
    final List<File> files;
    if (filePath.isDirectory()) {
        files = Arrays.asList(filePath.listFiles());
        Collections.sort(files);
    } else {
        files = Collections.singletonList(filePath);
    }
    List<IRubyObject> rubySwms = new ArrayList<>();
    for (File configFile : files) {
        final List<String> fileContent = Files.readLines(configFile, Charset.defaultCharset());
        final SourceWithMetadata swm = new SourceWithMetadata(""file"", configFile.getPath(), 1, 1, String.join(""\n"", fileContent));
        rubySwms.add(JavaUtil.convertJavaToRuby(RubyUtil.RUBY, swm));
    }
    return RubyUtil.RUBY.newArray(rubySwms);
}","/**
 * Load pipeline configuration from a path returning the list of SourceWithMetadata.
 *
 * The path refers to test's resources, if it point to single file that file is loaded, if reference a directory
 * then the full list of contained files is loaded in name order.
 */
", ,"/** * Load pipeline configuration from a path returning the list of SourceWithMetadata. * * The path refers to test's resources, if it point to single file that file is loaded, if reference a directory * then the full list of contained files is loaded in name order. */",211,231,[0],0,[0],0,[0],0,0,0,0,toSourceWithMetadataFromPath(String),org.logstash.config.ir.IRHelpers,toSourceWithMetadataFromPath/1[java.lang.String],False,212,4,3,2,1,3,15,20,1,7,1,15,0,0,1,0,0,0,3,2,8,0,1,0,0,0,48,9,0,True
631,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\RubyEnvTestCase.java,org.logstash.config.ir.RubyEnvTestCase,void ensureLoadpath(),"/**
 * Loads the logstash-core/lib path if the load service can't find {@code logstash/compiler}
 * because {@code environment.rb} hasn't been loaded yet.
 */
private static void ensureLoadpath() {
    final LoadService loader = RubyUtil.RUBY.getLoadService();
    final LibrarySearcher librarySearcher = new LibrarySearcher(loader);
    if (librarySearcher.findLibraryForLoad(""logstash/compiler"") == null) {
        final String gems = LS_HOME.resolve(""vendor"").resolve(""bundle"").resolve(""jruby"").resolve(""2.6.0"").toFile().getAbsolutePath();
        final RubyHash environment = RubyUtil.RUBY.getENV();
        environment.put(""GEM_HOME"", gems);
        environment.put(""GEM_PATH"", gems);
        Path logstashCore = LS_HOME.resolve(""logstash-core"");
        loader.addPaths(logstashCore.resolve(""lib"").toFile().getAbsolutePath());
    }
}","/**
 * Loads the logstash-core/lib path if the load service can't find {@code logstash/compiler}
 * because {@code environment.rb} hasn't been loaded yet.
 */
", ,/** * Loads the logstash-core/lib path if the load service can't find {@code logstash/compiler} * because {@code environment.rb} hasn't been loaded yet. */,43,56,[0],0,[0],0,[0],0,0,0,0,ensureLoadpath(),org.logstash.config.ir.RubyEnvTestCase,ensureLoadpath/0,False,43,3,1,1,0,2,11,12,0,5,0,11,0,0,0,1,0,0,9,0,5,0,1,0,0,0,28,10,0,True
632,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\compiler\CommonActionsTest.java,org.logstash.config.ir.compiler.CommonActionsTest,void testAddField(),"@Test
public void testAddField() {
    // add field to empty event
    Event e = new Event();
    String testField = ""test_field"";
    String testStringValue = ""test_value"";
    CommonActions.addField(e, Collections.singletonMap(testField, testStringValue));
    Assert.assertEquals(testStringValue, e.getField(testField));
    // add to existing field and convert to array value
    e = new Event(Collections.singletonMap(testField, testStringValue));
    CommonActions.addField(e, Collections.singletonMap(testField, testStringValue));
    Object value = e.getField(testField);
    Assert.assertTrue(value instanceof List);
    Assert.assertEquals(2, ((List) value).size());
    Assert.assertEquals(testStringValue, ((List) value).get(0));
    Assert.assertEquals(testStringValue, ((List) value).get(1));
    // add to existing array field
    String testStringValue2 = ""test_value2"";
    List<String> stringVals = Arrays.asList(testStringValue, testStringValue2);
    e = new Event(Collections.singletonMap(testField, stringVals));
    CommonActions.addField(e, Collections.singletonMap(testField, testStringValue));
    value = e.getField(testField);
    Assert.assertTrue(value instanceof List);
    Assert.assertEquals(3, ((List) value).size());
    Assert.assertEquals(testStringValue, ((List) value).get(0));
    Assert.assertEquals(testStringValue2, ((List) value).get(1));
    Assert.assertEquals(testStringValue, ((List) value).get(2));
    // add non-string value to empty event
    Long testLongValue = 42L;
    e = new Event();
    CommonActions.addField(e, Collections.singletonMap(testField, testLongValue));
    Assert.assertEquals(testLongValue, e.getField(testField));
    // add non-string value to existing field
    e = new Event(Collections.singletonMap(testField, testStringValue));
    CommonActions.addField(e, Collections.singletonMap(testField, testLongValue));
    value = e.getField(testField);
    Assert.assertTrue(value instanceof List);
    Assert.assertEquals(2, ((List) value).size());
    Assert.assertEquals(testStringValue, ((List) value).get(0));
    Assert.assertEquals(testLongValue, ((List) value).get(1));
    // add non-string value to existing array field
    e = new Event(Collections.singletonMap(testField, stringVals));
    CommonActions.addField(e, Collections.singletonMap(testField, testLongValue));
    value = e.getField(testField);
    Assert.assertTrue(value instanceof List);
    Assert.assertEquals(3, ((List) value).size());
    Assert.assertEquals(testStringValue, ((List) value).get(0));
    Assert.assertEquals(testStringValue2, ((List) value).get(1));
    Assert.assertEquals(testLongValue, ((List) value).get(2));
    // add field/value with dynamic values
    e = new Event(Collections.singletonMap(testField, testStringValue));
    String newField = ""%{"" + testField + ""}_field"";
    String newValue = ""%{"" + testField + ""}_value"";
    CommonActions.addField(e, Collections.singletonMap(newField, newValue));
    Assert.assertEquals(testStringValue + ""_value"", e.getField(testStringValue + ""_field""));
}", ,"// add field to empty event
[[SEP]]// add to existing field and convert to array value
[[SEP]]// add to existing array field
[[SEP]]// add non-string value to empty event
[[SEP]]// add non-string value to existing field
[[SEP]]// add non-string value to existing array field
[[SEP]]// add field/value with dynamic values
",// add field to empty event[[SEP]]// add to existing field and convert to array value[[SEP]]// add to existing array field[[SEP]]// add non-string value to empty event[[SEP]]// add non-string value to existing field[[SEP]]// add non-string value to existing array field[[SEP]]// add field/value with dynamic values,38,99,[0],0,"[0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0, 0]",0,0,0,0,testAddField(),org.logstash.config.ir.compiler.CommonActionsTest,testAddField/0,False,39,3,4,0,4,1,8,48,0,9,0,8,0,0,0,0,0,14,9,15,18,4,0,0,0,0,17,1,0,False
633,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\compiler\CommonActionsTest.java,org.logstash.config.ir.compiler.CommonActionsTest,void testAddTag(),"@Test
public void testAddTag() {
    // add tag to empty event
    Event e = new Event();
    String testTag = ""test_tag"";
    CommonActions.addTag(e, Collections.singletonList(testTag));
    Object value = e.getField(TAGS);
    Assert.assertTrue(value instanceof List);
    Assert.assertEquals(1, ((List) value).size());
    Assert.assertEquals(testTag, ((List) value).get(0));
    // add two tags to empty event
    e = new Event();
    String testTag2 = ""test_tag2"";
    CommonActions.addTag(e, Arrays.asList(testTag, testTag2));
    value = e.getField(TAGS);
    Assert.assertTrue(value instanceof List);
    Assert.assertEquals(2, ((List) value).size());
    Assert.assertEquals(testTag, ((List) value).get(0));
    Assert.assertEquals(testTag2, ((List) value).get(1));
    // add duplicate tag
    e = new Event();
    e.tag(testTag);
    CommonActions.addTag(e, Collections.singletonList(testTag));
    value = e.getField(TAGS);
    Assert.assertTrue(value instanceof List);
    Assert.assertEquals(1, ((List) value).size());
    Assert.assertEquals(testTag, ((List) value).get(0));
    // add dynamically-named tag
    e = new Event(Collections.singletonMap(testTag, testTag2));
    CommonActions.addTag(e, Collections.singletonList(""%{"" + testTag + ""}_foo""));
    value = e.getField(TAGS);
    Assert.assertTrue(value instanceof List);
    Assert.assertEquals(1, ((List) value).size());
    Assert.assertEquals(testTag2 + ""_foo"", ((List) value).get(0));
    // add non-string tag
    e = new Event();
    Long nonStringTag = 42L;
    CommonActions.addTag(e, Collections.singletonList(nonStringTag));
    value = e.getField(TAGS);
    Assert.assertTrue(value instanceof List);
    Assert.assertEquals(1, ((List) value).size());
    Assert.assertEquals(nonStringTag.toString(), ((List) value).get(0));
}", ,"// add tag to empty event
[[SEP]]// add two tags to empty event
[[SEP]]// add duplicate tag
[[SEP]]// add dynamically-named tag
[[SEP]]// add non-string tag
",// add tag to empty event[[SEP]]// add two tags to empty event[[SEP]]// add duplicate tag[[SEP]]// add dynamically-named tag[[SEP]]// add non-string tag,101,148,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,testAddTag(),org.logstash.config.ir.compiler.CommonActionsTest,testAddTag/0,False,102,3,5,0,5,1,11,37,0,5,0,11,0,0,0,0,0,11,5,12,13,2,0,0,0,0,18,1,0,False
634,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\compiler\CommonActionsTest.java,org.logstash.config.ir.compiler.CommonActionsTest,void testAddType(),"@Test
public void testAddType() {
    // add tag to empty event
    Map<String, Object> e = new HashMap<>();
    String testType = ""test_type"";
    Map<String, Object> e2 = CommonActions.addType(e, testType);
    Assert.assertEquals(testType, e2.get(""type""));
    // add type to already-typed event
    e = new HashMap<>();
    String existingType = ""existing_type"";
    e.put(""type"", existingType);
    e2 = CommonActions.addType(e, testType);
    Assert.assertEquals(existingType, e2.get(""type""));
}", ,"// add tag to empty event
[[SEP]]// add type to already-typed event
",// add tag to empty event[[SEP]]// add type to already-typed event,150,164,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testAddType(),org.logstash.config.ir.compiler.CommonActionsTest,testAddType/0,False,151,2,1,0,1,1,4,11,0,4,0,4,0,0,0,0,0,0,5,0,6,0,0,0,0,0,6,1,0,False
635,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\compiler\CommonActionsTest.java,org.logstash.config.ir.compiler.CommonActionsTest,void testRemoveField(),"@Test
public void testRemoveField() {
    // remove a field
    Event e = new Event();
    String testField = ""test_field"";
    String testValue = ""test_value"";
    e.setField(testField, testValue);
    CommonActions.removeField(e, Collections.singletonList(testField));
    Assert.assertFalse(e.getData().keySet().contains(testField));
    // remove non-existent field
    e = new Event();
    String testField2 = ""test_field2"";
    e.setField(testField2, testValue);
    CommonActions.removeField(e, Collections.singletonList(testField));
    Assert.assertFalse(e.getData().keySet().contains(testField));
    Assert.assertTrue(e.getData().keySet().contains(testField2));
    // remove multiple fields
    e = new Event();
    List<String> fields = new ArrayList<>();
    for (int k = 0; k < 3; k++) {
        String field = testField + k;
        e.setField(field, testValue);
        fields.add(field);
    }
    e.setField(testField, testValue);
    CommonActions.removeField(e, fields);
    for (String field : fields) {
        Assert.assertFalse(e.getData().keySet().contains(field));
    }
    Assert.assertTrue(e.getData().keySet().contains(testField));
    // remove dynamically-named field
    e = new Event();
    String otherField = ""other_field"";
    String otherValue = ""other_value"";
    e.setField(otherField, otherValue);
    String derivativeField = otherValue + ""_foo"";
    e.setField(derivativeField, otherValue);
    CommonActions.removeField(e, Collections.singletonList(""%{"" + otherField + ""}_foo""));
    Assert.assertFalse(e.getData().keySet().contains(derivativeField));
    Assert.assertTrue(e.getData().keySet().contains(otherField));
}", ,"// remove a field
[[SEP]]// remove non-existent field
[[SEP]]// remove multiple fields
[[SEP]]// remove dynamically-named field
",// remove a field[[SEP]]// remove non-existent field[[SEP]]// remove multiple fields[[SEP]]// remove dynamically-named field,166,209,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,testRemoveField(),org.logstash.config.ir.compiler.CommonActionsTest,testRemoveField/0,False,167,3,4,0,4,3,9,36,0,10,0,9,0,0,2,0,0,0,8,2,13,3,1,0,0,0,12,1,0,False
636,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\compiler\CommonActionsTest.java,org.logstash.config.ir.compiler.CommonActionsTest,void testRemoveTag(),"@Test
public void testRemoveTag() {
    // remove a tag
    Event e = new Event();
    String testTag = ""test_tag"";
    e.tag(testTag);
    CommonActions.removeTag(e, Collections.singletonList(testTag));
    Object o = e.getField(TAGS);
    Assert.assertTrue(o instanceof List);
    Assert.assertEquals(0, ((List) o).size());
    // remove non-existent tag
    e = new Event();
    e.tag(testTag);
    CommonActions.removeTag(e, Collections.singletonList(testTag + ""non-existent""));
    o = e.getField(TAGS);
    Assert.assertTrue(o instanceof List);
    Assert.assertEquals(1, ((List) o).size());
    Assert.assertEquals(testTag, ((List) o).get(0));
    // remove multiple tags
    e = new Event();
    List<String> tags = new ArrayList<>();
    for (int k = 0; k < 3; k++) {
        String tag = testTag + k;
        tags.add(tag);
        e.tag(tag);
    }
    CommonActions.removeTag(e, tags);
    o = e.getField(TAGS);
    Assert.assertTrue(o instanceof List);
    Assert.assertEquals(0, ((List) o).size());
    // remove tags when ""tags"" fields isn't tags
    e = new Event();
    Long nonTagValue = 42L;
    e.setField(TAGS, nonTagValue);
    CommonActions.removeTag(e, Collections.singletonList(testTag));
    o = e.getField(TAGS);
    Assert.assertFalse(o instanceof List);
    Assert.assertEquals(nonTagValue, o);
    // remove dynamically-named tag
    e = new Event();
    String otherField = ""other_field"";
    String otherValue = ""other_value"";
    e.setField(otherField, otherValue);
    e.tag(otherValue + ""_foo"");
    CommonActions.removeTag(e, Collections.singletonList(""%{"" + otherField + ""}_foo""));
    o = e.getField(TAGS);
    Assert.assertTrue(o instanceof List);
    Assert.assertEquals(0, ((List) o).size());
}", ,"// remove a tag
[[SEP]]// remove non-existent tag
[[SEP]]// remove multiple tags
[[SEP]]// remove tags when ""tags"" fields isn't tags
[[SEP]]// remove dynamically-named tag
","// remove a tag[[SEP]]// remove non-existent tag[[SEP]]// remove multiple tags[[SEP]]// remove tags when ""tags"" fields isn't tags[[SEP]]// remove dynamically-named tag",211,264,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,testRemoveTag(),org.logstash.config.ir.compiler.CommonActionsTest,testRemoveTag/0,False,212,3,5,0,5,2,11,43,0,9,0,11,0,0,1,0,0,5,7,8,17,4,1,0,0,0,22,1,0,False
637,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\compiler\CommonActionsTest.java,org.logstash.config.ir.compiler.CommonActionsTest,void testRemoveFieldFromMetadata(),"@Test
public void testRemoveFieldFromMetadata() {
    // remove a field
    Event e = new Event();
    String testField = ""test_field"";
    String testFieldRef = ""[@metadata]["" + testField + ""]"";
    String testValue = ""test_value"";
    e.setField(testFieldRef, testValue);
    CommonActions.removeField(e, Collections.singletonList(testFieldRef));
    Assert.assertFalse(e.getMetadata().keySet().contains(testField));
    // remove non-existent field
    e = new Event();
    String testField2 = ""test_field2"";
    String testField2Ref = ""[@metadata]["" + testField2 + ""]"";
    e.setField(testField2Ref, testValue);
    CommonActions.removeField(e, Collections.singletonList(testFieldRef));
    Assert.assertFalse(e.getMetadata().keySet().contains(testField));
    Assert.assertTrue(e.getMetadata().keySet().contains(testField2));
    // remove multiple fields
    e = new Event();
    List<String> fields = new ArrayList<>();
    List<String> fieldsRef = new ArrayList<>();
    for (int k = 0; k < 3; k++) {
        String field = testField + k;
        String fieldRef = ""[@metadata]["" + field + ""]"";
        e.setField(fieldRef, testValue);
        fields.add(field);
        fieldsRef.add(fieldRef);
    }
    e.setField(testFieldRef, testValue);
    CommonActions.removeField(e, fieldsRef);
    for (String field : fields) {
        Assert.assertFalse(e.getMetadata().keySet().contains(field));
    }
    Assert.assertTrue(e.getMetadata().keySet().contains(testField));
    // remove dynamically-named field
    e = new Event();
    String otherField = ""other_field"";
    String otherFieldRef = ""[@metadata]["" + otherField + ""]"";
    String otherValue = ""other_value"";
    e.setField(otherFieldRef, otherValue);
    String derivativeField = otherValue + ""_foo"";
    String derivativeFieldRef = ""[@metadata]["" + derivativeField + ""]"";
    e.setField(derivativeFieldRef, otherValue);
    CommonActions.removeField(e, Collections.singletonList(""[@metadata]["" + ""%{"" + otherField + ""}_foo"" + ""]""));
    Assert.assertFalse(e.getMetadata().keySet().contains(derivativeField));
    Assert.assertTrue(e.getMetadata().keySet().contains(otherField));
}", ,"// remove a field
[[SEP]]// remove non-existent field
[[SEP]]// remove multiple fields
[[SEP]]// remove dynamically-named field
",// remove a field[[SEP]]// remove non-existent field[[SEP]]// remove multiple fields[[SEP]]// remove dynamically-named field,266,316,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,testRemoveFieldFromMetadata(),org.logstash.config.ir.compiler.CommonActionsTest,testRemoveFieldFromMetadata/0,False,267,3,4,0,4,3,9,43,0,16,0,9,0,0,2,0,0,0,20,2,19,9,1,0,0,0,15,1,0,False
638,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\compiler\DatasetCompilerTest.java,org.logstash.config.ir.compiler.DatasetCompilerTest,void compilesOutputDataset(),"/**
 * Smoke test ensuring that output {@link Dataset} is compiled correctly.
 */
@Test
public void compilesOutputDataset() {
    assertThat(DatasetCompiler.outputDataset(Collections.emptyList(), PipelineTestUtil.buildOutput(events -> {
    }), true).instantiate().compute(RubyUtil.RUBY.newArray(), false, false), nullValue());
}","/**
 * Smoke test ensuring that output {@link Dataset} is compiled correctly.
 */
", ,/** * Smoke test ensuring that output {@link Dataset} is compiled correctly. */,41,51,[0],0,[0],0,[0],0,0,0,0,compilesOutputDataset(),org.logstash.config.ir.compiler.DatasetCompilerTest,compilesOutputDataset/0,False,42,5,4,0,4,1,8,5,0,1,0,8,0,0,0,0,0,0,0,0,0,0,1,0,0,1,13,1,0,True
639,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\compiler\FakeOutClass.java,org.logstash.config.ir.compiler.FakeOutClass,IRubyObject multiReceive(IRubyObject),"@JRubyMethod(name = AbstractOutputDelegatorExt.OUTPUT_METHOD_NAME)
public IRubyObject multiReceive(final IRubyObject args) {
    multiReceiveCallCount++;
    multiReceiveArgs = args;
    if (multiReceiveDelay > 0) {
        try {
            Thread.sleep(multiReceiveDelay);
        } catch (InterruptedException e) {
            // do nothing
        }
    }
    return this;
}", ,"// do nothing
",// do nothing,94,106,[0],0,[0],0,[0],0,0,0,0,multiReceive(IRubyObject),org.logstash.config.ir.compiler.FakeOutClass,multiReceive/1[org.logstash.config.ir.compiler.IRubyObject],False,95,3,0,0,0,3,1,12,1,0,1,1,0,0,0,0,1,0,0,1,1,0,2,0,0,0,14,1,0,False
640,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\compiler\OutputDelegatorTest.java,org.logstash.config.ir.compiler.OutputDelegatorTest,void outputStrategyTests(),"@Test
public void outputStrategyTests() {
    StrategyPair[] outputStrategies = new StrategyPair[] { new StrategyPair(""shared"", OutputStrategyExt.SharedOutputStrategyExt.class), new StrategyPair(""single"", OutputStrategyExt.SingleOutputStrategyExt.class), new StrategyPair(""legacy"", OutputStrategyExt.LegacyOutputStrategyExt.class) };
    for (StrategyPair pair : outputStrategies) {
        FakeOutClass.setOutStrategy(RUBY.getCurrentContext(), null, pair.symbol);
        OutputDelegatorExt outputDelegator = constructOutputDelegator();
        // test that output strategies are properly set
        IRubyObject outStrategy = outputDelegator.concurrency(RUBY.getCurrentContext());
        assertEquals(pair.symbol, outStrategy);
        // test that strategy classes are correctly instantiated
        IRubyObject strategyClass = outputDelegator.strategy();
        assertThat(strategyClass).isInstanceOf(pair.klazz);
        // test that metrics are properly set on the instance
        assertEquals(outputDelegator.namespacedMetric(), FakeOutClass.latestInstance.getMetricArgs());
    }
}", ,"// test that output strategies are properly set
[[SEP]]// test that strategy classes are correctly instantiated
[[SEP]]// test that metrics are properly set on the instance
",// test that output strategies are properly set[[SEP]]// test that strategy classes are correctly instantiated[[SEP]]// test that metrics are properly set on the instance,146,169,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,outputStrategyTests(),org.logstash.config.ir.compiler.OutputDelegatorTest,outputStrategyTests/0,False,147,10,7,0,7,2,10,12,0,4,0,10,1,1,1,0,0,0,3,0,4,0,1,0,0,0,19,1,0,False
641,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\graph\GraphTest.java,org.logstash.config.ir.graph.GraphTest,void testGraphCycleDetection(),"// Expect an Invalid IR Exception from the cycle
@Test(expected = org.logstash.config.ir.InvalidIRException.class)
public void testGraphCycleDetection() throws InvalidIRException {
    Graph g = Graph.empty();
    Vertex v1 = IRHelpers.createTestVertex();
    Vertex v2 = IRHelpers.createTestVertex();
    g.chainVertices(v1, v2);
    g.chainVertices(v2, v1);
}","// Expect an Invalid IR Exception from the cycle
", ,// Expect an Invalid IR Exception from the cycle,53,60,[0],0,[0],0,[0],0,0,0,0,testGraphCycleDetection(),org.logstash.config.ir.graph.GraphTest,testGraphCycleDetection/0,False,54,5,3,0,3,1,3,7,0,3,0,3,0,0,0,0,0,0,0,0,3,0,0,0,0,0,13,1,0,False
642,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\graph\IfVertexTest.java,org.logstash.config.ir.graph.IfVertexTest,void testIfVertexWithSecretsIsntLeaked(),"@Test
public void testIfVertexWithSecretsIsntLeaked() throws InvalidIRException {
    BooleanExpression booleanExpression = DSL.eEq(DSL.eEventValue(""password""), DSL.eValue(""${secret_key}""));
    ConfigVariableExpander cve = ConfigVariableExpanderTest.getFakeCve(Collections.singletonMap(""secret_key"", ""s3cr3t""), Collections.emptyMap());
    IfVertex ifVertex = new IfVertex(randMeta(), (BooleanExpression) ExpressionSubstitution.substituteBoolExpression(cve, booleanExpression));
    // Exercise
    String output = ifVertex.toString();
    // Verify
    assertThat(output, not(containsString(""s3cr3t"")));
}", ,"// Exercise
[[SEP]]// Verify
",// Exercise[[SEP]]// Verify,91,106,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testIfVertexWithSecretsIsntLeaked(),org.logstash.config.ir.graph.IfVertexTest,testIfVertexWithSecretsIsntLeaked/0,False,92,8,8,0,8,1,12,7,0,4,0,12,0,0,0,0,0,0,5,0,4,0,0,0,0,0,28,1,0,False
643,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\graph\QueueVertexTest.java,org.logstash.config.ir.graph.QueueVertexTest,void testConstruction(),"@Test
public void testConstruction() {
    try {
        new QueueVertex();
    } catch (IncompleteSourceWithMetadataException e) {
        // never happens
        throw new RuntimeException(e);
    }
}", ,"// never happens
",// never happens,27,35,[0],0,[0],0,[0],0,0,0,0,testConstruction(),org.logstash.config.ir.graph.QueueVertexTest,testConstruction/0,False,28,2,1,0,1,2,0,8,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,11,1,0,False
644,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\graph\VertexTest.java,org.logstash.config.ir.graph.VertexTest,void testIsLeafAndRoot(),"@Test
public void testIsLeafAndRoot() throws InvalidIRException {
    Graph graph = Graph.empty();
    Vertex v = IRHelpers.createTestVertex();
    graph.addVertex(v);
    // Nodes should be leaves and roots if they are isolated
    assertTrue(v.isLeaf());
    assertTrue(v.isRoot());
    Vertex otherV = IRHelpers.createTestVertex();
    graph.chainVertices(v, otherV);
    assertFalse(v.isLeaf());
    assertTrue(v.isRoot());
    assertTrue(otherV.isLeaf());
    assertFalse(otherV.isRoot());
}", ,"// Nodes should be leaves and roots if they are isolated
",// Nodes should be leaves and roots if they are isolated,41,59,[0],0,[0],0,[0],0,0,0,0,testIsLeafAndRoot(),org.logstash.config.ir.graph.VertexTest,testIsLeafAndRoot/0,False,42,4,6,0,6,1,8,13,0,3,0,8,0,0,0,0,0,0,0,0,3,0,0,0,0,0,18,1,0,False
645,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\graph\algorithms\BreadthFirstTest.java,org.logstash.config.ir.graph.algorithms.BreadthFirstTest,void testBFSBasic(),"@Test
public void testBFSBasic() throws InvalidIRException {
    Graph g = Graph.empty();
    g.chainVertices(IRHelpers.createTestVertex(), IRHelpers.createTestVertex(), IRHelpers.createTestVertex());
    // We don't *really* need threadsafety for the count,
    // but since we're using a lambda we need something that's final
    final AtomicInteger visitCount = new AtomicInteger();
    BreadthFirst.BfsResult res = BreadthFirst.breadthFirst(g.getRoots(), false, (v -> visitCount.incrementAndGet()));
    assertEquals(""It should visit each node once"", visitCount.get(), 3);
    assertThat(res.getVertices(), is(g.getVertices()));
}", ,"// We don't *really* need threadsafety for the count,
[[SEP]]// but since we're using a lambda we need something that's final
","// We don't *really* need threadsafety for the count,// but since we're using a lambda we need something that's final",35,47,[0],0,"[0, 0]",0,[0],0,0,0,0,testBFSBasic(),org.logstash.config.ir.graph.algorithms.BreadthFirstTest,testBFSBasic/0,False,36,5,7,0,7,1,12,8,0,4,0,12,0,0,0,0,0,1,1,1,3,0,0,0,0,1,23,1,0,False
646,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\imperative\ImperativeToGraphtest.java,org.logstash.config.ir.imperative.ImperativeToGraphtest,void convertSimpleExpression(),"@Test
public void convertSimpleExpression() throws InvalidIRException {
    ConfigVariableExpander cve = ConfigVariableExpander.withoutSecret(EnvironmentVariableProvider.defaultProvider());
    Graph imperative = iComposeSequence(randMeta(), iPlugin(randMeta(), FILTER, ""json""), iPlugin(randMeta(), FILTER, ""stuff"")).toGraph(cve);
    // Verify this is a valid graph
    imperative.validate();
    Graph regular = Graph.empty();
    regular.chainVertices(gPlugin(randMeta(), FILTER, ""json""), gPlugin(randMeta(), FILTER, ""stuff""));
    assertSyntaxEquals(imperative, regular);
}", ,"// Verify this is a valid graph
",// Verify this is a valid graph,38,48,[0],0,[0],0,[0],0,0,0,0,convertSimpleExpression(),org.logstash.config.ir.imperative.ImperativeToGraphtest,convertSimpleExpression/0,False,39,7,11,0,11,1,11,8,0,3,0,11,0,0,0,0,0,0,4,0,3,0,0,0,0,0,24,1,0,False
647,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\imperative\ImperativeToGraphtest.java,org.logstash.config.ir.imperative.ImperativeToGraphtest,void testIdsDontAffectSourceComponentEquality(),"@Test
public void testIdsDontAffectSourceComponentEquality() throws InvalidIRException {
    ConfigVariableExpander cve = ConfigVariableExpander.withoutSecret(EnvironmentVariableProvider.defaultProvider());
    Graph imperative = iComposeSequence(iPlugin(randMeta(), FILTER, ""json"", ""oneid""), iPlugin(randMeta(), FILTER, ""stuff"", ""anotherid"")).toGraph(cve);
    // Verify this is a valid graph
    imperative.validate();
    Graph regular = Graph.empty();
    regular.chainVertices(gPlugin(randMeta(), FILTER, ""json"", ""someotherid""), gPlugin(randMeta(), FILTER, ""stuff"", ""graphid""));
    assertSyntaxEquals(imperative, regular);
}", ,"// Verify this is a valid graph
",// Verify this is a valid graph,50,66,[0],0,[0],0,[0],0,0,0,0,testIdsDontAffectSourceComponentEquality(),org.logstash.config.ir.imperative.ImperativeToGraphtest,testIdsDontAffectSourceComponentEquality/0,False,51,7,11,0,11,1,11,8,0,3,0,11,0,0,0,0,0,0,8,0,3,0,0,0,0,0,28,1,0,False
648,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\imperative\ImperativeToGraphtest.java,org.logstash.config.ir.imperative.ImperativeToGraphtest,void convertComplexExpression(),"@Test
public void convertComplexExpression() throws InvalidIRException {
    ConfigVariableExpander cve = ConfigVariableExpander.withoutSecret(EnvironmentVariableProvider.defaultProvider());
    Graph imperative = iComposeSequence(iPlugin(randMeta(), FILTER, ""p1""), iPlugin(randMeta(), FILTER, ""p2""), iIf(randMeta(), eAnd(eTruthy(eValue(5l)), eTruthy(eValue(null))), iPlugin(randMeta(), FILTER, ""p3""), iComposeSequence(iPlugin(randMeta(), FILTER, ""p4""), iPlugin(randMeta(), FILTER, ""p5"")))).toGraph(cve);
    // Verify this is a valid graph
    imperative.validate();
    PluginVertex p1 = gPlugin(randMeta(), FILTER, ""p1"");
    PluginVertex p2 = gPlugin(randMeta(), FILTER, ""p2"");
    PluginVertex p3 = gPlugin(randMeta(), FILTER, ""p3"");
    PluginVertex p4 = gPlugin(randMeta(), FILTER, ""p4"");
    PluginVertex p5 = gPlugin(randMeta(), FILTER, ""p5"");
    IfVertex testIf = gIf(randMeta(), eAnd(eTruthy(eValue(5l)), eTruthy(eValue(null))));
    Graph expected = Graph.empty();
    expected.chainVertices(p1, p2, testIf);
    expected.chainVertices(true, testIf, p3);
    expected.chainVertices(false, testIf, p4);
    expected.chainVertices(p4, p5);
    assertSyntaxEquals(expected, imperative);
}", ,"// Verify this is a valid graph
",// Verify this is a valid graph,68,95,[0],0,[0],0,[0],0,0,0,0,convertComplexExpression(),org.logstash.config.ir.imperative.ImperativeToGraphtest,convertComplexExpression/0,False,69,9,19,0,19,1,19,17,0,9,0,19,0,0,0,0,0,0,10,2,9,0,0,0,0,0,37,1,0,False
649,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\imperative\ImperativeToGraphtest.java,org.logstash.config.ir.imperative.ImperativeToGraphtest,void deepDanglingPartialLeaves(),"// This test has an imperative grammar with nested ifs and dangling
// partial leaves. This makes sure they all wire-up right
@Test
public void deepDanglingPartialLeaves() throws InvalidIRException {
    ConfigVariableExpander cve = ConfigVariableExpander.withoutSecret(EnvironmentVariableProvider.defaultProvider());
    Graph imperative = iComposeSequence(iPlugin(randMeta(), FILTER, ""p0""), iIf(randMeta(), eTruthy(eValue(1)), iPlugin(randMeta(), FILTER, ""p1""), iIf(randMeta(), eTruthy(eValue(3)), iPlugin(randMeta(), FILTER, ""p5""))), iIf(randMeta(), eTruthy(eValue(2)), iPlugin(randMeta(), FILTER, ""p3""), iPlugin(randMeta(), FILTER, ""p4"")), iPlugin(randMeta(), FILTER, ""pLast"")).toGraph(cve);
    // Verify this is a valid graph
    imperative.validate();
    IfVertex if1 = gIf(randMeta(), eTruthy(eValue(1)));
    IfVertex if2 = gIf(randMeta(), eTruthy(eValue(2)));
    IfVertex if3 = gIf(randMeta(), eTruthy(eValue(3)));
    PluginVertex p0 = gPlugin(randMeta(), FILTER, ""p0"");
    PluginVertex p1 = gPlugin(randMeta(), FILTER, ""p1"");
    PluginVertex p2 = gPlugin(randMeta(), FILTER, ""p2"");
    PluginVertex p3 = gPlugin(randMeta(), FILTER, ""p3"");
    PluginVertex p4 = gPlugin(randMeta(), FILTER, ""p4"");
    PluginVertex p5 = gPlugin(randMeta(), FILTER, ""p5"");
    PluginVertex pLast = gPlugin(randMeta(), FILTER, ""pLast"");
    Graph expected = Graph.empty();
    expected.chainVertices(p0, if1);
    expected.chainVertices(true, if1, p1);
    expected.chainVertices(false, if1, if3);
    expected.chainVertices(true, if3, p5);
    expected.chainVertices(false, if3, if2);
    expected.chainVertices(p5, if2);
    expected.chainVertices(p1, if2);
    expected.chainVertices(true, if2, p3);
    expected.chainVertices(false, if2, p4);
    expected.chainVertices(p3, pLast);
    expected.chainVertices(p4, pLast);
    assertSyntaxEquals(imperative, expected);
}","// partial leaves. This makes sure they all wire-up right
","// Verify this is a valid graph
",// This test has an imperative grammar with nested ifs and dangling// partial leaves. This makes sure they all wire-up right[[SEP]]// Verify this is a valid graph,99,143,[0],0,[0],0,"[0, 0]",0,0,0,0,deepDanglingPartialLeaves(),org.logstash.config.ir.imperative.ImperativeToGraphtest,deepDanglingPartialLeaves/0,False,100,9,17,0,17,1,17,28,0,13,0,17,0,0,0,0,0,0,13,6,13,0,0,0,0,0,40,1,0,False
650,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\config\ir\imperative\ImperativeToGraphtest.java,org.logstash.config.ir.imperative.ImperativeToGraphtest,void convertComplexExpressionWithTerminal(),"// This is a good test for what the filter block will do, where there
// will be a  composed set of ifs potentially, all of which must terminate at a
// single node
@Test
public void convertComplexExpressionWithTerminal() throws InvalidIRException {
    ConfigVariableExpander cve = ConfigVariableExpander.withoutSecret(EnvironmentVariableProvider.defaultProvider());
    Graph imperative = iComposeSequence(iPlugin(randMeta(), FILTER, ""p1""), iIf(randMeta(), eTruthy(eValue(1)), iComposeSequence(iIf(randMeta(), eTruthy(eValue(2)), noop(), iPlugin(randMeta(), FILTER, ""p2"")), iIf(randMeta(), eTruthy(eValue(3)), iPlugin(randMeta(), FILTER, ""p3""), noop())), iComposeSequence(iIf(randMeta(), eTruthy(eValue(4)), iPlugin(randMeta(), FILTER, ""p4"")), iPlugin(randMeta(), FILTER, ""p5""))), iPlugin(randMeta(), FILTER, ""terminal"")).toGraph(cve);
    // Verify this is a valid graph
    imperative.validate();
    PluginVertex p1 = gPlugin(randMeta(), FILTER, ""p1"");
    PluginVertex p2 = gPlugin(randMeta(), FILTER, ""p2"");
    PluginVertex p3 = gPlugin(randMeta(), FILTER, ""p3"");
    PluginVertex p4 = gPlugin(randMeta(), FILTER, ""p4"");
    PluginVertex p5 = gPlugin(randMeta(), FILTER, ""p5"");
    PluginVertex terminal = gPlugin(randMeta(), FILTER, ""terminal"");
    IfVertex if1 = gIf(randMeta(), eTruthy(eValue(1)));
    IfVertex if2 = gIf(randMeta(), eTruthy(eValue(2)));
    IfVertex if3 = gIf(randMeta(), eTruthy(eValue(3)));
    IfVertex if4 = gIf(randMeta(), eTruthy(eValue(4)));
    Graph expected = Graph.empty();
    expected.chainVertices(p1, if1);
    expected.chainVertices(true, if1, if2);
    expected.chainVertices(false, if1, if4);
    expected.chainVertices(true, if2, if3);
    expected.chainVertices(false, if2, p2);
    expected.chainVertices(p2, if3);
    expected.chainVertices(true, if3, p3);
    expected.chainVertices(false, if3, terminal);
    expected.chainVertices(p3, terminal);
    expected.chainVertices(true, if4, p4);
    expected.chainVertices(false, if4, p5);
    expected.chainVertices(p4, p5);
    expected.chainVertices(p5, terminal);
    assertSyntaxEquals(imperative, expected);
}","// single node
","// Verify this is a valid graph
","// This is a good test for what the filter block will do, where there// will be a  composed set of ifs potentially, all of which must terminate at a// single node[[SEP]]// Verify this is a valid graph",148,196,[0],0,[0],0,"[0, 0]",0,0,0,0,convertComplexExpressionWithTerminal(),org.logstash.config.ir.imperative.ImperativeToGraphtest,convertComplexExpressionWithTerminal/0,False,149,9,18,0,18,1,18,30,0,13,0,18,0,0,0,0,0,0,12,8,13,0,0,0,0,0,40,1,0,False
651,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\execution\ShutdownWatcherExtTest.java,org.logstash.execution.ShutdownWatcherExtTest,void pipelineWithUnsafeShutdownShouldForceShutdown(),"@Test
public void pipelineWithUnsafeShutdownShouldForceShutdown() throws InterruptedException, IOException {
    String pipeline = resolvedPipeline(false);
    watcherShutdownStallingPipeline(pipeline);
    // non drain pipeline should print stall msg
    boolean printStalling = appender.getMessages().stream().anyMatch((msg) -> msg.contains(""stalling""));
    assertTrue(printStalling);
}", ,"// non drain pipeline should print stall msg
",// non drain pipeline should print stall msg,72,80,[0],0,[0],0,[0],0,0,0,0,pipelineWithUnsafeShutdownShouldForceShutdown(),org.logstash.execution.ShutdownWatcherExtTest,pipelineWithUnsafeShutdownShouldForceShutdown/0,False,73,2,2,0,2,1,7,6,0,3,0,7,2,2,0,0,0,0,1,0,2,0,0,0,0,1,18,1,0,False
652,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\FlowMetricTest.java,org.logstash.instrument.metrics.FlowMetricTest,void testBaselineFunctionality(),"@Test
public void testBaselineFunctionality() {
    final ManualAdvanceClock clock = new ManualAdvanceClock(Instant.now());
    final LongCounter numeratorMetric = new LongCounter(MetricKeys.EVENTS_KEY.asJavaString());
    final Metric<Number> denominatorMetric = new UptimeMetric(""uptime"", clock::nanoTime).withUnitsPrecise(UptimeMetric.ScaleUnits.SECONDS);
    final FlowMetric instance = new FlowMetric(clock::nanoTime, ""flow"", numeratorMetric, denominatorMetric);
    final Map<String, Double> ratesBeforeCaptures = instance.getValue();
    assertTrue(ratesBeforeCaptures.isEmpty());
    // 5 seconds pass, during which 1000 events are processed
    clock.advance(Duration.ofSeconds(5));
    numeratorMetric.increment(1000);
    instance.capture();
    final Map<String, Double> ratesAfterFirstCapture = instance.getValue();
    assertFalse(ratesAfterFirstCapture.isEmpty());
    assertEquals(Map.of(LIFETIME_KEY, 200.0, CURRENT_KEY, 200.0), ratesAfterFirstCapture);
    // 5 more seconds pass, during which 2000 more events are processed
    clock.advance(Duration.ofSeconds(5));
    numeratorMetric.increment(2000);
    instance.capture();
    final Map<String, Double> ratesAfterSecondCapture = instance.getValue();
    assertFalse(ratesAfterSecondCapture.isEmpty());
    assertEquals(Map.of(LIFETIME_KEY, 300.0, CURRENT_KEY, 400.0), ratesAfterSecondCapture);
    // 30 seconds pass, during which 11700 more events are seen by our numerator
    for (Integer eventCount : List.of(1883, 2117, 1901, 2299, 1608, 1892)) {
        clock.advance(Duration.ofSeconds(5));
        numeratorMetric.increment(eventCount);
        instance.capture();
    }
    final Map<String, Double> ratesAfterNthCapture = instance.getValue();
    assertFalse(ratesAfterNthCapture.isEmpty());
    assertEquals(Map.of(LIFETIME_KEY, 367.5, CURRENT_KEY, 378.4), ratesAfterNthCapture);
    // less than half a second passes, during which 0 events are seen by our numerator.
    // when our two most recent captures are very close together, we want to make sure that
    // we continue to provide _meaningful_ metrics, namely that:
    // (a) our CURRENT_KEY and LIFETIME_KEY account for newest capture, and
    // (b) our CURRENT_KEY does not report _only_ the delta since the very-recent capture
    clock.advance(Duration.ofMillis(10));
    instance.capture();
    final Map<String, Double> ratesAfterSmallAdvanceCapture = instance.getValue();
    assertFalse(ratesAfterNthCapture.isEmpty());
    assertEquals(Map.of(LIFETIME_KEY, 367.408, CURRENT_KEY, 377.645), ratesAfterSmallAdvanceCapture);
}", ,"// less than half a second passes, during which 0 events are seen by our numerator.
[[SEP]]// when our two most recent captures are very close together, we want to make sure that
[[SEP]]// we continue to provide _meaningful_ metrics, namely that:
[[SEP]]// (a) our CURRENT_KEY and LIFETIME_KEY account for newest capture, and
[[SEP]]// 5 seconds pass, during which 1000 events are processed
[[SEP]]// 5 more seconds pass, during which 2000 more events are processed
[[SEP]]// 30 seconds pass, during which 11700 more events are seen by our numerator
[[SEP]]// (b) our CURRENT_KEY does not report _only_ the delta since the very-recent capture
","// 5 seconds pass, during which 1000 events are processed[[SEP]]// 5 more seconds pass, during which 2000 more events are processed[[SEP]]// 30 seconds pass, during which 11700 more events are seen by our numerator[[SEP]]// less than half a second passes, during which 0 events are seen by our numerator.// when our two most recent captures are very close together, we want to make sure that// we continue to provide _meaningful_ metrics, namely that:// (a) our CURRENT_KEY and LIFETIME_KEY account for newest capture, and// (b) our CURRENT_KEY does not report _only_ the delta since the very-recent capture",16,62,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,testBaselineFunctionality(),org.logstash.instrument.metrics.FlowMetricTest,testBaselineFunctionality/0,False,17,6,9,0,9,2,15,33,0,9,0,15,0,0,1,0,0,0,2,20,9,0,1,0,0,0,31,1,0,False
653,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\ManualAdvanceClock.java,org.logstash.instrument.metrics.ManualAdvanceClock,long nanoTime(),"/**
 * @return an only-incrementing long value, meant as a drop-in replacement
 *         for {@link System#nanoTime()}, using this {@link ManualAdvanceClock}
 *         as the time source, and carrying the same constraints.
 */
public long nanoTime() {
    return zeroInstant.until(instant(), ChronoUnit.NANOS);
}","/**
 * @return an only-incrementing long value, meant as a drop-in replacement
 *         for {@link System#nanoTime()}, using this {@link ManualAdvanceClock}
 *         as the time source, and carrying the same constraints.
 */
", ,"/** * @return an only-incrementing long value, meant as a drop-in replacement *         for {@link System#nanoTime()}, using this {@link ManualAdvanceClock} *         as the time source, and carrying the same constraints. */",50,52,[0],0,[0],0,[0],0,0,0,0,nanoTime(),org.logstash.instrument.metrics.ManualAdvanceClock,nanoTime/0,False,50,1,1,0,1,1,2,3,1,0,0,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,21,1,0,True
654,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\MetricTypeTest.java,org.logstash.instrument.metrics.MetricTypeTest,void ensurePassivity(),"/**
 * The {@link String} version of the {@link MetricType} should be considered as a public contract, and thus non-passive to change. Admittedly this test is a bit silly since it
 * just duplicates the code, but should cause a developer to think twice if they are changing the public contract.
 */
@Test
public void ensurePassivity() {
    Map<MetricType, String> nameMap = new HashMap<>(EnumSet.allOf(MetricType.class).size());
    nameMap.put(MetricType.COUNTER_LONG, ""counter/long"");
    nameMap.put(MetricType.COUNTER_DECIMAL, ""counter/decimal"");
    nameMap.put(MetricType.GAUGE_TEXT, ""gauge/text"");
    nameMap.put(MetricType.GAUGE_BOOLEAN, ""gauge/boolean"");
    nameMap.put(MetricType.GAUGE_NUMBER, ""gauge/number"");
    nameMap.put(MetricType.GAUGE_UNKNOWN, ""gauge/unknown"");
    nameMap.put(MetricType.GAUGE_RUBYHASH, ""gauge/rubyhash"");
    nameMap.put(MetricType.GAUGE_RUBYTIMESTAMP, ""gauge/rubytimestamp"");
    nameMap.put(MetricType.FLOW_RATE, ""flow/rate"");
    // ensure we are testing all of the enumerations
    assertThat(EnumSet.allOf(MetricType.class).size()).isEqualTo(nameMap.size());
    nameMap.forEach((k, v) -> assertThat(k.asString()).isEqualTo(v));
    nameMap.forEach((k, v) -> assertThat(MetricType.fromString(v)).isEqualTo(k));
}","/**
 * The {@link String} version of the {@link MetricType} should be considered as a public contract, and thus non-passive to change. Admittedly this test is a bit silly since it
 * just duplicates the code, but should cause a developer to think twice if they are changing the public contract.
 */
","// ensure we are testing all of the enumerations
","/** * The {@link String} version of the {@link MetricType} should be considered as a public contract, and thus non-passive to change. Admittedly this test is a bit silly since it * just duplicates the code, but should cause a developer to think twice if they are changing the public contract. */[[SEP]]// ensure we are testing all of the enumerations",41,59,[1],1,[0],0,"[1, 0]",1,0,1,1,ensurePassivity(),org.logstash.instrument.metrics.MetricTypeTest,ensurePassivity/0,False,42,2,2,0,2,1,9,15,0,5,0,9,0,0,0,0,0,0,9,0,1,0,0,0,0,2,40,1,0,True
655,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\UptimeMetricTest.java,org.logstash.instrument.metrics.UptimeMetricTest,void withTemporalUnit(),"@Test
public void withTemporalUnit() {
    final ManualAdvanceClock clock = new ManualAdvanceClock(Instant.now());
    final UptimeMetric uptimeMetric = new UptimeMetric(""up_millis"", clock::nanoTime);
    clock.advance(Duration.ofMillis(1_000_000_000));
    // set-up: ensure advancing nanos reflects in our milli-based uptime
    assertEquals(Long.valueOf(1_000_000_000), uptimeMetric.getValue());
    final UptimeMetric secondsBasedCopy = uptimeMetric.withTimeUnit(""up_seconds"", TimeUnit.SECONDS);
    assertEquals(Long.valueOf(1_000_000), secondsBasedCopy.getValue());
    clock.advance(Duration.ofMillis(1_999));
    assertEquals(Long.valueOf(1_000_001_999), uptimeMetric.getValue());
    assertEquals(Long.valueOf(1_000_001), secondsBasedCopy.getValue());
}", ,"// set-up: ensure advancing nanos reflects in our milli-based uptime
",// set-up: ensure advancing nanos reflects in our milli-based uptime,51,66,[0],0,[0],0,[0],0,0,0,0,withTemporalUnit(),org.logstash.instrument.metrics.UptimeMetricTest,withTemporalUnit/0,False,52,3,5,0,5,1,7,11,0,3,0,7,0,0,0,0,0,0,2,6,3,0,0,0,0,0,19,1,0,False
656,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\UptimeMetricTest.java,org.logstash.instrument.metrics.UptimeMetricTest,void withUnitsPrecise(),"@Test
public void withUnitsPrecise() {
    final ManualAdvanceClock clock = new ManualAdvanceClock(Instant.now());
    final UptimeMetric uptimeMetric = new UptimeMetric(""up_millis"", clock::nanoTime);
    // 123.xx seconds
    clock.advance(Duration.ofNanos(123_456_789_987L));
    // set-up: ensure advancing nanos reflects in our milli-based uptime
    assertEquals(Long.valueOf(123_456L), uptimeMetric.getValue());
    final UptimeMetric.ScaledView secondsBasedView = uptimeMetric.withUnitsPrecise(""up_seconds"", UptimeMetric.ScaleUnits.SECONDS);
    final UptimeMetric.ScaledView millisecondsBasedView = uptimeMetric.withUnitsPrecise(""up_millis"", UptimeMetric.ScaleUnits.MILLISECONDS);
    final UptimeMetric.ScaledView microsecondsBasedView = uptimeMetric.withUnitsPrecise(""up_micros"", UptimeMetric.ScaleUnits.MICROSECONDS);
    final UptimeMetric.ScaledView nanosecondsBasedView = uptimeMetric.withUnitsPrecise(""up_nanos"", UptimeMetric.ScaleUnits.NANOSECONDS);
    assertEquals(new BigDecimal(""123.456789987""), secondsBasedView.getValue());
    assertEquals(new BigDecimal(""123456.789987""), millisecondsBasedView.getValue());
    assertEquals(new BigDecimal(""123456789.987""), microsecondsBasedView.getValue());
    assertEquals(new BigDecimal(""123456789987""), nanosecondsBasedView.getValue());
    clock.advance(Duration.ofMillis(1_999));
    assertEquals(Long.valueOf(125_455L), uptimeMetric.getValue());
    assertEquals(new BigDecimal(""125.455789987""), secondsBasedView.getValue());
    assertEquals(new BigDecimal(""125455.789987""), millisecondsBasedView.getValue());
    assertEquals(new BigDecimal(""125455789.987""), microsecondsBasedView.getValue());
    assertEquals(new BigDecimal(""125455789987""), nanosecondsBasedView.getValue());
}", ,"// 123.xx seconds
[[SEP]]// set-up: ensure advancing nanos reflects in our milli-based uptime
",// 123.xx seconds[[SEP]]// set-up: ensure advancing nanos reflects in our milli-based uptime,68,93,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,withUnitsPrecise(),org.logstash.instrument.metrics.UptimeMetricTest,withUnitsPrecise/0,False,69,4,6,0,6,1,9,20,0,6,0,9,0,0,0,0,0,0,13,4,6,0,0,0,0,0,29,1,0,False
657,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\gauge\BooleanGaugeTest.java,org.logstash.instrument.metrics.gauge.BooleanGaugeTest,void getValue(),"@Test
public void getValue() {
    BooleanGauge gauge = new BooleanGauge(""bar"", true);
    assertThat(gauge.getValue()).isTrue();
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_BOOLEAN);
    // Null initialize
    gauge = new BooleanGauge(""bar"");
    assertThat(gauge.getValue()).isNull();
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_BOOLEAN);
}", ,"// Null initialize
",// Null initialize,32,42,[0],0,[0],0,[0],0,0,0,0,getValue(),org.logstash.instrument.metrics.gauge.BooleanGaugeTest,getValue/0,False,33,3,4,0,4,1,6,8,0,1,0,6,0,0,0,0,0,0,2,0,2,0,0,0,0,0,7,1,0,False
658,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\gauge\LazyDelegatingGaugeTest.java,org.logstash.instrument.metrics.gauge.LazyDelegatingGaugeTest,void getValue(),"@Test
public void getValue() {
    // Long
    LazyDelegatingGauge gauge = new LazyDelegatingGauge(""bar"", 99l);
    assertThat(gauge.getValue()).isEqualTo(99l);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_NUMBER);
    // Double
    gauge = new LazyDelegatingGauge(""bar"", 99.0);
    assertThat(gauge.getValue()).isEqualTo(99.0);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_NUMBER);
    // Boolean
    gauge = new LazyDelegatingGauge(""bar"", true);
    assertThat(gauge.getValue()).isEqualTo(true);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_BOOLEAN);
    // Text
    gauge = new LazyDelegatingGauge(""bar"", ""something"");
    assertThat(gauge.getValue()).isEqualTo(""something"");
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_TEXT);
    // Ruby Hash
    gauge = new LazyDelegatingGauge(""bar"", RUBY_HASH);
    assertThat(gauge.getValue().toString()).isEqualTo(RUBY_HASH_AS_STRING);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_RUBYHASH);
    // Ruby Timestamp
    gauge = new LazyDelegatingGauge(""bar"", RUBY_TIMESTAMP);
    assertThat(gauge.getValue()).isEqualTo(TIMESTAMP);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_RUBYTIMESTAMP);
    // Unknown
    gauge = new LazyDelegatingGauge(""bar"", Collections.singleton(""value""));
    assertThat(gauge.getValue()).isEqualTo(Collections.singleton(""value""));
    assertThat(gauge.getValue()).isEqualTo(gauge.get());
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_UNKNOWN);
    // Null
    gauge = new LazyDelegatingGauge(""bar"");
    assertThat(gauge.getValue()).isNull();
    assertThat(gauge.get()).isNull();
    assertThat(gauge.getType()).isNull();
    assertThat(gauge.getName()).isNotEmpty();
}", ,"// Long
[[SEP]]// Double
[[SEP]]// Boolean
[[SEP]]// Text
[[SEP]]// Ruby Hash
[[SEP]]// Ruby Timestamp
[[SEP]]// Unknown
[[SEP]]// Null
",// Long[[SEP]]// Double[[SEP]]// Boolean[[SEP]]// Text[[SEP]]// Ruby Hash[[SEP]]// Ruby Timestamp[[SEP]]// Unknown[[SEP]]// Null,50,95,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0, 0, 0]",0,0,0,0,getValue(),org.logstash.instrument.metrics.gauge.LazyDelegatingGaugeTest,getValue/0,False,51,3,6,0,6,1,10,29,0,1,0,10,0,0,0,0,0,0,12,4,8,0,0,0,0,0,25,1,0,False
659,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\gauge\LazyDelegatingGaugeTest.java,org.logstash.instrument.metrics.gauge.LazyDelegatingGaugeTest,void set(),"@Test
public void set() {
    // Long
    LazyDelegatingGauge gauge = new LazyDelegatingGauge(""bar"");
    gauge.set(99l);
    assertThat(gauge.getValue()).isEqualTo(99l);
    gauge.set(199l);
    assertThat(gauge.getValue()).isEqualTo(199l);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_NUMBER);
    // Integer
    gauge = new LazyDelegatingGauge(""bar"");
    gauge.set(99);
    assertThat(gauge.getValue()).isEqualTo(99);
    gauge.set(199);
    assertThat(gauge.getValue()).isEqualTo(199);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_NUMBER);
    // Double
    gauge = new LazyDelegatingGauge(""bar"");
    gauge.set(99.0);
    assertThat(gauge.getValue()).isEqualTo(99.0);
    gauge.set(199.01);
    assertThat(gauge.getValue()).isEqualTo(199.01);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_NUMBER);
    // Boolean
    gauge = new LazyDelegatingGauge(""bar"");
    gauge.set(true);
    assertThat(gauge.getValue()).isEqualTo(true);
    gauge.set(false);
    assertThat(gauge.getValue()).isEqualTo(false);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_BOOLEAN);
    // Text
    gauge = new LazyDelegatingGauge(""bar"");
    gauge.set(""something"");
    assertThat(gauge.getValue()).isEqualTo(""something"");
    gauge.set(""something else"");
    assertThat(gauge.getValue()).isEqualTo(""something else"");
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_TEXT);
    // Ruby Hash
    gauge = new LazyDelegatingGauge(""bar"");
    gauge.set(RUBY_HASH);
    assertThat(gauge.getValue().toString()).isEqualTo(RUBY_HASH_AS_STRING);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_RUBYHASH);
    // Ruby Timestamp
    gauge = new LazyDelegatingGauge(""bar"");
    gauge.set(RUBY_TIMESTAMP);
    assertThat(gauge.getValue()).isEqualTo(TIMESTAMP);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_RUBYTIMESTAMP);
    // Unknown
    gauge = new LazyDelegatingGauge(""bar"");
    gauge.set(Collections.singleton(""value""));
    assertThat(gauge.getValue()).isEqualTo(Collections.singleton(""value""));
    // please don't change the type of gauge after already set
    gauge.set(URI.create(""foo""));
    assertThat(gauge.getValue()).isEqualTo(URI.create(""foo""));
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_UNKNOWN);
    // Null
    gauge = new LazyDelegatingGauge(""bar"");
    gauge.set(null);
    assertThat(gauge.getValue()).isNull();
    assertThat(gauge.getType()).isNull();
    // Valid, then Null
    gauge = new LazyDelegatingGauge(""bar"");
    gauge.set(""something"");
    assertThat(gauge.getValue()).isEqualTo(""something"");
    gauge.set(null);
    assertThat(gauge.getValue()).isNull();
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_TEXT);
}", ,"// Long
[[SEP]]// Integer
[[SEP]]// Double
[[SEP]]// Boolean
[[SEP]]// Text
[[SEP]]// Ruby Hash
[[SEP]]// Ruby Timestamp
[[SEP]]// Unknown
[[SEP]]// please don't change the type of gauge after already set
[[SEP]]// Null
[[SEP]]// Valid, then Null
","// Long[[SEP]]// Integer[[SEP]]// Double[[SEP]]// Boolean[[SEP]]// Text[[SEP]]// Ruby Hash[[SEP]]// Ruby Timestamp[[SEP]]// Unknown[[SEP]]// please don't change the type of gauge after already set[[SEP]]// Null[[SEP]]// Valid, then Null",97,172,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",0,0,0,0,set(),org.logstash.instrument.metrics.gauge.LazyDelegatingGaugeTest,set/0,False,98,2,4,0,4,1,9,56,0,1,0,9,0,0,0,0,0,0,20,12,10,0,0,0,0,0,27,1,0,False
660,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\gauge\NumberGaugeTest.java,org.logstash.instrument.metrics.gauge.NumberGaugeTest,void getValue(),"@Test
public void getValue() {
    NumberGauge gauge = new NumberGauge(""bar"", 99l);
    assertThat(gauge.getValue()).isEqualTo(99l);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_NUMBER);
    // other number type
    gauge.set(98.00);
    assertThat(gauge.getValue()).isEqualTo(98.00);
    // Null
    gauge = new NumberGauge(""bar"");
    assertThat(gauge.getValue()).isNull();
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_NUMBER);
}", ,"// other number type
[[SEP]]// Null
",// other number type[[SEP]]// Null,34,47,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,getValue(),org.logstash.instrument.metrics.gauge.NumberGaugeTest,getValue/0,False,35,3,5,0,5,1,6,10,0,1,0,6,0,0,0,0,0,0,2,4,2,0,0,0,0,0,8,1,0,False
661,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\gauge\RubyHashGaugeTest.java,org.logstash.instrument.metrics.gauge.RubyHashGaugeTest,void _setup(),"@Before
public void _setup() {
    // hacky workaround using the toString method to avoid mocking the Ruby runtime
    when(rubyHash.toString()).thenReturn(RUBY_HASH_AS_STRING);
}", ,"// hacky workaround using the toString method to avoid mocking the Ruby runtime
",// hacky workaround using the toString method to avoid mocking the Ruby runtime,47,51,[0],0,[1],1,[1],1,1,1,1,_setup(),org.logstash.instrument.metrics.gauge.RubyHashGaugeTest,_setup/0,False,48,1,0,0,0,1,3,3,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,1,0,False
662,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\gauge\RubyHashGaugeTest.java,org.logstash.instrument.metrics.gauge.RubyHashGaugeTest,void getValue(),"@Test
public void getValue() {
    RubyHashGauge gauge = new RubyHashGauge(""bar"", rubyHash);
    assertThat(gauge.getValue().toString()).isEqualTo(RUBY_HASH_AS_STRING);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_RUBYHASH);
    // Null initialize
    final RubyHashGauge gauge2 = new RubyHashGauge(""bar"");
    Throwable thrown = catchThrowable(() -> {
        gauge2.getValue().toString();
    });
    assertThat(thrown).isInstanceOf(NullPointerException.class);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_RUBYHASH);
}", ,"// Null initialize
",// Null initialize,53,66,[0],0,[0],0,[0],0,0,0,0,getValue(),org.logstash.instrument.metrics.gauge.RubyHashGaugeTest,getValue/0,False,54,3,4,0,4,1,8,12,0,3,0,8,0,0,0,0,0,0,2,0,3,0,1,0,0,1,24,1,0,False
663,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\gauge\RubyTimeStampGaugeTest.java,org.logstash.instrument.metrics.gauge.RubyTimeStampGaugeTest,void getValue(),"@Test
public void getValue() {
    RubyTimeStampGauge gauge = new RubyTimeStampGauge(""bar"", RUBY_TIMESTAMP);
    assertThat(gauge.getValue()).isEqualTo(RUBY_TIMESTAMP.getTimestamp());
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_RUBYTIMESTAMP);
    // Null initialize
    gauge = new RubyTimeStampGauge(""bar"");
    assertThat(gauge.getValue()).isNull();
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_RUBYTIMESTAMP);
}", ,"// Null initialize
",// Null initialize,47,57,[0],0,[0],0,[0],0,0,0,0,getValue(),org.logstash.instrument.metrics.gauge.RubyTimeStampGaugeTest,getValue/0,False,48,3,5,0,5,1,6,8,0,1,0,6,0,0,0,0,0,0,2,0,2,0,0,0,0,0,9,1,0,False
664,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\gauge\TextGaugeTest.java,org.logstash.instrument.metrics.gauge.TextGaugeTest,void getValue(),"@Test
public void getValue() {
    TextGauge gauge = new TextGauge(""bar"", ""baz"");
    assertThat(gauge.getValue()).isEqualTo(""baz"");
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_TEXT);
    // Null initialize
    gauge = new TextGauge(""bar"");
    assertThat(gauge.getValue()).isNull();
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_TEXT);
}", ,"// Null initialize
",// Null initialize,32,42,[0],0,[0],0,[0],0,0,0,0,getValue(),org.logstash.instrument.metrics.gauge.TextGaugeTest,getValue/0,False,33,3,4,0,4,1,5,8,0,1,0,5,0,0,0,0,0,0,4,0,2,0,0,0,0,0,7,1,0,False
665,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instrument\metrics\gauge\UnknownGaugeTest.java,org.logstash.instrument.metrics.gauge.UnknownGaugeTest,void getValue(),"@Test
public void getValue() {
    UnknownGauge gauge = new UnknownGauge(""bar"", URI.create(""baz""));
    assertThat(gauge.getValue()).isEqualTo(URI.create(""baz""));
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_UNKNOWN);
    // Null initialize
    gauge = new UnknownGauge(""bar"");
    assertThat(gauge.getValue()).isNull();
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_UNKNOWN);
}", ,"// Null initialize
",// Null initialize,36,46,[0],0,[0],0,[0],0,0,0,0,getValue(),org.logstash.instrument.metrics.gauge.UnknownGaugeTest,getValue/0,False,37,3,4,0,4,1,6,8,0,1,0,6,0,0,0,0,0,0,4,0,2,0,0,0,0,0,7,1,0,False
666,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instruments\monitors\HotThreadMonitorTest.java,org.logstash.instruments.monitors.HotThreadMonitorTest,void testOptionsOrderingCpu(),"@Test
public void testOptionsOrderingCpu() {
    Map<String, String> options = new HashMap<>();
    options.put(""ordered_by"", ""cpu"");
    // Using single element array to circumvent lambda expectation of 'effective final'
    final long[] lastCpuTime = { Long.MAX_VALUE };
    HotThreadsMonitor.detect(options).forEach(tr -> {
        Long cpuTime = (Long) tr.toMap().get(""cpu.time"");
        assertThat(lastCpuTime[0] >= cpuTime, is(true));
        lastCpuTime[0] = cpuTime;
    });
}", ,"// Using single element array to circumvent lambda expectation of 'effective final'
",// Using single element array to circumvent lambda expectation of 'effective final',105,116,[0],0,[0],0,[0],0,0,0,0,testOptionsOrderingCpu(),org.logstash.instruments.monitors.HotThreadMonitorTest,testOptionsOrderingCpu/0,False,106,4,2,0,2,2,7,11,0,4,0,7,0,0,0,0,0,0,3,2,4,0,1,0,0,1,13,1,0,False
667,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instruments\monitors\HotThreadMonitorTest.java,org.logstash.instruments.monitors.HotThreadMonitorTest,void testOptionsOrderingWait(),"@Test
public void testOptionsOrderingWait() {
    Map<String, String> options = new HashMap<>();
    options.put(""ordered_by"", ""wait"");
    // Using single element array to circumvent lambda expectation of 'effectively final'
    final long[] lastWaitTime = { Long.MAX_VALUE };
    HotThreadsMonitor.detect(options).forEach(tr -> {
        Long waitTime = (Long) tr.toMap().get(""waited.time"");
        assertThat(lastWaitTime[0] >= waitTime, is(true));
        lastWaitTime[0] = waitTime;
    });
}", ,"// Using single element array to circumvent lambda expectation of 'effectively final'
",// Using single element array to circumvent lambda expectation of 'effectively final',118,129,[0],0,[0],0,[0],0,0,0,0,testOptionsOrderingWait(),org.logstash.instruments.monitors.HotThreadMonitorTest,testOptionsOrderingWait/0,False,119,4,2,0,2,2,7,11,0,4,0,7,0,0,0,0,0,0,3,2,4,0,1,0,0,1,13,1,0,False
668,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\instruments\monitors\HotThreadMonitorTest.java,org.logstash.instruments.monitors.HotThreadMonitorTest,void testOptionsOrderingBlocked(),"@Test
public void testOptionsOrderingBlocked() {
    Map<String, String> options = new HashMap<>();
    options.put(""ordered_by"", ""block"");
    // Using single element array to circumvent lambda expectation of 'effectively final'
    final long[] lastBlockedTime = { Long.MAX_VALUE };
    HotThreadsMonitor.detect(options).forEach(tr -> {
        Long blockedTime = (Long) tr.toMap().get(""blocked.time"");
        assertThat(lastBlockedTime[0] >= blockedTime, is(true));
        lastBlockedTime[0] = blockedTime;
    });
}", ,"// Using single element array to circumvent lambda expectation of 'effectively final'
",// Using single element array to circumvent lambda expectation of 'effectively final',131,142,[0],0,[0],0,[0],0,0,0,0,testOptionsOrderingBlocked(),org.logstash.instruments.monitors.HotThreadMonitorTest,testOptionsOrderingBlocked/0,False,132,4,2,0,2,2,7,11,0,4,0,7,0,0,0,0,0,0,3,2,4,0,1,0,0,1,13,1,0,False
669,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\log\DefaultDeprecationLoggerTest.java,org.logstash.log.DefaultDeprecationLoggerTest,void testDeprecationLoggerWriteOut_root(),"@Test
public void testDeprecationLoggerWriteOut_root() throws IOException {
    final DefaultDeprecationLogger deprecationLogger = new DefaultDeprecationLogger(LogManager.getLogger(""test""));
    // Exercise
    deprecationLogger.deprecated(""Simple deprecation message"");
    String logs = LogTestUtils.loadLogFileContent(""logstash-deprecation.log"");
    assertTrue(""Deprecation logs MUST contains the out line"", logs.matches("".*\\[deprecation\\.test.*\\].*Simple deprecation message""));
}", ,"// Exercise
",// Exercise,68,77,[0],0,[0],0,[0],0,0,0,0,testDeprecationLoggerWriteOut_root(),org.logstash.log.DefaultDeprecationLoggerTest,testDeprecationLoggerWriteOut_root/0,False,69,4,2,0,2,1,5,6,0,2,0,5,0,0,0,0,0,0,5,0,2,0,0,0,0,0,22,1,0,False
670,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\log\DefaultDeprecationLoggerTest.java,org.logstash.log.DefaultDeprecationLoggerTest,void testDeprecationLoggerWriteOut_nested(),"@Test
public void testDeprecationLoggerWriteOut_nested() throws IOException {
    final DefaultDeprecationLogger deprecationLogger = new DefaultDeprecationLogger(LogManager.getLogger(""org.logstash.my_nested_logger""));
    // Exercise
    deprecationLogger.deprecated(""Simple deprecation message"");
    String logs = LogTestUtils.loadLogFileContent(""logstash-deprecation.log"");
    assertTrue(""Deprecation logs MUST contains the out line"", logs.matches("".*\\[org\\.logstash\\.deprecation\\.my_nested_logger.*\\].*Simple deprecation message""));
}", ,"// Exercise
",// Exercise,79,88,[0],0,[0],0,[0],0,0,0,0,testDeprecationLoggerWriteOut_nested(),org.logstash.log.DefaultDeprecationLoggerTest,testDeprecationLoggerWriteOut_nested/0,False,80,4,2,0,2,1,5,6,0,2,0,5,0,0,0,0,0,0,5,0,2,0,0,0,0,0,22,1,0,False
671,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\log\LogTestUtils.java,org.logstash.log.LogTestUtils,void reloadLogConfiguration(),"static void reloadLogConfiguration() {
    LoggerContext context = LoggerContext.getContext(false);
    // this forces the Log4j config to be discarded
    context.stop(1, TimeUnit.SECONDS);
}", ,"// this forces the Log4j config to be discarded
",// this forces the Log4j config to be discarded,47,50,[0],0,[0],0,[0],0,0,0,0,reloadLogConfiguration(),org.logstash.log.LogTestUtils,reloadLogConfiguration/0,False,47,1,6,4,2,1,2,4,0,1,0,2,0,0,0,0,0,0,0,1,1,0,0,0,0,0,7,8,0,False
672,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\log\PluginDeprecationLoggerTest.java,org.logstash.log.PluginDeprecationLoggerTest,void testJavaPluginUsesDeprecationLogger(),"@Test
public void testJavaPluginUsesDeprecationLogger() throws IOException {
    Map<String, Object> config = new HashMap<>();
    TestingDeprecationPlugin sut = new TestingDeprecationPlugin(new ConfigurationImpl(config), new ContextImpl(null, null));
    // Exercise
    Event evt = new Event(Collections.singletonMap(""message"", ""Spock move me back""));
    sut.encode(evt, null);
    // Verify
    String logs = LogTestUtils.loadLogFileContent(""logstash-deprecation.log"");
    assertTrue(""Deprecation logs MUST contains the out line"", logs.matches("".*Deprecated feature teleportation""));
}", ,"// Exercise
[[SEP]]// Verify
",// Exercise[[SEP]]// Verify,68,80,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testJavaPluginUsesDeprecationLogger(),org.logstash.log.PluginDeprecationLoggerTest,testJavaPluginUsesDeprecationLogger/0,False,69,6,4,0,4,1,5,8,0,4,0,5,0,0,0,0,0,0,5,0,4,0,0,0,0,0,31,1,0,False
673,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\ConfigurationImplTest.java,org.logstash.plugins.ConfigurationImplTest,void testPasswordDefaultValue(),"@Test
public void testPasswordDefaultValue() {
    // default values for passwords are a bad idea, but they should still work
    String myPassword = ""mysecret"";
    PluginConfigSpec<Password> passwordConfig = PluginConfigSpec.passwordSetting(""passwordTest"", myPassword, false, false);
    Configuration config = new ConfigurationImpl(Collections.emptyMap());
    Password p = config.get(passwordConfig);
    Assert.assertEquals(Password.class, p.getClass());
    Assert.assertEquals(myPassword, p.getPassword());
    Assert.assertEquals(""<password>"", p.toString());
}", ,"// default values for passwords are a bad idea, but they should still work
","// default values for passwords are a bad idea, but they should still work",199,209,[0],0,[0],0,[0],0,0,0,0,testPasswordDefaultValue(),org.logstash.plugins.ConfigurationImplTest,testPasswordDefaultValue/0,False,200,5,1,0,1,1,7,9,0,4,0,7,0,0,0,0,0,0,3,0,4,0,0,0,0,0,11,1,0,False
674,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\PluginUtilValidateConfigTest.java,org.logstash.plugins.PluginUtilValidateConfigTest,Collection testParameters(),"@SuppressWarnings(""rawtypes"")
@Parameterized.Parameters(name = ""{index}: {0}"")
public static Collection testParameters() {
    List<ValidateConfigTestCase> testParameters = new ArrayList<>();
    // optional config items, none provided
    List<PluginConfigSpec<?>> configSpec01 = Arrays.asList(PluginConfigSpec.stringSetting(""foo1""), PluginConfigSpec.stringSetting(""foo2""));
    TestingPlugin p01 = new TestingPlugin(configSpec01);
    Configuration config01 = new ConfigurationImpl(Collections.emptyMap());
    testParameters.add(new ValidateConfigTestCase(""optional config items, none provided"", p01, config01, Collections.emptyList(), Collections.emptyList()));
    // optional config items, some provided
    List<PluginConfigSpec<?>> configSpec02 = Arrays.asList(PluginConfigSpec.stringSetting(""foo1""), PluginConfigSpec.stringSetting(""foo2""));
    TestingPlugin p02 = new TestingPlugin(configSpec02);
    Configuration config02 = new ConfigurationImpl(Collections.singletonMap(""foo1"", ""bar""));
    testParameters.add(new ValidateConfigTestCase(""optional config items, some provided"", p02, config02, Collections.emptyList(), Collections.emptyList()));
    // optional config items, all provided
    List<PluginConfigSpec<?>> configSpec03 = Arrays.asList(PluginConfigSpec.stringSetting(""foo1""), PluginConfigSpec.stringSetting(""foo2""));
    TestingPlugin p03 = new TestingPlugin(configSpec03);
    Map<String, Object> configMap03 = new HashMap<>();
    configMap03.put(""foo1"", ""bar"");
    configMap03.put(""foo2"", ""bar"");
    Configuration config03 = new ConfigurationImpl(configMap03);
    testParameters.add(new ValidateConfigTestCase(""optional config items, all provided"", p03, config03, Collections.emptyList(), Collections.emptyList()));
    // optional config items, too many provided
    List<PluginConfigSpec<?>> configSpec04 = Arrays.asList(PluginConfigSpec.stringSetting(""foo1""), PluginConfigSpec.stringSetting(""foo2""));
    TestingPlugin p04 = new TestingPlugin(configSpec04);
    Map<String, Object> configMap04 = new HashMap<>();
    configMap04.put(""foo1"", ""bar"");
    configMap04.put(""foo2"", ""bar"");
    configMap04.put(""foo3"", ""bar"");
    Configuration config04 = new ConfigurationImpl(configMap04);
    testParameters.add(new ValidateConfigTestCase(""optional config items, too many provided"", p04, config04, Collections.singletonList(""foo3""), Collections.emptyList()));
    // required config items, all provided
    List<PluginConfigSpec<?>> configSpec05 = Arrays.asList(PluginConfigSpec.requiredStringSetting(""foo""));
    TestingPlugin p05 = new TestingPlugin(configSpec05);
    Configuration config05 = new ConfigurationImpl(Collections.singletonMap(""foo"", ""bar""));
    testParameters.add(new ValidateConfigTestCase(""required config items, all provided"", p05, config05, Collections.emptyList(), Collections.emptyList()));
    // required config items, some provided
    List<PluginConfigSpec<?>> configSpec06 = Arrays.asList(PluginConfigSpec.requiredStringSetting(""foo1""), PluginConfigSpec.requiredStringSetting(""foo2""));
    TestingPlugin p06 = new TestingPlugin(configSpec06);
    Configuration config06 = new ConfigurationImpl(Collections.singletonMap(""foo1"", ""bar""));
    testParameters.add(new ValidateConfigTestCase(""required config items, some provided"", p06, config06, Collections.emptyList(), Collections.singletonList(""foo2"")));
    // required config items, too many provided
    List<PluginConfigSpec<?>> configSpec07 = Arrays.asList(PluginConfigSpec.requiredStringSetting(""foo1""), PluginConfigSpec.requiredStringSetting(""foo2""));
    TestingPlugin p07 = new TestingPlugin(configSpec07);
    Map<String, Object> configMap07 = new HashMap<>();
    configMap07.put(""foo1"", ""bar"");
    configMap07.put(""foo3"", ""bar"");
    Configuration config07 = new ConfigurationImpl(configMap07);
    testParameters.add(new ValidateConfigTestCase(""required config items, too many provided"", p07, config07, Collections.singletonList(""foo3""), Collections.singletonList(""foo2"")));
    // optional+required config items, some provided
    List<PluginConfigSpec<?>> configSpec08 = Arrays.asList(PluginConfigSpec.requiredStringSetting(""foo1""), PluginConfigSpec.requiredStringSetting(""foo2""), PluginConfigSpec.stringSetting(""foo3""), PluginConfigSpec.stringSetting(""foo4""));
    TestingPlugin p08 = new TestingPlugin(configSpec08);
    Map<String, Object> configMap08 = new HashMap<>();
    configMap08.put(""foo1"", ""bar"");
    configMap08.put(""foo2"", ""bar"");
    configMap08.put(""foo3"", ""bar"");
    Configuration config08 = new ConfigurationImpl(configMap08);
    testParameters.add(new ValidateConfigTestCase(""optional+required config items, some provided"", p08, config08, Collections.emptyList(), Collections.emptyList()));
    // optional+required config items, some missing
    List<PluginConfigSpec<?>> configSpec09 = Arrays.asList(PluginConfigSpec.requiredStringSetting(""foo1""), PluginConfigSpec.requiredStringSetting(""foo2""), PluginConfigSpec.stringSetting(""foo3""), PluginConfigSpec.stringSetting(""foo4""));
    TestingPlugin p09 = new TestingPlugin(configSpec09);
    Map<String, Object> configMap09 = new HashMap<>();
    configMap09.put(""foo1"", ""bar"");
    configMap09.put(""foo3"", ""bar"");
    Configuration config09 = new ConfigurationImpl(configMap09);
    testParameters.add(new ValidateConfigTestCase(""optional+required config items, some missing"", p09, config09, Collections.emptyList(), Collections.singletonList(""foo2"")));
    // optional+required config items, some missing, some invalid
    List<PluginConfigSpec<?>> configSpec10 = Arrays.asList(PluginConfigSpec.requiredStringSetting(""foo1""), PluginConfigSpec.requiredStringSetting(""foo2""), PluginConfigSpec.stringSetting(""foo3""), PluginConfigSpec.stringSetting(""foo4""));
    TestingPlugin p10 = new TestingPlugin(configSpec10);
    Map<String, Object> configMap10 = new HashMap<>();
    configMap10.put(""foo1"", ""bar"");
    configMap10.put(""foo3"", ""bar"");
    configMap10.put(""foo5"", ""bar"");
    Configuration config10 = new ConfigurationImpl(configMap10);
    testParameters.add(new ValidateConfigTestCase(""optional+required config items, some missing, some invalid"", p10, config10, Collections.singletonList(""foo5""), Collections.singletonList(""foo2"")));
    return testParameters;
}", ,"// optional config items, none provided
[[SEP]]// optional config items, some provided
[[SEP]]// optional config items, all provided
[[SEP]]// optional config items, too many provided
[[SEP]]// required config items, all provided
[[SEP]]// required config items, some provided
[[SEP]]// required config items, too many provided
[[SEP]]// optional+required config items, some provided
[[SEP]]// optional+required config items, some missing
[[SEP]]// optional+required config items, some missing, some invalid
","// optional config items, none provided[[SEP]]// optional config items, some provided[[SEP]]// optional config items, all provided[[SEP]]// optional config items, too many provided[[SEP]]// required config items, all provided[[SEP]]// required config items, some provided[[SEP]]// required config items, too many provided[[SEP]]// optional+required config items, some provided[[SEP]]// optional+required config items, some missing[[SEP]]// optional+required config items, some missing, some invalid",48,170,[0],0,"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",0,0,0,0,testParameters(),org.logstash.plugins.PluginUtilValidateConfigTest,testParameters/0,False,50,6,2,0,2,1,9,65,1,37,0,9,0,0,0,0,0,0,80,0,37,0,0,0,0,0,56,9,0,False
675,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\codecs\LineTest.java,org.logstash.plugins.codecs.LineTest,void testSuccessiveDecodesWithTrailingDelimiter(),"@Test
public void testSuccessiveDecodesWithTrailingDelimiter() {
    // setup inputs
    String delimiter = System.lineSeparator();
    String[] inputs = { ""foo"", ""bar"", ""baz"" };
    String input = String.join(delimiter, inputs) + delimiter;
    byte[] inputBytes = input.getBytes();
    TestEventConsumer eventConsumer = new TestEventConsumer();
    TestEventConsumer flushedEvents = new TestEventConsumer();
    Line line = getLineCodec(null, null);
    // first call to decode
    ByteBuffer buffer = ByteBuffer.allocate(inputBytes.length * 3);
    buffer.put(inputBytes);
    buffer.flip();
    line.decode(buffer, eventConsumer);
    assertEquals(inputs.length, eventConsumer.events.size());
    compareMessages(inputs, eventConsumer.events, flushedEvents.events);
    // second call to encode
    eventConsumer.events.clear();
    buffer.compact();
    buffer.put(inputBytes);
    buffer.flip();
    line.decode(buffer, eventConsumer);
    assertEquals(inputs.length, eventConsumer.events.size());
    compareMessages(inputs, eventConsumer.events, flushedEvents.events);
    buffer.compact();
    buffer.flip();
    line.flush(buffer, flushedEvents);
    assertEquals(0, flushedEvents.events.size());
}", ,"// setup inputs
[[SEP]]// first call to decode
[[SEP]]// second call to encode
",// setup inputs[[SEP]]// first call to decode[[SEP]]// second call to encode,77,109,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,testSuccessiveDecodesWithTrailingDelimiter(),org.logstash.plugins.codecs.LineTest,testSuccessiveDecodesWithTrailingDelimiter/0,False,78,5,5,0,5,1,14,26,0,8,0,14,2,1,0,0,0,0,3,2,8,2,0,0,0,0,27,1,0,False
676,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\codecs\LineTest.java,org.logstash.plugins.codecs.LineTest,void testDecodeWithCharset(),"@Test
public void testDecodeWithCharset() {
    TestEventConsumer flushConsumer = new TestEventConsumer();
    // decode with cp-1252
    Line cp1252decoder = new Line(new ConfigurationImpl(Collections.singletonMap(""charset"", ""cp1252"")), new TestContext());
    byte[] rightSingleQuoteInCp1252 = { (byte) 0x92 };
    ByteBuffer b1 = ByteBuffer.wrap(rightSingleQuoteInCp1252);
    cp1252decoder.decode(b1, flushConsumer);
    assertEquals(0, flushConsumer.events.size());
    cp1252decoder.flush(b1, flushConsumer);
    assertEquals(1, flushConsumer.events.size());
    String fromCp1252 = (String) flushConsumer.events.get(0).get(Line.MESSAGE_FIELD);
    // decode with UTF-8
    flushConsumer.events.clear();
    Line utf8decoder = new Line(new ConfigurationImpl(Collections.emptyMap()), new TestContext());
    byte[] rightSingleQuoteInUtf8 = { (byte) 0xE2, (byte) 0x80, (byte) 0x99 };
    ByteBuffer b2 = ByteBuffer.wrap(rightSingleQuoteInUtf8);
    utf8decoder.decode(b2, flushConsumer);
    assertEquals(0, flushConsumer.events.size());
    utf8decoder.flush(b2, flushConsumer);
    assertEquals(1, flushConsumer.events.size());
    String fromUtf8 = (String) flushConsumer.events.get(0).get(Line.MESSAGE_FIELD);
    assertEquals(fromCp1252, fromUtf8);
}", ,"// decode with cp-1252
[[SEP]]// decode with UTF-8
",// decode with cp-1252[[SEP]]// decode with UTF-8,234,259,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testDecodeWithCharset(),org.logstash.plugins.codecs.LineTest,testDecodeWithCharset/0,False,235,6,5,0,5,1,10,21,0,9,0,10,0,0,0,0,0,0,2,10,9,0,0,0,0,0,29,1,0,False
677,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\codecs\LineTest.java,org.logstash.plugins.codecs.LineTest,void testEncode(),"@Test
public void testEncode() throws IOException {
    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
    Line line = new Line(new ConfigurationImpl(Collections.emptyMap()), null);
    Event e = new Event();
    e.setField(""myfield1"", ""myvalue1"");
    e.setField(""myfield2"", 42L);
    line.encode(e, outputStream);
    e.setField(""myfield1"", ""myvalue2"");
    e.setField(""myfield2"", 43L);
    line.encode(e, outputStream);
    String delimiter = Line.DEFAULT_DELIMITER;
    String resultingString = outputStream.toString();
    // first delimiter should occur at the halfway point of the string
    assertEquals(resultingString.indexOf(delimiter), (resultingString.length() / 2) - delimiter.length());
    // second delimiter should occur at end of string
    assertEquals(resultingString.lastIndexOf(delimiter), resultingString.length() - delimiter.length());
}", ,"// first delimiter should occur at the halfway point of the string
[[SEP]]// second delimiter should occur at end of string
",// first delimiter should occur at the halfway point of the string[[SEP]]// second delimiter should occur at end of string,261,279,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testEncode(),org.logstash.plugins.codecs.LineTest,testEncode/0,False,262,4,4,0,4,1,8,15,0,5,0,8,0,0,0,0,0,1,6,3,5,3,0,0,0,0,21,1,0,False
678,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\codecs\LineTest.java,org.logstash.plugins.codecs.LineTest,void testEncodeWithCharset(),"@Test
public void testEncodeWithCharset() throws IOException {
    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
    byte[] rightSingleQuoteInUtf8 = { (byte) 0xE2, (byte) 0x80, (byte) 0x99 };
    String rightSingleQuote = new String(rightSingleQuoteInUtf8, Charset.forName(""UTF-8""));
    // encode with cp-1252
    Map<String, Object> config = new HashMap<>();
    config.put(""charset"", ""cp1252"");
    config.put(""format"", ""%{message}"");
    config.put(""delimiter"", """");
    Event e1 = new Event(Collections.singletonMap(""message"", rightSingleQuote));
    Line cp1252encoder = new Line(new ConfigurationImpl(config), new TestContext());
    byte[] rightSingleQuoteInCp1252 = { (byte) 0x92 };
    cp1252encoder.encode(e1, outputStream);
    byte[] resultBytes = outputStream.toByteArray();
    Assert.assertArrayEquals(rightSingleQuoteInCp1252, resultBytes);
}", ,"// encode with cp-1252
",// encode with cp-1252,296,314,[0],0,[0],0,[0],0,0,0,0,testEncodeWithCharset(),org.logstash.plugins.codecs.LineTest,testEncodeWithCharset/0,False,297,6,4,0,4,1,6,15,0,8,0,6,0,0,0,0,0,0,8,4,8,0,0,0,0,0,32,1,0,False
679,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\codecs\LineTest.java,org.logstash.plugins.codecs.LineTest,void testClone(),"@Test
public void testClone() throws IOException {
    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
    String delimiter = ""x"";
    String charset = ""cp1252"";
    byte[] rightSingleQuoteInUtf8 = { (byte) 0xE2, (byte) 0x80, (byte) 0x99 };
    String rightSingleQuote = new String(rightSingleQuoteInUtf8, Charset.forName(""UTF-8""));
    // encode with cp-1252
    Map<String, Object> config = new HashMap<>();
    config.put(""charset"", charset);
    config.put(""format"", ""%{message}"");
    config.put(""delimiter"", delimiter);
    Event e1 = new Event(Collections.singletonMap(""message"", rightSingleQuote));
    Line codec = new Line(new ConfigurationImpl(config), new TestContext());
    // clone codec
    Codec clone = codec.cloneCodec();
    Assert.assertEquals(codec.getClass(), clone.getClass());
    Line line2 = (Line) clone;
    // verify charset and delimiter
    byte[] rightSingleQuoteAndXInCp1252 = { (byte) 0x92, (byte) 0x78 };
    line2.encode(e1, outputStream);
    Assert.assertArrayEquals(rightSingleQuoteAndXInCp1252, outputStream.toByteArray());
}", ,"// encode with cp-1252
[[SEP]]// clone codec
[[SEP]]// verify charset and delimiter
",// encode with cp-1252[[SEP]]// clone codec[[SEP]]// verify charset and delimiter,333,358,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,testClone(),org.logstash.plugins.codecs.LineTest,testClone/0,False,334,7,4,0,4,1,10,19,0,11,0,10,0,0,0,0,0,0,8,5,11,0,0,0,0,0,35,1,0,False
680,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\codecs\PlainTest.java,org.logstash.plugins.codecs.PlainTest,void testDecodeWithCharset(),"@Test
public void testDecodeWithCharset() {
    TestEventConsumer eventConsumer = new TestEventConsumer();
    // decode with cp-1252
    Plain cp1252decoder = new Plain(new ConfigurationImpl(Collections.singletonMap(""charset"", ""cp1252"")), new TestContext());
    byte[] rightSingleQuoteInCp1252 = { (byte) 0x92 };
    ByteBuffer b1 = ByteBuffer.wrap(rightSingleQuoteInCp1252);
    cp1252decoder.decode(b1, eventConsumer);
    assertEquals(1, eventConsumer.events.size());
    cp1252decoder.flush(b1, eventConsumer);
    assertEquals(1, eventConsumer.events.size());
    String fromCp1252 = (String) eventConsumer.events.get(0).get(Plain.MESSAGE_FIELD);
    // decode with UTF-8
    eventConsumer.events.clear();
    Plain utf8decoder = new Plain(new ConfigurationImpl(Collections.emptyMap()), new TestContext());
    byte[] rightSingleQuoteInUtf8 = { (byte) 0xE2, (byte) 0x80, (byte) 0x99 };
    ByteBuffer b2 = ByteBuffer.wrap(rightSingleQuoteInUtf8);
    utf8decoder.decode(b2, eventConsumer);
    assertEquals(1, eventConsumer.events.size());
    utf8decoder.flush(b2, eventConsumer);
    assertEquals(1, eventConsumer.events.size());
    String fromUtf8 = (String) eventConsumer.events.get(0).get(Plain.MESSAGE_FIELD);
    assertEquals(fromCp1252, fromUtf8);
}", ,"// decode with cp-1252
[[SEP]]// decode with UTF-8
",// decode with cp-1252[[SEP]]// decode with UTF-8,62,87,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testDecodeWithCharset(),org.logstash.plugins.codecs.PlainTest,testDecodeWithCharset/0,False,63,6,2,0,2,1,10,21,0,9,0,10,0,0,0,0,0,0,2,10,9,0,0,0,0,0,29,1,0,False
681,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\codecs\PlainTest.java,org.logstash.plugins.codecs.PlainTest,"void testDecode(String, String, Integer, String[])","private void testDecode(String charset, String inputString, Integer expectedPreflushEvents, String[] expectedMessages) {
    Plain plain = getPlainCodec(charset);
    byte[] inputBytes = null;
    try {
        inputBytes = inputString.getBytes(""UTF-8"");
    } catch (UnsupportedEncodingException ex) {
        Assert.fail();
    }
    TestEventConsumer eventConsumer = new TestEventConsumer();
    ByteBuffer inputBuffer = ByteBuffer.wrap(inputBytes, 0, inputBytes.length);
    plain.decode(inputBuffer, eventConsumer);
    if (expectedPreflushEvents != null) {
        assertEquals(expectedPreflushEvents.intValue(), eventConsumer.events.size());
    }
    inputBuffer.compact();
    inputBuffer.flip();
    // flushing the plain codec should never produce events
    TestEventConsumer flushConsumer = new TestEventConsumer();
    plain.flush(inputBuffer, flushConsumer);
    assertEquals(0, flushConsumer.events.size());
    compareMessages(expectedMessages, eventConsumer.events, flushConsumer.events);
}", ,"// flushing the plain codec should never produce events
",// flushing the plain codec should never produce events,89,114,[0],0,[0],0,[0],0,0,0,0,"testDecode(String, String, Integer, String[])",org.logstash.plugins.codecs.PlainTest,"testDecode/4[java.lang.String,java.lang.String,java.lang.Integer,java.lang.String[]]",False,89,5,6,3,3,3,12,22,0,5,4,12,1,1,0,1,1,0,1,2,6,0,1,0,0,0,28,2,0,False
682,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\codecs\PlainTest.java,org.logstash.plugins.codecs.PlainTest,void testEncodeWithCharset(),"@Test
public void testEncodeWithCharset() throws IOException {
    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
    byte[] rightSingleQuoteInUtf8 = { (byte) 0xE2, (byte) 0x80, (byte) 0x99 };
    String rightSingleQuote = new String(rightSingleQuoteInUtf8, Charset.forName(""UTF-8""));
    // encode with cp-1252
    Map<String, Object> config = new HashMap<>();
    config.put(""charset"", ""cp1252"");
    config.put(""format"", ""%{message}"");
    config.put(""delimiter"", """");
    Event e1 = new Event(Collections.singletonMap(""message"", rightSingleQuote));
    Plain cp1252encoder = new Plain(new ConfigurationImpl(config), new TestContext());
    byte[] rightSingleQuoteInCp1252 = { (byte) 0x92 };
    cp1252encoder.encode(e1, outputStream);
    byte[] resultBytes = outputStream.toByteArray();
    Assert.assertArrayEquals(rightSingleQuoteInCp1252, resultBytes);
}", ,"// encode with cp-1252
",// encode with cp-1252,136,154,[0],0,[0],0,[0],0,0,0,0,testEncodeWithCharset(),org.logstash.plugins.codecs.PlainTest,testEncodeWithCharset/0,False,137,6,2,0,2,1,6,15,0,8,0,6,0,0,0,0,0,0,8,4,8,0,0,0,0,0,32,1,0,False
683,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\codecs\PlainTest.java,org.logstash.plugins.codecs.PlainTest,void testClone(),"@Test
public void testClone() throws IOException {
    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
    String charset = ""cp1252"";
    byte[] rightSingleQuoteInUtf8 = { (byte) 0xE2, (byte) 0x80, (byte) 0x99 };
    String rightSingleQuote = new String(rightSingleQuoteInUtf8, Charset.forName(""UTF-8""));
    // encode with cp-1252
    Map<String, Object> config = new HashMap<>();
    config.put(""charset"", charset);
    config.put(""format"", ""%{message}"");
    Event e1 = new Event(Collections.singletonMap(""message"", rightSingleQuote));
    Plain codec = new Plain(new ConfigurationImpl(config), new TestContext());
    // clone codec
    Codec clone = codec.cloneCodec();
    Assert.assertEquals(codec.getClass(), clone.getClass());
    Plain plain2 = (Plain) clone;
    // verify charset and delimiter
    byte[] rightSingleQuoteInCp1252 = { (byte) 0x92 };
    plain2.encode(e1, outputStream);
    Assert.assertArrayEquals(rightSingleQuoteInCp1252, outputStream.toByteArray());
}", ,"// encode with cp-1252
[[SEP]]// clone codec
[[SEP]]// verify charset and delimiter
",// encode with cp-1252[[SEP]]// clone codec[[SEP]]// verify charset and delimiter,173,196,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,testClone(),org.logstash.plugins.codecs.PlainTest,testClone/0,False,174,7,2,0,2,1,9,17,0,10,0,9,0,0,0,0,0,0,6,4,10,0,0,0,0,0,32,1,0,False
684,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\factory\PluginFactoryExtTest.java,org.logstash.plugins.factory.PluginFactoryExtTest,void testPluginIdResolvedWithEnvironmentVariables(),"@Test
public void testPluginIdResolvedWithEnvironmentVariables() throws InvalidIRException {
    PluginFactoryExt.PluginResolver mockPluginResolver = wrapWithSearchable(MockInputPlugin.class);
    SourceWithMetadata sourceWithMetadata = new SourceWithMetadata(""proto"", ""path"", 1, 8, ""input {mockinput{ id => \""${CUSTOM}\""}} output{mockoutput{}}"");
    final PipelineIR pipelineIR = compilePipeline(sourceWithMetadata);
    PluginMetricsFactoryExt metricsFactory = createMetricsFactory();
    ExecutionContextFactoryExt execContextFactory = createExecutionContextFactory();
    Map<String, String> envVars = new HashMap<>();
    envVars.put(""CUSTOM"", ""test"");
    PluginFactoryExt sut = new PluginFactoryExt(RubyUtil.RUBY, RubyUtil.PLUGIN_FACTORY_CLASS, mockPluginResolver);
    sut.init(pipelineIR, metricsFactory, execContextFactory, RubyUtil.FILTER_DELEGATOR_CLASS, envVars::get);
    RubyString pluginName = RubyUtil.RUBY.newString(""mockinput"");
    // Exercise
    IRubyObject pluginInstance = sut.buildInput(pluginName, RubyHash.newHash(RubyUtil.RUBY), sourceWithMetadata);
    // Verify
    IRubyObject id = pluginInstance.callMethod(RUBY.getCurrentContext(), ""id"");
    assertEquals(""Resolved config setting MUST be evaluated with substitution"", envVars.get(""CUSTOM""), id.toString());
}", ,"// Exercise
[[SEP]]// Verify
",// Exercise[[SEP]]// Verify,85,108,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,testPluginIdResolvedWithEnvironmentVariables(),org.logstash.plugins.factory.PluginFactoryExtTest,testPluginIdResolvedWithEnvironmentVariables/0,False,86,10,5,0,5,1,14,15,0,10,0,14,4,1,0,0,0,0,9,2,10,0,0,0,0,0,55,1,0,False
685,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\outputs\StdoutTest.java,org.logstash.plugins.outputs.StdoutTest,void testUnderlyingStreamIsNotClosed(),"/**
 * Verifies that the stdout output is reloadable because it does not close the underlying
 * output stream which, outside of test cases, is always {@link java.lang.System#out}.
 */
@Test
public void testUnderlyingStreamIsNotClosed() {
    OutputStream dummyOutputStream = new ByteArrayOutputStream(0) {

        @Override
        public void close() throws IOException {
            streamWasClosed = true;
            super.close();
        }
    };
    Stdout stdout = new Stdout(ID, new ConfigurationImpl(Collections.emptyMap(), new TestPluginFactory()), new TestContext(), dummyOutputStream);
    stdout.output(getTestEvents());
    stdout.stop();
    assertFalse(streamWasClosed);
}","/**
 * Verifies that the stdout output is reloadable because it does not close the underlying
 * output stream which, outside of test cases, is always {@link java.lang.System#out}.
 */
", ,"/** * Verifies that the stdout output is reloadable because it does not close the underlying * output stream which, outside of test cases, is always {@link java.lang.System#out}. */",50,65,[0],0,[0],0,[0],0,0,0,0,testUnderlyingStreamIsNotClosed(),org.logstash.plugins.outputs.StdoutTest,testUnderlyingStreamIsNotClosed/0,False,51,6,7,0,7,1,5,13,0,2,0,5,1,1,0,0,0,0,0,1,2,0,0,1,0,0,43,1,0,True
686,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\pipeline\PipelineBusTest.java,org.logstash.plugins.pipeline.PipelineBusTest,void subscribeUnsubscribe(),"@Test
public void subscribeUnsubscribe() throws InterruptedException {
    assertThat(bus.listen(input, address)).isTrue();
    assertThat(bus.addressStates.get(address).getInput()).isSameAs(input);
    bus.unlisten(input, address);
    // Key should have been pruned
    assertThat(bus.addressStates.containsKey(address)).isFalse();
}", ,"// Key should have been pruned
",// Key should have been pruned,54,63,[0],0,[0],0,[0],0,0,0,0,subscribeUnsubscribe(),org.logstash.plugins.pipeline.PipelineBusTest,subscribeUnsubscribe/0,False,55,3,3,0,3,1,9,6,0,0,0,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,1,0,False
687,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\pipeline\PipelineBusTest.java,org.logstash.plugins.pipeline.PipelineBusTest,void senderRegisterUnregister(),"@Test
public void senderRegisterUnregister() {
    bus.registerSender(output, addresses);
    assertThat(bus.addressStates.get(address).hasOutput(output)).isTrue();
    bus.unregisterSender(output, addresses);
    // We should have pruned this address
    assertThat(bus.addressStates.containsKey(address)).isFalse();
}", ,"// We should have pruned this address
",// We should have pruned this address,65,75,[0],0,[0],0,[0],0,0,0,0,senderRegisterUnregister(),org.logstash.plugins.pipeline.PipelineBusTest,senderRegisterUnregister/0,False,66,3,3,0,3,1,8,6,0,0,0,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,1,0,False
688,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\pipeline\PipelineBusTest.java,org.logstash.plugins.pipeline.PipelineBusTest,void listenUnlistenUpdatesOutputReceivers(),"@Test
public void listenUnlistenUpdatesOutputReceivers() throws InterruptedException {
    bus.registerSender(output, addresses);
    bus.listen(input, address);
    final ConcurrentHashMap<String, AddressState> outputAddressesToInputs = bus.outputsToAddressStates.get(output);
    outputAddressesToInputs.get(address).getInput().internalReceive(Stream.of(rubyEvent()));
    assertThat(input.eventCount.longValue()).isEqualTo(1L);
    bus.unlisten(input, address);
    TestPipelineInput newInput = new TestPipelineInput();
    bus.listen(newInput, address);
    outputAddressesToInputs.get(address).getInput().internalReceive(Stream.of(rubyEvent()));
    // The new event went to the new input, not the old one
    assertThat(newInput.eventCount.longValue()).isEqualTo(1L);
    assertThat(input.eventCount.longValue()).isEqualTo(1L);
}", ,"// The new event went to the new input, not the old one
","// The new event went to the new input, not the old one",116,135,[0],0,[0],0,[0],0,0,0,0,listenUnlistenUpdatesOutputReceivers(),org.logstash.plugins.pipeline.PipelineBusTest,listenUnlistenUpdatesOutputReceivers/0,False,117,6,7,0,7,1,12,13,0,2,0,12,1,1,0,0,0,0,0,3,2,0,0,0,0,0,22,1,0,False
689,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\pipeline\PipelineBusTest.java,org.logstash.plugins.pipeline.PipelineBusTest,void missingInputEventuallySucceeds(),"@Test
public void missingInputEventuallySucceeds() throws InterruptedException {
    bus.registerSender(output, addresses);
    // bus.sendEvent should block at this point since there is no attached listener
    // For this test we want to make sure that the background thread has had time to actually block
    // since if we start the input too soon we aren't testing anything
    // The below code attempts to make sure this happens, though it's hard to be deterministic
    // without making sendEvent take weird arguments the non-test code really doesn't need
    CountDownLatch sendLatch = new CountDownLatch(1);
    Thread sendThread = new Thread(() -> {
        sendLatch.countDown();
        bus.sendEvents(output, Collections.singleton(rubyEvent()), true);
    });
    sendThread.start();
    // Try to ensure that the send actually happened. The latch gets us close,
    // the sleep takes us the full way (hopefully)
    sendLatch.await();
    Thread.sleep(1000);
    bus.listen(input, address);
    // This would block if there's an error in the code
    sendThread.join();
    assertThat(input.eventCount.longValue()).isEqualTo(1L);
}", ,"// bus.sendEvent should block at this point since there is no attached listener
[[SEP]]// For this test we want to make sure that the background thread has had time to actually block
[[SEP]]// since if we start the input too soon we aren't testing anything
[[SEP]]// The below code attempts to make sure this happens, though it's hard to be deterministic
[[SEP]]// Try to ensure that the send actually happened. The latch gets us close,
[[SEP]]// without making sendEvent take weird arguments the non-test code really doesn't need
[[SEP]]// the sleep takes us the full way (hopefully)
[[SEP]]// This would block if there's an error in the code
","// bus.sendEvent should block at this point since there is no attached listener// For this test we want to make sure that the background thread has had time to actually block// since if we start the input too soon we aren't testing anything// The below code attempts to make sure this happens, though it's hard to be deterministic// without making sendEvent take weird arguments the non-test code really doesn't need[[SEP]]// Try to ensure that the send actually happened. The latch gets us close,// the sleep takes us the full way (hopefully)[[SEP]]// This would block if there's an error in the code",143,170,[0],0,"[0, 0, 0, 0, 0, 1, 0, 0]",1,"[0, 0, 0]",0,0,0,0,missingInputEventuallySucceeds(),org.logstash.plugins.pipeline.PipelineBusTest,missingInputEventuallySucceeds/0,False,144,3,4,0,4,1,13,15,0,2,0,13,1,1,0,0,0,0,0,3,2,0,1,0,0,1,17,1,0,False
690,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\pipeline\PipelineBusTest.java,org.logstash.plugins.pipeline.PipelineBusTest,void whenInDefaultNonBlockingModeInputsShutdownInstantly(),"@Test
public void whenInDefaultNonBlockingModeInputsShutdownInstantly() throws InterruptedException {
    // Test confirms the default. If we decide to change the default we should change this test
    assertThat(bus.isBlockOnUnlisten()).isFalse();
    bus.registerSender(output, addresses);
    bus.listen(input, address);
    // This test would block forever if this is not non-block
    bus.unlisten(input, address);
    bus.unregisterSender(output, addresses);
}", ,"// Test confirms the default. If we decide to change the default we should change this test
[[SEP]]// This test would block forever if this is not non-block
",// Test confirms the default. If we decide to change the default we should change this test[[SEP]]// This test would block forever if this is not non-block,172,182,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,whenInDefaultNonBlockingModeInputsShutdownInstantly(),org.logstash.plugins.pipeline.PipelineBusTest,whenInDefaultNonBlockingModeInputsShutdownInstantly/0,False,173,2,5,0,5,1,7,7,0,0,0,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,13,1,0,False
691,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\pipeline\PipelineBusTest.java,org.logstash.plugins.pipeline.PipelineBusTest,void whenInBlockingModeInputsShutdownLast(),"@Test
public void whenInBlockingModeInputsShutdownLast() throws InterruptedException {
    bus.registerSender(output, addresses);
    bus.listen(input, address);
    bus.setBlockOnUnlisten(true);
    Thread unlistenThread = new Thread(() -> {
        try {
            bus.unlisten(input, address);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    });
    unlistenThread.start();
    // This should unblock the listener thread
    bus.unregisterSender(output, addresses);
    unlistenThread.join();
    assertThat(bus.addressStates).isEmpty();
}", ,"// This should unblock the listener thread
",// This should unblock the listener thread,184,205,[0],0,[0],0,[0],0,0,0,0,whenInBlockingModeInputsShutdownLast(),org.logstash.plugins.pipeline.PipelineBusTest,whenInBlockingModeInputsShutdownLast/0,False,185,2,5,0,5,2,10,18,0,1,0,10,0,0,0,0,1,0,0,0,1,0,2,0,0,1,15,1,0,False
692,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\pipeline\PipelineBusTest.java,org.logstash.plugins.pipeline.PipelineBusTest,void whenInputFailsOutputRetryOnlyNotYetDelivered(),"@Test
public void whenInputFailsOutputRetryOnlyNotYetDelivered() throws InterruptedException {
    bus.registerSender(output, addresses);
    int expectedReceiveInvocations = 2;
    CountDownLatch sendsCoupleOfCallsLatch = new CountDownLatch(expectedReceiveInvocations);
    int positionOfFailure = 1;
    input = new TestFailPipelineInput(sendsCoupleOfCallsLatch, positionOfFailure);
    bus.listen(input, address);
    final List<JrubyEventExtLibrary.RubyEvent> events = new ArrayList<>();
    events.add(rubyEvent());
    events.add(rubyEvent());
    events.add(rubyEvent());
    CountDownLatch senderThreadStarted = new CountDownLatch(1);
    Thread sendThread = new Thread(() -> {
        senderThreadStarted.countDown();
        // Exercise
        bus.sendEvents(output, events, true);
    });
    sendThread.start();
    // Ensure server thread is started
    senderThreadStarted.await();
    // Ensure that send actually happened a couple of times.
    // Send method retry mechanism sleeps 1 second on each retry!
    boolean coupleOfCallsDone = sendsCoupleOfCallsLatch.await(3, TimeUnit.SECONDS);
    sendThread.join();
    // Verify
    assertThat(coupleOfCallsDone).isTrue();
    assertThat(((TestFailPipelineInput) input).getLastBatchSize()).isEqualTo(events.size() - positionOfFailure);
}", ,"// Ensure that send actually happened a couple of times.
[[SEP]]// Exercise
[[SEP]]// Ensure server thread is started
[[SEP]]// Send method retry mechanism sleeps 1 second on each retry!
[[SEP]]// Verify
",// Exercise[[SEP]]// Ensure server thread is started[[SEP]]// Ensure that send actually happened a couple of times.// Send method retry mechanism sleeps 1 second on each retry![[SEP]]// Verify,207,240,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,whenInputFailsOutputRetryOnlyNotYetDelivered(),org.logstash.plugins.pipeline.PipelineBusTest,whenInputFailsOutputRetryOnlyNotYetDelivered/0,False,208,5,6,0,6,1,15,24,0,7,0,15,1,1,0,0,0,1,0,4,8,1,1,0,0,1,38,1,0,False
693,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\plugins\pipeline\PipelineBusTest.java,org.logstash.plugins.pipeline.PipelineBusTest.TestFailPipelineInput,ReceiveResponse internalReceive(Stream<JrubyEventExtLibrary.RubyEvent>),"@Override
public ReceiveResponse internalReceive(Stream<JrubyEventExtLibrary.RubyEvent> events) {
    receiveCalls.countDown();
    if (receiveInvocationsCount == 0) {
        // simulate a fail on first invocation at desired position
        receiveInvocationsCount++;
        return ReceiveResponse.failedAt(positionOfFailure, new Exception(""An artificial fail""));
    } else {
        receiveInvocationsCount++;
        lastBatchSize = (int) events.count();
        return ReceiveResponse.completed();
    }
}", ,"// simulate a fail on first invocation at desired position
",// simulate a fail on first invocation at desired position,275,288,[0],0,[0],0,[0],0,0,0,0,internalReceive(Stream<RubyEvent>),org.logstash.plugins.pipeline.PipelineBusTest$TestFailPipelineInput,internalReceive/1[java.util.stream.Stream<org.logstash.ext.JrubyEventExtLibrary.RubyEvent>],False,276,2,0,0,0,2,4,12,2,0,1,4,0,0,0,1,0,0,1,1,1,0,1,0,0,0,12,1,0,False
694,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\SecretIdentifierTest.java,org.logstash.secret.SecretIdentifierTest,void testBasic(),"/**
 * Example usage
 */
@Test
public void testBasic() {
    SecretIdentifier id = new SecretIdentifier(""foo"");
    assertThat(id.toExternalForm()).isEqualTo(""urn:logstash:secret:v1:foo"");
    assertThat(id.getKey()).isEqualTo(""foo"");
}","/**
 * Example usage
 */
", ,/** * Example usage */,36,41,[0],0,[0],0,[0],0,0,0,0,testBasic(),org.logstash.secret.SecretIdentifierTest,testBasic/0,False,37,2,3,0,3,1,4,5,0,1,0,4,0,0,0,0,0,0,3,0,1,0,0,0,0,0,9,1,0,True
695,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\SecretIdentifierTest.java,org.logstash.secret.SecretIdentifierTest,void testCase(),"/**
 * identifiers are case insensitive
 */
@Test
public void testCase() {
    SecretIdentifier id = new SecretIdentifier(""FOO"");
    assertThat(id.toExternalForm()).isEqualTo(""urn:logstash:secret:v1:foo"");
    SecretIdentifier id2 = new SecretIdentifier(""foo"");
    assertThat(id).isEqualTo(id2);
    assertThat(id.getKey()).isEqualTo(id2.getKey());
    assertThat(id.toExternalForm()).isEqualTo(id.toExternalForm()).isEqualTo(id.toString()).isEqualTo(id2.toString());
}","/**
 * identifiers are case insensitive
 */
", ,/** * identifiers are case insensitive */,46,54,[0],0,[0],0,[0],0,0,0,0,testCase(),org.logstash.secret.SecretIdentifierTest,testCase/0,False,47,2,4,0,4,1,5,8,0,2,0,5,0,0,0,0,0,0,3,0,2,0,0,0,0,0,11,1,0,True
696,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\SecretIdentifierTest.java,org.logstash.secret.SecretIdentifierTest,void testColon(),"/**
 * Colon in the key don't cause issues with parsing the colon separated URN
 */
@Test
public void testColon() {
    SecretIdentifier id = new SecretIdentifier(""foo:bar"");
    assertThat(id.toExternalForm()).isEqualTo(""urn:logstash:secret:v1:foo:bar"");
    assertThat(id.getKey()).isEqualTo(""foo:bar"");
}","/**
 * Colon in the key don't cause issues with parsing the colon separated URN
 */
", ,/** * Colon in the key don't cause issues with parsing the colon separated URN */,59,64,[0],0,[0],0,[0],0,0,0,0,testColon(),org.logstash.secret.SecretIdentifierTest,testColon/0,False,60,2,3,0,3,1,4,5,0,1,0,4,0,0,0,0,0,0,3,0,1,0,0,0,0,0,19,1,0,True
697,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\SecretIdentifierTest.java,org.logstash.secret.SecretIdentifierTest,void testFromExternal(),"/**
 * valid urns should be able to be constructed from the urn
 */
@Test
public void testFromExternal() {
    assertThat(SecretIdentifier.fromExternalForm(""urn:logstash:secret:v1:foo"")).isEqualTo(new SecretIdentifier(""foo""));
    assertThat(SecretIdentifier.fromExternalForm(""urn:logstash:secret:v1:foo:bar"")).isEqualTo(new SecretIdentifier(""foo:bar""));
}","/**
 * valid urns should be able to be constructed from the urn
 */
", ,/** * valid urns should be able to be constructed from the urn */,74,78,[0],0,[0],0,[0],0,0,0,0,testFromExternal(),org.logstash.secret.SecretIdentifierTest,testFromExternal/0,False,75,2,2,0,2,1,3,4,0,0,0,3,0,0,0,0,0,0,4,0,0,0,0,0,0,0,17,1,0,True
698,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\cli\SecretStoreCliTest.java,org.logstash.secret.cli.SecretStoreCliTest,void testList(),"@Test
public void testList() {
    cli.command(""list"", existingStoreConfig, null);
    // contents of the existing store is a-z for both the key and value
    for (int i = 65; i <= 90; i++) {
        String expected = new String(new byte[] { (byte) i });
        assertListed(expected.toLowerCase());
    }
    assertThat(terminal.out).doesNotContain(""keystore.seed"");
}", ,"// contents of the existing store is a-z for both the key and value
",// contents of the existing store is a-z for both the key and value,95,105,[0],0,[0],0,[0],0,0,0,0,testList(),org.logstash.secret.cli.SecretStoreCliTest,testList/0,False,96,4,2,0,2,2,5,8,0,2,0,5,1,1,1,0,0,0,2,2,2,0,1,0,0,0,7,1,0,False
699,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\cli\SecretStoreCliTest.java,org.logstash.secret.cli.SecretStoreCliTest,void testAddEmptyValue(),"@Test
public void testAddEmptyValue() {
    terminal.in = ""y"";
    cli.command(""create"", newStoreConfig, null);
    assertCreated();
    terminal.reset();
    // sets the value
    terminal.in = """";
    String id = UUID.randomUUID().toString();
    cli.command(""add"", newStoreConfig.clone(), id);
    assertThat(terminal.out).containsIgnoringCase(""ERROR"");
}", ,"// sets the value
",// sets the value,140,151,[0],0,[0],0,[0],0,0,0,0,testAddEmptyValue(),org.logstash.secret.cli.SecretStoreCliTest,testAddEmptyValue/0,False,141,5,4,0,4,1,8,10,0,1,0,8,1,1,0,0,0,0,5,0,3,0,0,0,0,0,8,1,0,False
700,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\cli\SecretStoreCliTest.java,org.logstash.secret.cli.SecretStoreCliTest,void testAdd(),"@Test
public void testAdd() {
    terminal.in = ""y"";
    cli.command(""create"", newStoreConfig, null);
    assertCreated();
    terminal.reset();
    // sets the value
    terminal.in = UUID.randomUUID().toString();
    String id = UUID.randomUUID().toString();
    cli.command(""add"", newStoreConfig.clone(), id);
    terminal.reset();
    cli.command(""list"", newStoreConfig, null);
    assertListed(id);
}", ,"// sets the value
",// sets the value,153,167,[0],0,[0],0,[0],0,0,0,0,testAdd(),org.logstash.secret.cli.SecretStoreCliTest,testAdd/0,False,154,5,5,0,5,1,7,12,0,1,0,7,2,1,0,0,0,0,4,0,3,0,0,0,0,0,6,1,0,False
701,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\cli\SecretStoreCliTest.java,org.logstash.secret.cli.SecretStoreCliTest,void testRemove(),"@Test
public void testRemove() {
    terminal.in = ""y"";
    cli.command(""create"", newStoreConfig, null);
    assertCreated();
    terminal.reset();
    // sets the value
    terminal.in = UUID.randomUUID().toString();
    String id = UUID.randomUUID().toString();
    cli.command(""add"", newStoreConfig.clone(), id);
    System.out.println(terminal.out);
    terminal.reset();
    cli.command(""list"", newStoreConfig.clone(), null);
    assertListed(id);
    terminal.reset();
    cli.command(""remove"", newStoreConfig.clone(), id);
    terminal.reset();
    cli.command(""list"", newStoreConfig, null);
    assertThat(terminal.out).doesNotContain(id);
}", ,"// sets the value
",// sets the value,169,191,[0],0,[0],0,[0],0,0,0,0,testRemove(),org.logstash.secret.cli.SecretStoreCliTest,testRemove/0,False,170,5,5,0,5,1,10,18,0,1,0,10,2,1,0,0,0,0,6,0,3,0,0,0,0,0,7,1,0,False
702,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\cli\SecretStoreCliTest.java,org.logstash.secret.cli.SecretStoreCliTest,void testRemoveMissing(),"@Test
public void testRemoveMissing() {
    terminal.in = ""y"";
    cli.command(""create"", newStoreConfig, null);
    assertCreated();
    terminal.reset();
    // sets the value
    terminal.in = UUID.randomUUID().toString();
    String id = UUID.randomUUID().toString();
    cli.command(""add"", newStoreConfig.clone(), id);
    System.out.println(terminal.out);
    terminal.reset();
    cli.command(""list"", newStoreConfig.clone(), null);
    assertListed(id);
    terminal.reset();
    cli.command(""remove"", newStoreConfig.clone(), ""notthere"");
    assertThat(terminal.out).containsIgnoringCase(""error"");
}", ,"// sets the value
",// sets the value,193,212,[0],0,[0],0,[0],0,0,0,0,testRemoveMissing(),org.logstash.secret.cli.SecretStoreCliTest,testRemoveMissing/0,False,194,5,5,0,5,1,10,16,0,1,0,10,2,1,0,0,0,0,7,0,3,0,0,0,0,0,8,1,0,False
703,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\SecretStoreFactoryTest.java,org.logstash.secret.store.SecretStoreFactoryTest,void testDefaultLoadWithEnvPass(),"@Test
public void testDefaultLoadWithEnvPass() throws Exception {
    String pass = UUID.randomUUID().toString();
    final Map<String, String> modifiedEnvironment = new HashMap<String, String>(System.getenv()) {

        {
            put(ENVIRONMENT_PASS_KEY, pass);
        }
    };
    final SecretStoreFactory secretStoreFactory = SecretStoreFactory.withEnvironment(modifiedEnvironment);
    // Each usage of the secure config requires it's own instance since implementations can/should clear all the values once used.
    SecureConfig secureConfig1 = new SecureConfig();
    secureConfig1.add(""keystore.file"", folder.newFolder().toPath().resolve(""logstash.keystore"").toString().toCharArray());
    SecureConfig secureConfig2 = secureConfig1.clone();
    SecureConfig secureConfig3 = secureConfig1.clone();
    SecureConfig secureConfig4 = secureConfig1.clone();
    // ensure that with only the environment we can retrieve the marker from the store
    SecretStore secretStore = secretStoreFactory.create(secureConfig1);
    validateMarker(secretStore);
    // ensure that aren't simply using the defaults
    boolean expectedException = false;
    try {
        new JavaKeyStore().create(secureConfig2);
    } catch (SecretStoreException e) {
        expectedException = true;
    }
    assertThat(expectedException).isTrue();
    // ensure that direct key access using the system key wil work
    secureConfig3.add(KEYSTORE_ACCESS_KEY, pass.toCharArray());
    secretStore = new JavaKeyStore().load(secureConfig3);
    validateMarker(secretStore);
    // ensure that pass will work again
    secretStore = secretStoreFactory.load(secureConfig4);
    validateMarker(secretStore);
}", ,"// Each usage of the secure config requires it's own instance since implementations can/should clear all the values once used.
[[SEP]]// ensure that with only the environment we can retrieve the marker from the store
[[SEP]]// ensure that aren't simply using the defaults
[[SEP]]// ensure that direct key access using the system key wil work
[[SEP]]// ensure that pass will work again
",// Each usage of the secure config requires it's own instance since implementations can/should clear all the values once used.[[SEP]]// ensure that with only the environment we can retrieve the marker from the store[[SEP]]// ensure that aren't simply using the defaults[[SEP]]// ensure that direct key access using the system key wil work[[SEP]]// ensure that pass will work again,99,136,[0],0,"[0, 0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,testDefaultLoadWithEnvPass(),org.logstash.secret.store.SecretStoreFactoryTest,testDefaultLoadWithEnvPass/0,False,100,6,11,0,11,2,19,30,0,9,0,19,1,1,0,0,1,0,2,0,12,0,1,1,0,0,32,1,0,False
704,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\SecretStoreFactoryTest.java,org.logstash.secret.store.SecretStoreFactoryTest,void testErrorLoading(),"/**
 * Ensures that load failure is the correct type.
 */
@Test
public void testErrorLoading() {
    thrown.expect(SecretStoreException.LoadException.class);
    // default implementation requires a path
    secretStoreFactory.load(new SecureConfig());
}","/**
 * Ensures that load failure is the correct type.
 */
","// default implementation requires a path
",/** * Ensures that load failure is the correct type. */[[SEP]]// default implementation requires a path,141,146,[0],0,[0],0,"[0, 0]",0,0,0,0,testErrorLoading(),org.logstash.secret.store.SecretStoreFactoryTest,testErrorLoading/0,False,142,4,2,0,2,1,2,4,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,12,1,0,True
705,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\SecureConfigTest.java,org.logstash.secret.store.SecureConfigTest,void test(),"@Test
public void test() throws Exception {
    Set<String> expected = new HashSet(100);
    IntStream.range(0, 100).forEach(i -> expected.add(UUID.randomUUID().toString()));
    expected.forEach(s -> secureConfig.add(s, s.toCharArray()));
    expected.forEach(s -> assertThat(secureConfig.getPlainText(s)).isEqualTo(s.toCharArray()));
    expected.forEach(s -> assertThat(secureConfig.has(s)));
    SecureConfig clone = secureConfig.clone();
    expected.forEach(s -> assertThat(clone.getPlainText(s)).isEqualTo(s.toCharArray()));
    expected.forEach(s -> assertThat(clone.has(s)));
    secureConfig.clearValues();
    // manually reset cleared flag to allow the assertions
    secureConfig.cleared = false;
    expected.forEach(s -> assertThat(secureConfig.getPlainText(s)).containsOnly('\0'));
    // clone is not zero'ed
    expected.forEach(s -> assertThat(clone.getPlainText(s)).isEqualTo(s.toCharArray()));
}", ,"// manually reset cleared flag to allow the assertions
[[SEP]]// clone is not zero'ed
",// manually reset cleared flag to allow the assertions[[SEP]]// clone is not zero'ed,46,62,[0],0,"[0, 0]",0,"[0, 0]",0,0,0,0,test(),org.logstash.secret.store.SecureConfigTest,test/0,False,47,2,5,0,5,1,15,14,0,10,0,15,0,0,0,0,0,0,0,3,3,0,0,0,0,8,13,1,0,False
706,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void main(String...),"/**
 * Launch a second JVM with the expected args
 * <ul>
 * <li>arg[0] - the descriptor to identify which test this is for</li>
 * <li>arg[1] - path to file to write as marker that the second JVM is ready to be tested</li>
 * <li>arg[2..n] - any additional information needed for the test</li>
 * </ul>
 *
 * @param args the args as described
 * @throws IOException when i/o exceptions happen
 */
public static void main(String... args) throws IOException, InterruptedException {
    Path magicFile = Paths.get(args[1]);
    // Use a second JVM to lock the keystore for 2 seconds
    if (EXTERNAL_TEST_FILE_LOCK.equals(args[0])) {
        Path keystoreFile = Paths.get(args[2]);
        FileLock fileLock = null;
        try (final FileOutputStream keystore = new FileOutputStream(keystoreFile.toFile(), true)) {
            fileLock = keystore.getChannel().tryLock();
            assertThat(fileLock).isNotNull();
            // write the magic file to let the other process know the test is ready
            try (final OutputStream os = Files.newOutputStream(magicFile)) {
                os.write(args[0].getBytes(StandardCharsets.UTF_8));
                Thread.sleep(2000);
            } finally {
                Files.delete(magicFile);
            }
        } finally {
            if (fileLock != null) {
                fileLock.release();
            }
        }
    } else if (EXTERNAL_TEST_WRITE.equals(args[0])) {
        Path keyStoreFile = Paths.get(args[2]);
        SecureConfig config = new SecureConfig();
        config.add(""keystore.file"", keyStoreFile.toAbsolutePath().toString().toCharArray());
        JavaKeyStore keyStore = new JavaKeyStore().create(config);
        writeAtoZ(keyStore);
        validateAtoZ(keyStore);
        // write the magic file to let the other process know the test is ready
        try (final OutputStream os = Files.newOutputStream(magicFile)) {
            os.write(args[0].getBytes(StandardCharsets.UTF_8));
        } finally {
            Files.delete(magicFile);
        }
    }
}","/**
 * Launch a second JVM with the expected args
 * <ul>
 * <li>arg[0] - the descriptor to identify which test this is for</li>
 * <li>arg[1] - path to file to write as marker that the second JVM is ready to be tested</li>
 * <li>arg[2..n] - any additional information needed for the test</li>
 * </ul>
 *
 * @param args the args as described
 * @throws IOException when i/o exceptions happen
 */
","// Use a second JVM to lock the keystore for 2 seconds
[[SEP]]// write the magic file to let the other process know the test is ready
[[SEP]]// write the magic file to let the other process know the test is ready
",/** * Launch a second JVM with the expected args * <ul> * <li>arg[0] - the descriptor to identify which test this is for</li> * <li>arg[1] - path to file to write as marker that the second JVM is ready to be tested</li> * <li>arg[2..n] - any additional information needed for the test</li> * </ul> * * @param args the args as described * @throws IOException when i/o exceptions happen */[[SEP]]// Use a second JVM to lock the keystore for 2 seconds[[SEP]]// write the magic file to let the other process know the test is ready[[SEP]]// write the magic file to let the other process know the test is ready,89,126,[0],0,"[0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,main(String[]),org.logstash.secret.store.backend.JavaKeyStoreTest,main/1[java.lang.String[]],False,89,3,6,0,6,4,20,37,0,9,1,20,2,1,0,1,3,0,1,8,10,0,3,0,0,0,62,9,0,True
707,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void validateAtoZ(JavaKeyStore),"private static void validateAtoZ(JavaKeyStore keyStore) {
    // contents of the existing is a-z for both the key and value
    for (int i = 65; i <= 90; i++) {
        byte[] expected = new byte[] { (byte) i };
        SecretIdentifier id = new SecretIdentifier(new String(expected, StandardCharsets.UTF_8));
        assertThat(keyStore.retrieveSecret(id)).isEqualTo(expected);
    }
}", ,"// contents of the existing is a-z for both the key and value
",// contents of the existing is a-z for both the key and value,128,135,[0],0,[0],0,[0],0,0,0,0,validateAtoZ(JavaKeyStore),org.logstash.secret.store.backend.JavaKeyStoreTest,validateAtoZ/1[org.logstash.secret.store.backend.JavaKeyStore],False,128,3,6,4,2,2,3,7,0,3,1,3,0,0,1,0,0,0,0,2,3,0,1,0,0,0,14,10,0,False
708,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void writeAtoZ(JavaKeyStore),"private static void writeAtoZ(JavaKeyStore keyStore) {
    // a-z key and value
    for (int i = 65; i <= 90; i++) {
        byte[] expected = new byte[] { (byte) i };
        SecretIdentifier id = new SecretIdentifier(new String(expected, StandardCharsets.UTF_8));
        keyStore.persistSecret(id, expected);
    }
}", ,"// a-z key and value
",// a-z key and value,137,144,[0],0,[0],0,[0],0,0,0,0,writeAtoZ(JavaKeyStore),org.logstash.secret.store.backend.JavaKeyStoreTest,writeAtoZ/1[org.logstash.secret.store.backend.JavaKeyStore],False,137,3,4,2,2,2,1,7,0,3,1,1,0,0,1,0,0,0,0,2,3,0,1,0,0,0,12,10,0,False
709,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void basicTest(),"/**
 * Simple example usage.
 */
@Test
public void basicTest() {
    String password = ""pAssW3rd!"";
    // persist
    keyStore.persistSecret(new SecretIdentifier(""mysql.password""), password.getBytes(StandardCharsets.UTF_8));
    // retrieve
    byte[] secret = keyStore.retrieveSecret(new SecretIdentifier(""mysql.password""));
    assertThat(new String(secret, StandardCharsets.UTF_8)).isEqualTo(password);
    // purge
    keyStore.purgeSecret(new SecretIdentifier(""mysql.password""));
    secret = keyStore.retrieveSecret(new SecretIdentifier(""mysql.password""));
    assertThat(secret).isNull();
}","/**
 * Simple example usage.
 */
","// persist
[[SEP]]// retrieve
[[SEP]]// purge
",/** * Simple example usage. */[[SEP]]// persist[[SEP]]// retrieve[[SEP]]// purge,166,178,[0],0,"[0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,basicTest(),org.logstash.secret.store.backend.JavaKeyStoreTest,basicTest/0,False,167,4,4,0,4,1,7,9,0,2,0,7,0,0,0,0,0,0,5,0,3,0,0,0,0,0,10,1,0,True
710,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void isLogstashKeystore(),"/**
 * Tests that the magic marker that identifies this a logstash keystore is present.  This marker helps to ensure that we are only dealing with our keystore, we do not want
 * to support arbitrary keystores.
 *
 * @throws Exception when ever it wants to.
 */
@Test
public void isLogstashKeystore() throws Exception {
    // newly created
    byte[] marker = keyStore.retrieveSecret(LOGSTASH_MARKER);
    assertThat(new String(marker, StandardCharsets.UTF_8)).isEqualTo(LOGSTASH_MARKER.getKey());
    // exiting
    JavaKeyStore existingKeyStore = new JavaKeyStore().load(withDefinedPassConfig);
    marker = existingKeyStore.retrieveSecret(LOGSTASH_MARKER);
    assertThat(new String(marker, StandardCharsets.UTF_8)).isEqualTo(LOGSTASH_MARKER.getKey());
}","/**
 * Tests that the magic marker that identifies this a logstash keystore is present.  This marker helps to ensure that we are only dealing with our keystore, we do not want
 * to support arbitrary keystores.
 *
 * @throws Exception when ever it wants to.
 */
","// newly created
[[SEP]]// exiting
","/** * Tests that the magic marker that identifies this a logstash keystore is present.  This marker helps to ensure that we are only dealing with our keystore, we do not want * to support arbitrary keystores. * * @throws Exception when ever it wants to. */[[SEP]]// newly created[[SEP]]// exiting",194,204,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,0,0,isLogstashKeystore(),org.logstash.secret.store.backend.JavaKeyStoreTest,isLogstashKeystore/0,False,195,4,4,0,4,1,5,7,0,2,0,5,0,0,0,0,0,0,0,0,3,0,0,0,0,0,52,1,0,True
711,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void notLogstashKeystore(),"/**
 * Tests that trying to read a random file throws the right error.
 *
 * @throws Exception when ever it wants to.
 */
@Test
public void notLogstashKeystore() throws Exception {
    thrown.expect(SecretStoreException.class);
    SecureConfig altConfig = new SecureConfig();
    Path altPath = folder.newFolder().toPath().resolve(""alt.not.a.logstash.keystore"");
    try (OutputStream out = Files.newOutputStream(altPath)) {
        byte[] randomBytes = new byte[300];
        new Random().nextBytes(randomBytes);
        out.write(randomBytes);
    }
    altConfig.add(""keystore.file"", altPath.toString().toCharArray());
    new JavaKeyStore().load(altConfig);
}","/**
 * Tests that trying to read a random file throws the right error.
 *
 * @throws Exception when ever it wants to.
 */
", ,/** * Tests that trying to read a random file throws the right error. * * @throws Exception when ever it wants to. */,211,223,[0],0,[0],0,[0],0,0,0,0,notLogstashKeystore(),org.logstash.secret.store.backend.JavaKeyStoreTest,notLogstashKeystore/0,False,212,5,4,0,4,1,11,12,0,4,0,11,0,0,0,0,1,0,2,1,4,0,1,0,0,0,30,1,0,True
712,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void notLogstashKeystoreNoMarker(),"/**
 * Tests that when the magic marker that identifies this a logstash keystore is not present the correct exception is thrown.
 *
 * @throws Exception when ever it wants to.
 */
@Test
public void notLogstashKeystoreNoMarker() throws Exception {
    thrown.expect(SecretStoreException.LoadException.class);
    withDefinedPassConfig.add(""keystore.file"", Paths.get(this.getClass().getClassLoader().getResource(""not.a.logstash.keystore"").toURI()).toString().toCharArray().clone());
    new JavaKeyStore().load(withDefinedPassConfig);
}","/**
 * Tests that when the magic marker that identifies this a logstash keystore is not present the correct exception is thrown.
 *
 * @throws Exception when ever it wants to.
 */
", ,/** * Tests that when the magic marker that identifies this a logstash keystore is not present the correct exception is thrown. * * @throws Exception when ever it wants to. */,230,235,[0],0,[0],0,[0],0,0,0,0,notLogstashKeystoreNoMarker(),org.logstash.secret.store.backend.JavaKeyStoreTest,notLogstashKeystoreNoMarker/0,False,231,4,3,0,3,1,11,5,0,0,0,11,0,0,0,0,0,0,2,0,0,0,0,0,0,0,30,1,0,True
713,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void overwriteExisting(),"/**
 * Overwrite should be no-error overwrite
 */
@Test
public void overwriteExisting() {
    SecretIdentifier id = new SecretIdentifier(""myId"");
    int originalSize = keyStore.list().size();
    keyStore.persistSecret(id, ""password1"".getBytes(StandardCharsets.UTF_8));
    assertThat(keyStore.list().size()).isEqualTo(originalSize + 1);
    assertThat(new String(keyStore.retrieveSecret(id), StandardCharsets.UTF_8)).isEqualTo(""password1"");
    keyStore.persistSecret(id, ""password2"".getBytes(StandardCharsets.UTF_8));
    assertThat(keyStore.list().size()).isEqualTo(originalSize + 1);
    assertThat(new String(keyStore.retrieveSecret(id), StandardCharsets.UTF_8)).isEqualTo(""password2"");
}","/**
 * Overwrite should be no-error overwrite
 */
", ,/** * Overwrite should be no-error overwrite */,240,252,[0],0,[0],0,[0],0,0,0,0,overwriteExisting(),org.logstash.secret.store.backend.JavaKeyStoreTest,overwriteExisting/0,False,241,3,4,0,4,1,7,10,0,2,0,7,0,0,0,0,0,0,5,2,2,2,0,0,0,0,14,1,0,True
714,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void purgeMissingSecret(),"/**
 * Purging missing secrets should be no-error no-op
 */
@Test
public void purgeMissingSecret() {
    Collection<SecretIdentifier> original = keyStore.list();
    keyStore.purgeSecret(new SecretIdentifier(""does-not-exist""));
    assertThat(keyStore.list().toArray()).containsExactlyInAnyOrder(original.toArray());
}","/**
 * Purging missing secrets should be no-error no-op
 */
", ,/** * Purging missing secrets should be no-error no-op */,257,262,[0],0,[0],0,[0],0,0,0,0,purgeMissingSecret(),org.logstash.secret.store.backend.JavaKeyStoreTest,purgeMissingSecret/0,False,258,3,3,0,3,1,5,5,0,1,0,5,0,0,0,0,0,0,1,0,1,0,0,0,0,0,16,1,0,True
715,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void readExisting(),"/**
 * Tests that we can read a pre-existing keystore from disk.
 *
 * @throws Exception when ever it wants to.
 */
@Test
public void readExisting() throws Exception {
    // uses an explicit password
    validateAtoZ(new JavaKeyStore().load(this.withDefinedPassConfig));
    // uses an implicit password
    validateAtoZ(new JavaKeyStore().load(this.withDefaultPassConfig));
}","/**
 * Tests that we can read a pre-existing keystore from disk.
 *
 * @throws Exception when ever it wants to.
 */
","// uses an explicit password
[[SEP]]// uses an implicit password
",/** * Tests that we can read a pre-existing keystore from disk. * * @throws Exception when ever it wants to. */[[SEP]]// uses an explicit password[[SEP]]// uses an implicit password,269,276,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,0,0,readExisting(),org.logstash.secret.store.backend.JavaKeyStoreTest,readExisting/0,False,270,3,3,0,3,1,2,4,0,0,0,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,22,1,0,True
716,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void readWriteListDelete(),"/**
 * Comprehensive tests that uses a freshly created keystore to write 26 entries, list them, read them, and delete them.
 */
@Test
public void readWriteListDelete() {
    writeAtoZ(keyStore);
    Collection<SecretIdentifier> foundIds = keyStore.list();
    assertThat(keyStore.list().size()).isEqualTo(26 + 1);
    validateAtoZ(keyStore);
    foundIds.stream().filter(id -> !id.equals(LOGSTASH_MARKER)).forEach(id -> keyStore.purgeSecret(id));
    assertThat(keyStore.list().size()).isEqualTo(1);
    assertThat(keyStore.list().stream().findFirst().get()).isEqualTo(LOGSTASH_MARKER);
}","/**
 * Comprehensive tests that uses a freshly created keystore to write 26 entries, list them, read them, and delete them.
 */
", ,"/** * Comprehensive tests that uses a freshly created keystore to write 26 entries, list them, read them, and delete them. */",281,290,[0],0,[0],0,[0],0,0,0,0,readWriteListDelete(),org.logstash.secret.store.backend.JavaKeyStoreTest,readWriteListDelete/0,False,282,4,5,0,5,1,13,9,0,3,0,13,2,1,0,0,0,0,0,3,1,1,0,0,0,2,39,1,0,True
717,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void retrieveMissingSecret(),"/**
 * Retrieving missing should be no-error, null result
 */
@Test
public void retrieveMissingSecret() {
    assertThat(keyStore.retrieveSecret(new SecretIdentifier(""does-not-exist""))).isNull();
}","/**
 * Retrieving missing should be no-error, null result
 */
", ,"/** * Retrieving missing should be no-error, null result */",295,298,[0],0,[0],0,[0],0,0,0,0,retrieveMissingSecret(),org.logstash.secret.store.backend.JavaKeyStoreTest,retrieveMissingSecret/0,False,296,3,2,0,2,1,3,3,0,0,0,3,0,0,0,0,0,0,1,0,0,0,0,0,0,0,14,1,0,True
718,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void retrieveWithInvalidInput(),"/**
 * Invalid input should be no-error, null result
 */
@Test
public void retrieveWithInvalidInput() {
    assertThat(keyStore.retrieveSecret(null)).isNull();
}","/**
 * Invalid input should be no-error, null result
 */
", ,"/** * Invalid input should be no-error, null result */",303,306,[0],0,[0],0,[0],0,0,0,0,retrieveWithInvalidInput(),org.logstash.secret.store.backend.JavaKeyStoreTest,retrieveWithInvalidInput/0,False,304,2,1,0,1,1,3,3,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,12,1,0,True
719,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void tamperedKeystore(),"/**
 * Test to ensure that keystore is tamper proof.  This really ends up testing the Java's KeyStore implementation, not the code here....but an important attribute to ensure
 * for any type of secret store.
 *
 * @throws Exception when ever it wants to
 */
@Test
public void tamperedKeystore() throws Exception {
    thrown.expect(SecretStoreException.class);
    byte[] keyStoreAsBytes = Files.readAllBytes(Paths.get(new String(keyStorePath)));
    // bump the middle byte by 1
    int tamperLocation = keyStoreAsBytes.length / 2;
    keyStoreAsBytes[tamperLocation] = (byte) (keyStoreAsBytes[tamperLocation] + 1);
    Path tamperedPath = folder.newFolder().toPath().resolve(""tampered.logstash.keystore"");
    Files.write(tamperedPath, keyStoreAsBytes);
    SecureConfig sc = new SecureConfig();
    sc.add(""keystore.file"", tamperedPath.toString().toCharArray());
    new JavaKeyStore().load(sc);
}","/**
 * Test to ensure that keystore is tamper proof.  This really ends up testing the Java's KeyStore implementation, not the code here....but an important attribute to ensure
 * for any type of secret store.
 *
 * @throws Exception when ever it wants to
 */
","// bump the middle byte by 1
","/** * Test to ensure that keystore is tamper proof.  This really ends up testing the Java's KeyStore implementation, not the code here....but an important attribute to ensure * for any type of secret store. * * @throws Exception when ever it wants to */[[SEP]]// bump the middle byte by 1",314,326,[0],0,[0],0,"[0, 0]",0,0,0,0,tamperedKeystore(),org.logstash.secret.store.backend.JavaKeyStoreTest,tamperedKeystore/0,False,315,5,4,0,4,1,11,11,0,4,0,11,0,0,0,0,0,1,2,2,5,2,0,0,0,0,40,1,0,True
720,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testAlreadyCreated(),"/**
 * Ensures correct error when trying to re-create a pre-existing store
 *
 * @throws IOException when it goes boom.
 */
@Test
public void testAlreadyCreated() throws IOException {
    thrown.expect(SecretStoreException.AlreadyExistsException.class);
    SecureConfig secureConfig = new SecureConfig();
    secureConfig.add(""keystore.file"", keyStorePath.clone());
    new JavaKeyStore().create(secureConfig);
}","/**
 * Ensures correct error when trying to re-create a pre-existing store
 *
 * @throws IOException when it goes boom.
 */
", ,/** * Ensures correct error when trying to re-create a pre-existing store * * @throws IOException when it goes boom. */,333,339,[0],0,[0],0,[0],0,0,0,0,testAlreadyCreated(),org.logstash.secret.store.backend.JavaKeyStoreTest,testAlreadyCreated/0,False,334,4,4,0,4,1,4,6,0,1,0,4,0,0,0,0,0,0,1,0,1,0,0,0,0,0,26,1,0,True
721,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testDefaultPermissions(),"/**
 * The default permissions should be restrictive for Posix filesystems.
 *
 * @throws Exception when it goes boom.
 */
@Test
public void testDefaultPermissions() throws Exception {
    PosixFileAttributeView attrs = Files.getFileAttributeView(Paths.get(new String(keyStorePath)), PosixFileAttributeView.class);
    boolean isWindows = System.getProperty(""os.name"").startsWith(""Windows"");
    // not all Windows FS are Posix
    if (!isWindows && attrs == null) {
        fail(""Can not determine POSIX file permissions for "" + keyStore + "" this is likely an error in the test"");
    }
    // if we got attributes, lets assert them.
    if (attrs != null) {
        Set<PosixFilePermission> permissions = attrs.readAttributes().permissions();
        EnumSet<PosixFilePermission> expected = EnumSet.of(OWNER_READ, OWNER_WRITE, GROUP_READ, OTHERS_READ);
        assertThat(permissions.toArray()).containsExactlyInAnyOrder(expected.toArray());
    }
}","/**
 * The default permissions should be restrictive for Posix filesystems.
 *
 * @throws Exception when it goes boom.
 */
","// not all Windows FS are Posix
[[SEP]]// if we got attributes, lets assert them.
","/** * The default permissions should be restrictive for Posix filesystems. * * @throws Exception when it goes boom. */[[SEP]]// not all Windows FS are Posix[[SEP]]// if we got attributes, lets assert them.",346,361,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,0,0,testDefaultPermissions(),org.logstash.secret.store.backend.JavaKeyStoreTest,testDefaultPermissions/0,False,347,1,0,0,0,4,12,12,0,4,0,12,0,0,0,2,0,0,4,0,4,1,1,0,0,0,40,1,0,True
722,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testEmptyNotAllowedOnCreate(),"/**
 * Empty passwords are not allowed
 *
 * @throws IOException when ever it wants to
 */
@Test
public void testEmptyNotAllowedOnCreate() throws IOException {
    thrown.expect(SecretStoreException.CreateException.class);
    Path altPath = folder.newFolder().toPath().resolve(""alt.logstash.keystore"");
    SecureConfig altConfig = new SecureConfig();
    altConfig.add(""keystore.file"", altPath.toString().toCharArray());
    altConfig.add(SecretStoreFactory.KEYSTORE_ACCESS_KEY, """".toCharArray());
    new JavaKeyStore().create(altConfig);
}","/**
 * Empty passwords are not allowed
 *
 * @throws IOException when ever it wants to
 */
", ,/** * Empty passwords are not allowed * * @throws IOException when ever it wants to */,384,392,[0],0,[0],0,[0],0,0,0,0,testEmptyNotAllowedOnCreate(),org.logstash.secret.store.backend.JavaKeyStoreTest,testEmptyNotAllowedOnCreate/0,False,385,4,4,0,4,1,8,8,0,2,0,8,0,0,0,0,0,0,3,0,2,0,0,0,0,0,25,1,0,True
723,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testEmptyNotAllowedOnExisting(),"/**
 * Empty passwords should always throw an Access Exception
 *
 * @throws Exception when ever it wants to
 */
@Test
public void testEmptyNotAllowedOnExisting() throws Exception {
    thrown.expect(SecretStoreException.AccessException.class);
    Path altPath = folder.newFolder().toPath().resolve(""alt.logstash.keystore"");
    SecureConfig altConfig = new SecureConfig();
    altConfig.add(""keystore.file"", altPath.toString().toCharArray());
    SecureConfig altConfig2 = altConfig.clone();
    altConfig2.add(""keystore.file"", altPath.toString().toCharArray());
    altConfig2.add(SecretStoreFactory.KEYSTORE_ACCESS_KEY, """".toCharArray());
    new JavaKeyStore().create(altConfig);
    new JavaKeyStore().load(altConfig2);
}","/**
 * Empty passwords should always throw an Access Exception
 *
 * @throws Exception when ever it wants to
 */
", ,/** * Empty passwords should always throw an Access Exception * * @throws Exception when ever it wants to */,399,410,[0],0,[0],0,[0],0,0,0,0,testEmptyNotAllowedOnExisting(),org.logstash.secret.store.backend.JavaKeyStoreTest,testEmptyNotAllowedOnExisting/0,False,400,4,6,0,6,1,10,11,0,3,0,10,0,0,0,0,0,0,4,0,3,0,0,0,0,0,25,1,0,True
724,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testExternalUpdateList(),"/**
 * Simulates different JVMs modifying the keystore and ensure a consistent list view
 *
 * @throws IOException when it goes boom.
 */
@Test
public void testExternalUpdateList() throws IOException {
    Path altPath = folder.newFolder().toPath().resolve(""alt.logstash.keystore"");
    SecureConfig secureConfig = new SecureConfig();
    secureConfig.add(""keystore.file"", altPath.toString().toCharArray());
    JavaKeyStore keyStore1 = new JavaKeyStore().create(secureConfig.clone());
    JavaKeyStore keyStore2 = new JavaKeyStore().load(secureConfig);
    String value = UUID.randomUUID().toString();
    SecretIdentifier id = new SecretIdentifier(value);
    // jvm1 persist, jvm2 list
    keyStore1.persistSecret(id, value.getBytes(StandardCharsets.UTF_8));
    assertThat(keyStore2.list().stream().map(k -> keyStore2.retrieveSecret(k)).map(v -> new String(v, StandardCharsets.UTF_8)).collect(Collectors.toSet())).contains(value);
    // purge from jvm1
    assertThat(new String(keyStore2.retrieveSecret(new SecretIdentifier(value)), StandardCharsets.UTF_8)).isEqualTo(value);
    keyStore1.purgeSecret(id);
    assertThat(keyStore2.retrieveSecret(new SecretIdentifier(value))).isNull();
}","/**
 * Simulates different JVMs modifying the keystore and ensure a consistent list view
 *
 * @throws IOException when it goes boom.
 */
","// jvm1 persist, jvm2 list
[[SEP]]// purge from jvm1
","/** * Simulates different JVMs modifying the keystore and ensure a consistent list view * * @throws IOException when it goes boom. */[[SEP]]// jvm1 persist, jvm2 list[[SEP]]// purge from jvm1",417,433,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,0,0,testExternalUpdateList(),org.logstash.secret.store.backend.JavaKeyStoreTest,testExternalUpdateList/0,False,418,4,11,0,11,1,25,14,0,8,0,25,0,0,0,0,0,0,2,0,6,0,0,0,0,2,43,1,0,True
725,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testExternalUpdatePersist(),"/**
 * Simulates different JVMs modifying the keystore and ensure a consistent view
 *
 * @throws IOException when it goes boom.
 */
@Test
public void testExternalUpdatePersist() throws IOException {
    Path altPath = folder.newFolder().toPath().resolve(""alt.logstash.keystore"");
    SecureConfig secureConfig = new SecureConfig();
    secureConfig.add(""keystore.file"", altPath.toString().toCharArray());
    JavaKeyStore keyStore1 = new JavaKeyStore().create(secureConfig.clone());
    JavaKeyStore keyStore2 = new JavaKeyStore().load(secureConfig);
    String value1 = UUID.randomUUID().toString();
    String value2 = UUID.randomUUID().toString();
    SecretIdentifier id1 = new SecretIdentifier(value1);
    SecretIdentifier id2 = new SecretIdentifier(value2);
    // jvm1 persist id1, jvm2 persist id2
    keyStore1.persistSecret(id1, value1.getBytes(StandardCharsets.UTF_8));
    keyStore2.persistSecret(id2, value2.getBytes(StandardCharsets.UTF_8));
    // both keystores should contain both values
    assertThat(keyStore1.list().stream().map(k -> keyStore1.retrieveSecret(k)).map(v -> new String(v, StandardCharsets.UTF_8)).collect(Collectors.toSet())).contains(value1, value2);
    assertThat(keyStore2.list().stream().map(k -> keyStore2.retrieveSecret(k)).map(v -> new String(v, StandardCharsets.UTF_8)).collect(Collectors.toSet())).contains(value1, value2);
    // purge from jvm1
    keyStore1.purgeSecret(id1);
    keyStore1.purgeSecret(id2);
    assertThat(keyStore1.retrieveSecret(new SecretIdentifier(value1))).isNull();
    assertThat(keyStore1.retrieveSecret(new SecretIdentifier(value2))).isNull();
    assertThat(keyStore2.retrieveSecret(new SecretIdentifier(value1))).isNull();
    assertThat(keyStore2.retrieveSecret(new SecretIdentifier(value2))).isNull();
}","/**
 * Simulates different JVMs modifying the keystore and ensure a consistent view
 *
 * @throws IOException when it goes boom.
 */
","// jvm1 persist id1, jvm2 persist id2
[[SEP]]// both keystores should contain both values
[[SEP]]// purge from jvm1
","/** * Simulates different JVMs modifying the keystore and ensure a consistent view * * @throws IOException when it goes boom. */[[SEP]]// jvm1 persist id1, jvm2 persist id2[[SEP]]// both keystores should contain both values[[SEP]]// purge from jvm1",440,466,[0],0,"[0, 0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,testExternalUpdatePersist(),org.logstash.secret.store.backend.JavaKeyStoreTest,testExternalUpdatePersist/0,False,441,4,11,0,11,1,24,21,0,12,0,24,0,0,0,0,0,0,2,0,8,0,0,0,0,4,44,1,0,True
726,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testExternalUpdateRead(),"/**
 * Simulates different JVMs modifying the keystore and ensure a consistent read view
 *
 * @throws IOException when it goes boom.
 */
@Test
public void testExternalUpdateRead() throws IOException {
    Path altPath = folder.newFolder().toPath().resolve(""alt.logstash.keystore"");
    SecureConfig secureConfig = new SecureConfig();
    secureConfig.add(""keystore.file"", altPath.toString().toCharArray());
    secureConfig.add(SecretStoreFactory.KEYSTORE_ACCESS_KEY, ""mypass"".toCharArray());
    JavaKeyStore keyStore1 = new JavaKeyStore().create(secureConfig.clone());
    JavaKeyStore keyStore2 = new JavaKeyStore().load(secureConfig);
    String value = UUID.randomUUID().toString();
    SecretIdentifier id = new SecretIdentifier(value);
    // jvm1 persist, jvm2 read
    keyStore1.persistSecret(id, value.getBytes(StandardCharsets.UTF_8));
    assertThat(new String(keyStore2.retrieveSecret(new SecretIdentifier(value)), StandardCharsets.UTF_8)).isEqualTo(value);
    // purge from jvm2
    assertThat(new String(keyStore1.retrieveSecret(new SecretIdentifier(value)), StandardCharsets.UTF_8)).isEqualTo(value);
    keyStore2.purgeSecret(id);
    assertThat(keyStore1.retrieveSecret(new SecretIdentifier(value))).isNull();
}","/**
 * Simulates different JVMs modifying the keystore and ensure a consistent read view
 *
 * @throws IOException when it goes boom.
 */
","// jvm1 persist, jvm2 read
[[SEP]]// purge from jvm2
","/** * Simulates different JVMs modifying the keystore and ensure a consistent read view * * @throws IOException when it goes boom. */[[SEP]]// jvm1 persist, jvm2 read[[SEP]]// purge from jvm2",473,490,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,0,0,testExternalUpdateRead(),org.logstash.secret.store.backend.JavaKeyStoreTest,testExternalUpdateRead/0,False,474,4,10,0,10,1,18,15,0,6,0,18,0,0,0,0,0,0,3,0,6,0,0,0,0,0,41,1,0,True
727,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testFileLock(),"/**
 * Spins up a second VM, locks the underlying keystore, asserts correct exception, once lock is released and now can write
 *
 * @throws Exception when exceptions happen
 */
@Test
public void testFileLock() throws Exception {
    boolean isWindows = System.getProperty(""os.name"").startsWith(""Windows"");
    Path magicFile = folder.newFolder().toPath().resolve(EXTERNAL_TEST_FILE_LOCK);
    String java = System.getProperty(""java.home"") + File.separator + ""bin"" + File.separator + ""java"";
    ProcessBuilder builder = new ProcessBuilder(java, ""-cp"", System.getProperty(""java.class.path""), getClass().getCanonicalName(), EXTERNAL_TEST_FILE_LOCK, magicFile.toAbsolutePath().toString(), new String(keyStorePath));
    Future<Integer> future = Executors.newScheduledThreadPool(1).submit(() -> builder.start().waitFor());
    boolean passed = false;
    while (!future.isDone()) {
        try {
            Files.readAllBytes(magicFile);
        } catch (NoSuchFileException sfe) {
            Thread.sleep(100);
            continue;
        }
        try {
            keyStore.persistSecret(new SecretIdentifier(""foo""), ""bar"".getBytes(StandardCharsets.UTF_8));
        } catch (SecretStoreException.PersistException e) {
            assertThat(e.getCause().getMessage()).contains(""locked"");
            passed = true;
        }
        break;
    }
    assertThat(passed).isTrue();
    // The keystore.store method on Windows checks for the file lock and does not allow _any_ interaction with the keystore if it is locked.
    if (!isWindows) {
        // can still read
        byte[] marker = keyStore.retrieveSecret(LOGSTASH_MARKER);
        assertThat(new String(marker, StandardCharsets.UTF_8)).isEqualTo(LOGSTASH_MARKER.getKey());
    }
    // block until other JVM finishes
    future.get();
    // can write/read now
    SecretIdentifier id = new SecretIdentifier(""foo2"");
    keyStore.persistSecret(id, ""bar"".getBytes(StandardCharsets.UTF_8));
    assertThat(new String(keyStore.retrieveSecret(id), StandardCharsets.UTF_8)).isEqualTo(""bar"");
}","/**
 * Spins up a second VM, locks the underlying keystore, asserts correct exception, once lock is released and now can write
 *
 * @throws Exception when exceptions happen
 */
","// The keystore.store method on Windows checks for the file lock and does not allow _any_ interaction with the keystore if it is locked.
[[SEP]]// can still read
[[SEP]]// block until other JVM finishes
[[SEP]]// can write/read now
","/** * Spins up a second VM, locks the underlying keystore, asserts correct exception, once lock is released and now can write * * @throws Exception when exceptions happen */[[SEP]]// The keystore.store method on Windows checks for the file lock and does not allow _any_ interaction with the keystore if it is locked.[[SEP]]// can still read[[SEP]]// block until other JVM finishes[[SEP]]// can write/read now",497,538,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,testFileLock(),org.logstash.secret.store.backend.JavaKeyStoreTest,testFileLock/0,False,498,4,4,0,4,5,27,34,0,8,0,27,0,0,1,0,2,0,13,2,9,1,2,0,0,1,64,1,0,True
728,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testGeneratedSecret(),"/**
 * Simulates different JVMs can read using a default (non-provided) password
 *
 * @throws IOException when it goes boom.
 */
@Test
public void testGeneratedSecret() throws IOException {
    Path altPath = folder.newFolder().toPath().resolve(""alt.logstash.keystore"");
    SecureConfig altConfig = new SecureConfig();
    altConfig.add(""keystore.file"", altPath.toString().toCharArray());
    // note - no password given here.
    JavaKeyStore keyStore1 = new JavaKeyStore().create(altConfig.clone());
    JavaKeyStore keyStore2 = new JavaKeyStore().load(altConfig);
    String value = UUID.randomUUID().toString();
    SecretIdentifier id = new SecretIdentifier(value);
    // jvm1 persist, jvm2 read
    keyStore1.persistSecret(id, value.getBytes(StandardCharsets.UTF_8));
    assertThat(new String(keyStore2.retrieveSecret(new SecretIdentifier(value)), StandardCharsets.UTF_8)).isEqualTo(value);
}","/**
 * Simulates different JVMs can read using a default (non-provided) password
 *
 * @throws IOException when it goes boom.
 */
","// note - no password given here.
[[SEP]]// jvm1 persist, jvm2 read
","/** * Simulates different JVMs can read using a default (non-provided) password * * @throws IOException when it goes boom. */[[SEP]]// note - no password given here.[[SEP]]// jvm1 persist, jvm2 read",545,558,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,0,0,testGeneratedSecret(),org.logstash.secret.store.backend.JavaKeyStoreTest,testGeneratedSecret/0,False,546,4,9,0,9,1,16,11,0,6,0,16,0,0,0,0,0,0,2,0,6,0,0,0,0,0,36,1,0,True
729,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testLargeKeysAndValues(),"/**
 * Test upper sane bounds.
 */
@Test
public void testLargeKeysAndValues() {
    int keySize = 1000;
    int valueSize = 100000;
    StringBuilder keyBuilder = new StringBuilder(keySize);
    IntStream.range(0, keySize).forEach(i -> keyBuilder.append('k'));
    String key = keyBuilder.toString();
    StringBuilder valueBuilder = new StringBuilder(valueSize);
    IntStream.range(0, valueSize).forEach(i -> valueBuilder.append('v'));
    String value = valueBuilder.toString();
    SecretIdentifier id = new SecretIdentifier(key);
    keyStore.persistSecret(id, value.getBytes(StandardCharsets.UTF_8));
    byte[] secret = keyStore.retrieveSecret(id);
    assertThat(new String(secret, StandardCharsets.UTF_8)).isEqualTo(value);
    keyStore.purgeSecret(id);
}","/**
 * Test upper sane bounds.
 */
", ,/** * Test upper sane bounds. */,563,582,[0],0,[0],0,[0],0,0,0,0,testLargeKeysAndValues(),org.logstash.secret.store.backend.JavaKeyStoreTest,testLargeKeysAndValues/0,False,564,4,4,0,4,1,10,15,0,10,0,10,0,0,0,0,0,0,0,4,8,0,0,0,0,2,20,1,0,True
730,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testNonAscii(),"/**
 * Ensure that non-ascii keys and values are properly handled.
 *
 * @throws Exception when the clowns cry
 */
@Test
public void testNonAscii() throws Exception {
    int[] codepoints = { 0xD83E, 0xDD21, 0xD83E, 0xDD84 };
    String nonAscii = new String(codepoints, 0, codepoints.length);
    SecureConfig sc = new SecureConfig();
    sc.add(SecretStoreFactory.KEYSTORE_ACCESS_KEY, nonAscii.toCharArray());
    sc.add(""keystore.file"", (new String(keyStorePath) + "".nonAscii"").toCharArray());
    JavaKeyStore nonAsciiKeyStore = new JavaKeyStore().create(sc);
    SecretIdentifier id = new SecretIdentifier(nonAscii);
    nonAsciiKeyStore.persistSecret(id, nonAscii.getBytes(StandardCharsets.UTF_8));
    assertThat(new String(nonAsciiKeyStore.retrieveSecret(id), StandardCharsets.UTF_8)).isEqualTo(nonAscii);
}","/**
 * Ensure that non-ascii keys and values are properly handled.
 *
 * @throws Exception when the clowns cry
 */
", ,/** * Ensure that non-ascii keys and values are properly handled. * * @throws Exception when the clowns cry */,604,617,[0],0,[0],0,[0],0,0,0,0,testNonAscii(),org.logstash.secret.store.backend.JavaKeyStoreTest,testNonAscii/0,False,605,5,7,0,7,1,8,11,0,5,0,8,0,0,0,0,0,1,2,5,5,1,0,0,0,0,31,1,0,True
731,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testRestrictivePermissions(),"/**
 * Ensure the permissions can be set to be set more restrictive then default
 *
 * @throws Exception when it goes boom.
 */
@Test
public void testRestrictivePermissions() throws Exception {
    String beforeTest = JavaKeyStore.filePermissions;
    JavaKeyStore.filePermissions = ""rw-------"";
    try {
        Path altPath = folder.newFolder().toPath().resolve(""alt.logstash.keystore"");
        SecureConfig secureConfig = new SecureConfig();
        secureConfig.add(""keystore.file"", altPath.toString().toCharArray());
        keyStore = new JavaKeyStore().create(secureConfig);
        assertThat(altPath.toFile().exists()).isTrue();
        PosixFileAttributeView attrs = Files.getFileAttributeView(altPath, PosixFileAttributeView.class);
        boolean isWindows = System.getProperty(""os.name"").startsWith(""Windows"");
        // not all Windows FS are Posix
        if (!isWindows && attrs == null) {
            fail(""Can not determine POSIX file permissions for "" + keyStore + "" this is likely an error in the test"");
        }
        // if we got attributes, lets assert them.
        if (attrs != null) {
            Set<PosixFilePermission> permissions = attrs.readAttributes().permissions();
            EnumSet<PosixFilePermission> expected = EnumSet.of(OWNER_READ, OWNER_WRITE);
            assertThat(permissions.toArray()).containsExactlyInAnyOrder(expected.toArray());
        }
    } finally {
        JavaKeyStore.filePermissions = beforeTest;
    }
}","/**
 * Ensure the permissions can be set to be set more restrictive then default
 *
 * @throws Exception when it goes boom.
 */
","// not all Windows FS are Posix
[[SEP]]// if we got attributes, lets assert them.
","/** * Ensure the permissions can be set to be set more restrictive then default * * @throws Exception when it goes boom. */[[SEP]]// not all Windows FS are Posix[[SEP]]// if we got attributes, lets assert them.",624,651,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,0,0,testRestrictivePermissions(),org.logstash.secret.store.backend.JavaKeyStoreTest,testRestrictivePermissions/0,False,625,3,4,0,4,4,21,24,0,7,0,21,0,0,0,2,1,0,7,0,10,1,2,0,0,0,52,1,0,True
732,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void testWithRealSecondJvm(),"/**
 * Spins up a second JVM, writes all the data, then read it from this JVM
 *
 * @throws Exception when exceptions happen
 */
@Ignore(""This test timed out on Windows. Issue: https://github.com/elastic/logstash/issues/9916"")
@Test
public void testWithRealSecondJvm() throws Exception {
    Path magicFile = folder.newFolder().toPath().resolve(EXTERNAL_TEST_FILE_LOCK);
    Path altPath = folder.newFolder().toPath().resolve(""alt.logstash.keystore"");
    String java = System.getProperty(""java.home"") + File.separator + ""bin"" + File.separator + ""java"";
    ProcessBuilder builder = new ProcessBuilder(java, ""-cp"", System.getProperty(""java.class.path""), getClass().getCanonicalName(), EXTERNAL_TEST_WRITE, magicFile.toAbsolutePath().toString(), altPath.toAbsolutePath().toString());
    Future<Integer> future = Executors.newScheduledThreadPool(1).submit(() -> builder.start().waitFor());
    while (!future.isDone()) {
        try {
            Files.readAllBytes(magicFile);
        } catch (NoSuchFileException sfe) {
            Thread.sleep(100);
            continue;
        }
    }
    SecureConfig config = new SecureConfig();
    config.add(""keystore.file"", altPath.toAbsolutePath().toString().toCharArray());
    JavaKeyStore keyStore = new JavaKeyStore().load(config);
    validateAtoZ(keyStore);
}","/**
 * Spins up a second JVM, writes all the data, then read it from this JVM
 *
 * @throws Exception when exceptions happen
 */
", ,"/** * Spins up a second JVM, writes all the data, then read it from this JVM * * @throws Exception when exceptions happen */[[SEP]]//github.com/elastic/logstash/issues/9916"")",658,681,[0],0,[0],0,"[0, 0]",0,0,0,0,testWithRealSecondJvm(),org.logstash.secret.store.backend.JavaKeyStoreTest,testWithRealSecondJvm/0,False,660,5,5,0,5,3,19,20,0,7,0,19,1,1,1,0,1,0,8,2,7,1,2,0,0,1,65,1,0,True
733,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\secret\store\backend\JavaKeyStoreTest.java,org.logstash.secret.store.backend.JavaKeyStoreTest,void wrongPassword(),"/**
 * Ensure that the when the wrong password is presented the corrected exception is thrown.
 *
 * @throws Exception when ever it wants to
 */
@Test
public void wrongPassword() throws Exception {
    thrown.expect(SecretStoreException.AccessException.class);
    withDefinedPassConfig.add(SecretStoreFactory.KEYSTORE_ACCESS_KEY, ""wrongpassword"".toCharArray());
    new JavaKeyStore().load(withDefinedPassConfig);
}","/**
 * Ensure that the when the wrong password is presented the corrected exception is thrown.
 *
 * @throws Exception when ever it wants to
 */
", ,/** * Ensure that the when the wrong password is presented the corrected exception is thrown. * * @throws Exception when ever it wants to */,688,693,[0],0,[0],0,[0],0,0,0,0,wrongPassword(),org.logstash.secret.store.backend.JavaKeyStoreTest,wrongPassword/0,False,689,4,3,0,3,1,4,5,0,0,0,4,0,0,0,0,0,0,1,0,0,0,0,0,0,0,23,1,0,True
734,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\util\CloudSettingIdTest.java,org.logstash.util.CloudSettingIdTest,void testNullInputDoenstThrowAnException(),"// when given unacceptable input
@Test
public void testNullInputDoenstThrowAnException() {
    new CloudSettingId(null);
}","// when given unacceptable input
", ,// when given unacceptable input,45,48,[0],0,[0],0,[0],0,0,0,0,testNullInputDoenstThrowAnException(),org.logstash.util.CloudSettingIdTest,testNullInputDoenstThrowAnException/0,False,46,2,0,0,0,1,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,1,0,False
735,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\util\CloudSettingIdTest.java,org.logstash.util.CloudSettingIdTest,void testDecodingWithoutLabelSegment(),"// without a label
@Test
public void testDecodingWithoutLabelSegment() {
    sut = new CloudSettingId(""dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy"");
    assertEquals(""#label is empty"", """", sut.getLabel());
    assertEquals(""#decode is set"", ""us-east-1.aws.found.io$notareal$identifier"", sut.getDecoded());
}","// without a label
", ,// without a label,103,109,[0],0,[0],0,[0],0,0,0,0,testDecodingWithoutLabelSegment(),org.logstash.util.CloudSettingIdTest,testDecodingWithoutLabelSegment/0,False,104,2,0,0,0,1,3,5,0,0,0,3,0,0,0,0,0,0,5,0,1,0,0,0,0,0,13,1,0,False
736,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\util\CloudSettingIdTest.java,org.logstash.util.CloudSettingIdTest,void testAccessorsWithAcceptableInput(),"// when given acceptable input, the accessors:
@Test
public void testAccessorsWithAcceptableInput() {
    assertEquals(""#original has a value"", input, sut.getOriginal());
    assertEquals(""#decoded has a value"", ""us-east-1.aws.found.io$notareal$identifier"", sut.getDecoded());
    assertEquals(""#label has a value"", ""foobar"", sut.getLabel());
    assertEquals(""#elasticsearch_host has a value"", ""notareal.us-east-1.aws.found.io:443"", sut.getElasticsearchHost());
    assertEquals(""#elasticsearch_scheme has a value"", ""https"", sut.getElasticsearchScheme());
    assertEquals(""#kibana_host has a value"", ""identifier.us-east-1.aws.found.io:443"", sut.getKibanaHost());
    assertEquals(""#kibana_scheme has a value"", ""https"", sut.getKibanaScheme());
    assertEquals(""#to_s has a value of #decoded"", sut.toString(), sut.getDecoded());
}","// when given acceptable input, the accessors:
", ,"// when given acceptable input, the accessors:",112,122,[0],0,[0],0,[0],0,0,0,0,testAccessorsWithAcceptableInput(),org.logstash.util.CloudSettingIdTest,testAccessorsWithAcceptableInput/0,False,113,1,0,0,0,1,9,10,0,0,0,9,0,0,0,0,0,0,14,0,0,0,0,0,0,0,12,1,0,False
737,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\util\CloudSettingIdTest.java,org.logstash.util.CloudSettingIdTest,void testGivenAcceptableInputEmptyKibanaUUID(),"// when given acceptable input (with empty kibana uuid), the accessors:
@Test
public void testGivenAcceptableInputEmptyKibanaUUID() {
    input = ""a-test:ZWNlLmhvbWUubGFuJHRlc3Qk"";
    // ece.home.lan$test$
    sut = new CloudSettingId(input);
    assertEquals(""#original has a value"", input, sut.getOriginal());
    assertEquals(""#decoded has a value"", ""ece.home.lan$test$"", sut.getDecoded());
    assertEquals(""#label has a value"", ""a-test"", sut.getLabel());
    assertEquals(""#elasticsearch_host has a value"", ""test.ece.home.lan:443"", sut.getElasticsearchHost());
    assertEquals(""#elasticsearch_scheme has a value"", ""https"", sut.getElasticsearchScheme());
    // NOTE: kibana part is not relevant -> this is how python/beats(go) code behaves
    assertEquals(""#kibana_host has a value"", "".ece.home.lan:443"", sut.getKibanaHost());
    assertEquals(""#kibana_scheme has a value"", ""https"", sut.getKibanaScheme());
    assertEquals(""#toString has a value of #decoded"", sut.getDecoded(), sut.toString());
}","// when given acceptable input (with empty kibana uuid), the accessors:
","// ece.home.lan$test$
[[SEP]]// NOTE: kibana part is not relevant -> this is how python/beats(go) code behaves
","// when given acceptable input (with empty kibana uuid), the accessors:[[SEP]]// ece.home.lan$test$[[SEP]]// NOTE: kibana part is not relevant -> this is how python/beats(go) code behaves",166,180,[0],0,"[0, 0]",0,"[0, 0, 0]",0,0,1,0,testGivenAcceptableInputEmptyKibanaUUID(),org.logstash.util.CloudSettingIdTest,testGivenAcceptableInputEmptyKibanaUUID/0,False,167,2,0,0,0,1,9,12,0,0,0,9,0,0,0,0,0,0,15,0,2,0,0,0,0,0,20,1,0,False
738,..\projects\logstash-8.5.2\logstash-core\src\test\java\org\logstash\util\CloudSettingIdTest.java,org.logstash.util.CloudSettingIdTest,void testWithRealWorldInput(),"// a lengthy real-world input, the accessors:
@Test
public void testWithRealWorldInput() {
    // eastus2.azure.elastic-cloud.com:9243$40b343116cfa4ebcb76c11ee2329f92d$43d09252502c4189a376fd0cf2cd0848
    input = ""ZWFzdHVzMi5henVyZS5lbGFzdGljLWNsb3VkLmNvbTo5MjQzJDQwYjM0MzExNmNmYTRlYmNiNzZjMTFlZTIzMjlmOTJkJDQzZDA5MjUyNTAyYzQxODlhMzc2ZmQwY2YyY2QwODQ4"";
    sut = new CloudSettingId(input);
    assertEquals(""#original has a value"", input, sut.getOriginal());
    assertEquals(""#decoded has a value"", ""eastus2.azure.elastic-cloud.com:9243$40b343116cfa4ebcb76c11ee2329f92d$43d09252502c4189a376fd0cf2cd0848"", sut.getDecoded());
    assertEquals(""#label has a value"", """", sut.getLabel());
    assertEquals(""#elasticsearch_host has a value"", ""40b343116cfa4ebcb76c11ee2329f92d.eastus2.azure.elastic-cloud.com:9243"", sut.getElasticsearchHost());
    assertEquals(""#kibana_host has a value"", ""43d09252502c4189a376fd0cf2cd0848.eastus2.azure.elastic-cloud.com:9243"", sut.getKibanaHost());
    assertEquals(""#toString has a value of #decoded"", sut.getDecoded(), sut.toString());
}","// a lengthy real-world input, the accessors:
","// eastus2.azure.elastic-cloud.com:9243$40b343116cfa4ebcb76c11ee2329f92d$43d09252502c4189a376fd0cf2cd0848
","// a lengthy real-world input, the accessors:[[SEP]]// eastus2.azure.elastic-cloud.com:9243$40b343116cfa4ebcb76c11ee2329f92d$43d09252502c4189a376fd0cf2cd0848",183,195,[0],0,[0],0,"[0, 0]",0,0,0,0,testWithRealWorldInput(),org.logstash.util.CloudSettingIdTest,testWithRealWorldInput/0,False,184,2,0,0,0,1,7,10,0,0,0,7,0,0,0,0,0,0,11,0,2,0,0,0,0,0,16,1,0,False
739,..\projects\logstash-8.5.2\qa\integration\src\test\java\org\logstash\integration\RSpecTests.java,org.logstash.integration.RSpecTests,Ruby initializeGlobalRuntime(Path),"private static Ruby initializeGlobalRuntime(final Path root) {
    String[] args = new String[] { ""--disable-did_you_mean"" };
    Ruby runtime = Ruby.newInstance(Logstash.initRubyConfig(root, null, /* qa/integration */
    args));
    if (runtime != RubyUtil.RUBY)
        throw new AssertionError(""runtime already initialized"");
    return runtime;
}", ,"/* qa/integration */
",/* qa/integration */,59,64,[0],0,[0],0,[0],0,0,0,0,initializeGlobalRuntime(Path),org.logstash.integration.RSpecTests,initializeGlobalRuntime/1[java.nio.file.Path],False,59,1,1,1,0,2,2,6,1,2,1,2,0,0,0,1,0,0,2,0,2,0,1,0,0,0,12,10,0,False
740,..\projects\logstash-8.5.2\qa\integration\src\test\java\org\logstash\integration\RSpecTests.java,org.logstash.integration.RSpecTests,void rspecTests(),"@Test
public void rspecTests() throws Exception {
    RubyUtil.RUBY.getGlobalVariables().set(""$JUNIT_ARGV"", Rubyfier.deep(RubyUtil.RUBY, Arrays.asList(""-fd"", ""--pattern"", System.getProperty(""org.logstash.integration.specs"", ""specs/**/*_spec.rb""))));
    // qa/integration/rspec.rb
    final Path rspec = Paths.get(""rspec.rb"");
    final IRubyObject result = RubyUtil.RUBY.executeScript(new String(Files.readAllBytes(rspec), StandardCharsets.UTF_8), rspec.toFile().getAbsolutePath());
    if (!result.toJava(Long.class).equals(0L)) {
        Assert.fail(""RSpec test suit saw at least one failure."");
    }
}", ,"// qa/integration/rspec.rb
",/**/[[SEP]]// qa/integration/rspec.rb,66,81,[0],0,[0],0,"[0, 0]",0,0,0,0,rspecTests(),org.logstash.integration.RSpecTests,rspecTests/0,False,67,3,1,0,1,2,13,8,0,2,0,13,0,0,0,0,0,0,7,1,2,0,1,0,0,0,16,1,0,False
741,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\DataStore.java,org.logstash.benchmark.cli.DataStore,"void store(Map<LsMetricStats, ListStatistics>)","/**
 * @param data Measured Data
 * @throws IOException On Failure
 */
void store(Map<LsMetricStats, ListStatistics> data) throws IOException;","/**
 * @param data Measured Data
 * @throws IOException On Failure
 */
", ,/** * @param data Measured Data * @throws IOException On Failure */,55,55,[0],0,[0],0,[0],0,0,0,0,"store(Map<LsMetricStats, ListStatistics>)",org.logstash.benchmark.cli.DataStore,"store/1[java.util.Map<org.logstash.benchmark.cli.ui.LsMetricStats,org.logstash.benchmark.cli.ListStatistics>]",False,51,2,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,0,0,True
742,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\JRubyInstallation.java,org.logstash.benchmark.cli.JRubyInstallation,JRubyInstallation bootstrapJruby(Path),"/**
 * Sets up a JRuby used to bootstrap a Logstash Installation.
 * @param pwd Cache Directory to work in
 * @return instance
 * @throws IOException On I/O Error during JRuby Download or Installation
 * @throws NoSuchAlgorithmException On SSL Issue during JRuby Download
 */
public static JRubyInstallation bootstrapJruby(final Path pwd) throws IOException, NoSuchAlgorithmException {
    LsBenchDownloader.downloadDecompress(pwd.resolve(""jruby"").toFile(), String.format(""http://jruby.org.s3.amazonaws.com/downloads/%s/jruby-bin-%s.tar.gz"", JRUBY_DEFAULT_VERSION, JRUBY_DEFAULT_VERSION));
    return new JRubyInstallation(pwd.resolve(""jruby"").resolve(String.format(""jruby-%s"", JRUBY_DEFAULT_VERSION)));
}","/**
 * Sets up a JRuby used to bootstrap a Logstash Installation.
 * @param pwd Cache Directory to work in
 * @return instance
 * @throws IOException On I/O Error during JRuby Download or Installation
 * @throws NoSuchAlgorithmException On SSL Issue during JRuby Download
 */
", ,"/** * Sets up a JRuby used to bootstrap a Logstash Installation. * @param pwd Cache Directory to work in * @return instance * @throws IOException On I/O Error during JRuby Download or Installation * @throws NoSuchAlgorithmException On SSL Issue during JRuby Download */[[SEP]]//jruby.org.s3.amazonaws.com/downloads/%s/jruby-bin-%s.tar.gz"", JRUBY_DEFAULT_VERSION, JRUBY_DEFAULT_VERSION));",57,69,[0],0,[0],0,"[0, 0]",0,0,0,0,bootstrapJruby(Path),org.logstash.benchmark.cli.JRubyInstallation,bootstrapJruby/1[java.nio.file.Path],False,58,2,3,1,2,1,4,4,1,0,1,4,0,0,0,0,0,0,4,0,0,0,0,0,0,0,35,9,0,True
743,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\LogstashInstallation.java,org.logstash.benchmark.cli.LogstashInstallation,void execute(String),"/**
 * Runs the Logstash Installation with the given configuration.
 * @param configuration Configuration as String
 * @throws IOException On I/O Exception
 * @throws InterruptedException Iff Interrupted
 */
void execute(String configuration) throws IOException, InterruptedException;","/**
 * Runs the Logstash Installation with the given configuration.
 * @param configuration Configuration as String
 * @throws IOException On I/O Exception
 * @throws InterruptedException Iff Interrupted
 */
", ,/** * Runs the Logstash Installation with the given configuration. * @param configuration Configuration as String * @throws IOException On I/O Exception * @throws InterruptedException Iff Interrupted */,49,49,[0],0,[0],0,[0],0,0,0,0,execute(String),org.logstash.benchmark.cli.LogstashInstallation,execute/1[java.lang.String],False,43,0,4,4,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,0,0,True
744,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\LogstashInstallation.java,org.logstash.benchmark.cli.LogstashInstallation,"void execute(String, File, int)","/**
 * Runs the Logstash Installation with the given configuration and given File piped to
 * standard input.
 * @param configuration Configuration as String
 * @param data Data file piped to standard input
 * @throws IOException On I/O Exception
 * @throws InterruptedException Iff Interrupted
 */
void execute(String configuration, File data, int repeat) throws IOException, InterruptedException;","/**
 * Runs the Logstash Installation with the given configuration and given File piped to
 * standard input.
 * @param configuration Configuration as String
 * @param data Data file piped to standard input
 * @throws IOException On I/O Exception
 * @throws InterruptedException Iff Interrupted
 */
", ,/** * Runs the Logstash Installation with the given configuration and given File piped to * standard input. * @param configuration Configuration as String * @param data Data file piped to standard input * @throws IOException On I/O Exception * @throws InterruptedException Iff Interrupted */,59,59,[0],0,[0],0,[0],0,0,0,0,"execute(String, File, int)",org.logstash.benchmark.cli.LogstashInstallation,"execute/3[java.lang.String,java.io.File,int]",False,51,0,4,4,0,1,0,1,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,0,0,True
745,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\LogstashInstallation.java,org.logstash.benchmark.cli.LogstashInstallation,String metrics(),"/**
 * Returns the url under which the metrics from uri `_node/stats/?pretty` can be found.
 * @return Metrics URL
 */
String metrics();","/**
 * Returns the url under which the metrics from uri `_node/stats/?pretty` can be found.
 * @return Metrics URL
 */
", ,/** * Returns the url under which the metrics from uri `_node/stats/?pretty` can be found. * @return Metrics URL */,65,65,[0],0,[0],0,[0],0,0,0,0,metrics(),org.logstash.benchmark.cli.LogstashInstallation,metrics/0,False,61,0,5,5,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,0,0,True
746,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\LogstashInstallation.java,org.logstash.benchmark.cli.LogstashInstallation,void configure(BenchmarkMeta),"/**
 * Temporarily (will be reverted after the benchmark execution finishes) overwrites the existing
 * logstash.yml configuration in this installation.
 * @param meta Benchmark settings
 */
void configure(BenchmarkMeta meta);","/**
 * Temporarily (will be reverted after the benchmark execution finishes) overwrites the existing
 * logstash.yml configuration in this installation.
 * @param meta Benchmark settings
 */
", ,/** * Temporarily (will be reverted after the benchmark execution finishes) overwrites the existing * logstash.yml configuration in this installation. * @param meta Benchmark settings */,72,72,[0],0,[0],0,[0],0,0,0,0,configure(BenchmarkMeta),org.logstash.benchmark.cli.LogstashInstallation,configure/1[org.logstash.benchmark.cli.BenchmarkMeta],False,67,1,5,5,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,18,0,0,True
747,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\LogstashInstallation.java,org.logstash.benchmark.cli.LogstashInstallation.FromRelease,"void download(File, String)","private static void download(final File pwd, final String version) throws IOException, NoSuchAlgorithmException {
    final String arch = retrieveCPUArchitecture();
    final String os = retrieveOS();
    final String extension = ""windows"".equals(os) ? ""zip"" : ""tar.gz"";
    final String downloadUrl;
    if (compareVersions(version, ""7.10.0"") >= 0) {
        // version >= 7.10.0 url format
        downloadUrl = String.format(""https://artifacts.elastic.co/downloads/logstash/logstash-%s-%s-%s.%s"", version, os, arch, extension);
    } else {
        // version < 7.10.0 url format
        downloadUrl = String.format(""https://artifacts.elastic.co/downloads/logstash/logstash-%s.%s"", version, extension);
    }
    LsBenchDownloader.downloadDecompress(pwd, downloadUrl);
}", ,"// version >= 7.10.0 url format
[[SEP]]// version < 7.10.0 url format
","// version >= 7.10.0 url format[[SEP]]//artifacts.elastic.co/downloads/logstash/logstash-%s-%s-%s.%s"", version, os, arch, extension);[[SEP]]// version < 7.10.0 url format[[SEP]]//artifacts.elastic.co/downloads/logstash/logstash-%s.%s"", version, extension);",115,136,[0],0,"[0, 0]",0,"[0, 0, 0, 0]",0,0,0,0,"download(File, String)",org.logstash.benchmark.cli.LogstashInstallation$FromRelease,"download/2[java.io.File,java.lang.String]",False,116,2,5,1,4,3,6,13,0,4,2,6,3,1,0,0,0,0,6,1,5,0,1,0,0,0,22,10,0,False
748,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\LogstashInstallation.java,org.logstash.benchmark.cli.LogstashInstallation.FromLocalPath,"void pipeRepeatedly(File, OutputStream, int)","/**
 * Pipes the content of the given input {@link File} to the given {@link OutputStream}
 * repeatedly.
 * @param input Input File
 * @param out Output Stream
 * @param count Number of repeats
 * @throws IOException On Failure
 */
private static void pipeRepeatedly(final File input, final OutputStream out, final int count) throws IOException {
    for (int i = 0; i < count; ++i) {
        try (final InputStream file = new FileInputStream(input)) {
            IOUtils.copy(file, out, 16 * 4096);
        }
    }
}","/**
 * Pipes the content of the given input {@link File} to the given {@link OutputStream}
 * repeatedly.
 * @param input Input File
 * @param out Output Stream
 * @param count Number of repeats
 * @throws IOException On Failure
 */
", ,/** * Pipes the content of the given input {@link File} to the given {@link OutputStream} * repeatedly. * @param input Input File * @param out Output Stream * @param count Number of repeats * @throws IOException On Failure */,289,296,[0],0,[0],0,[0],0,0,0,0,"pipeRepeatedly(File, OutputStream, int)",org.logstash.benchmark.cli.LogstashInstallation$FromLocalPath,"pipeRepeatedly/3[java.io.File,java.io.OutputStream,int]",False,290,0,1,1,0,2,1,7,0,2,3,1,0,0,1,0,1,0,0,3,2,1,2,0,0,0,24,10,0,True
749,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\LsMetricsMonitor.java,org.logstash.benchmark.cli.LsMetricsMonitor.MonitorExecution,"EnumMap<LsMetricStats, ListStatistics> stopAndGet()","/**
 * Stops metric collection and returns the collected results.
 * @return Statistical results from metric collection
 * @throws InterruptedException On Failure
 * @throws ExecutionException On Failure
 * @throws TimeoutException On Failure
 */
public EnumMap<LsMetricStats, ListStatistics> stopAndGet() throws InterruptedException, ExecutionException, TimeoutException {
    monitor.stop();
    return future.get(20L, TimeUnit.SECONDS);
}","/**
 * Stops metric collection and returns the collected results.
 * @return Statistical results from metric collection
 * @throws InterruptedException On Failure
 * @throws ExecutionException On Failure
 * @throws TimeoutException On Failure
 */
", ,/** * Stops metric collection and returns the collected results. * @return Statistical results from metric collection * @throws InterruptedException On Failure * @throws ExecutionException On Failure * @throws TimeoutException On Failure */,181,185,[0],0,[0],0,[0],0,0,0,0,stopAndGet(),org.logstash.benchmark.cli.LsMetricsMonitor$MonitorExecution,stopAndGet/0,False,182,3,6,5,1,1,2,4,1,0,0,2,0,0,0,0,0,0,0,1,0,0,0,0,0,0,19,1,0,True
750,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\Main.java,org.logstash.benchmark.cli.Main,void main(String...),"/**
 * CLI Entrypoint.
 * @param args Cli Args
 */
public static void main(final String... args) throws IOException, NoSuchAlgorithmException {
    final OptionParser parser = new OptionParser();
    final OptionSpecBuilder gitbuilder = parser.accepts(UserInput.GIT_VERSION_PARAM, UserInput.GIT_VERSION_HELP);
    final OptionSpecBuilder localbuilder = parser.accepts(UserInput.LOCAL_VERSION_PARAM, UserInput.LOCAL_VERSION_HELP);
    final OptionSpecBuilder distributionbuilder = parser.accepts(UserInput.DISTRIBUTION_VERSION_PARAM, UserInput.DISTRIBUTION_VERSION_HELP);
    final OptionSpec<String> git = gitbuilder.requiredUnless(UserInput.DISTRIBUTION_VERSION_PARAM, UserInput.LOCAL_VERSION_PARAM).availableUnless(UserInput.DISTRIBUTION_VERSION_PARAM, UserInput.LOCAL_VERSION_PARAM).withRequiredArg().ofType(String.class).forHelp();
    final OptionSpec<String> distribution = distributionbuilder.requiredUnless(UserInput.LOCAL_VERSION_PARAM, UserInput.GIT_VERSION_PARAM).availableUnless(UserInput.LOCAL_VERSION_PARAM, UserInput.GIT_VERSION_PARAM).withRequiredArg().ofType(String.class).forHelp();
    final OptionSpec<String> local = localbuilder.requiredUnless(UserInput.DISTRIBUTION_VERSION_PARAM, UserInput.GIT_VERSION_PARAM).availableUnless(UserInput.DISTRIBUTION_VERSION_PARAM, UserInput.GIT_VERSION_PARAM).withRequiredArg().ofType(String.class).forHelp();
    final OptionSpec<String> testcase = parser.accepts(UserInput.TEST_CASE_PARAM, UserInput.TEST_CASE_HELP).withRequiredArg().ofType(String.class).defaultsTo(GeneratorToStdout.IDENTIFIER).forHelp();
    final OptionSpec<File> testcaseconfig = parser.accepts(UserInput.TEST_CASE_CONFIG_PARAM, UserInput.TEST_CASE_CONFIG_HELP).withRequiredArg().ofType(File.class).forHelp();
    final OptionSpec<File> testcasedata = parser.accepts(UserInput.TEST_CASE_DATA_PARAM, UserInput.TEST_CASE_DATA_HELP).withRequiredArg().ofType(File.class).forHelp();
    final OptionSpec<File> pwd = parser.accepts(UserInput.WORKING_DIRECTORY_PARAM, UserInput.WORKING_DIRECTORY_HELP).withRequiredArg().ofType(File.class).defaultsTo(UserInput.WORKING_DIRECTORY_DEFAULT).forHelp();
    final OptionSpec<String> esout = parser.accepts(UserInput.ES_OUTPUT_PARAM, UserInput.ES_OUTPUT_HELP).withRequiredArg().ofType(String.class).defaultsTo(UserInput.ES_OUTPUT_DEFAULT).forHelp();
    final OptionSpec<Integer> repeats = parser.accepts(UserInput.REPEAT_PARAM, UserInput.REPEAT_PARAM_HELP).withRequiredArg().ofType(Integer.class).defaultsTo(1).forHelp();
    final OptionSpec<Integer> workers = parser.accepts(UserInput.LS_WORKER_THREADS, UserInput.LS_WORKER_THREADS_HELP).withRequiredArg().ofType(Integer.class).defaultsTo(UserInput.LS_WORKER_THREADS_DEFAULT).forHelp();
    final OptionSpec<Integer> batchsize = parser.accepts(UserInput.LS_BATCH_SIZE, UserInput.LS_BATCH_SIZE_HELP).withRequiredArg().ofType(Integer.class).defaultsTo(UserInput.LS_BATCHSIZE_DEFAULT).forHelp();
    final OptionSet options;
    try {
        options = parser.parse(args);
    } catch (final OptionException ex) {
        parser.printHelpOn(System.out);
        throw ex;
    }
    final LsVersionType type;
    final String version;
    if (options.has(distribution)) {
        type = LsVersionType.DISTRIBUTION;
        version = options.valueOf(distribution);
    } else if (options.has(git)) {
        type = LsVersionType.GIT;
        version = options.valueOf(git);
    } else {
        type = LsVersionType.LOCAL;
        version = options.valueOf(local);
    }
    final Properties settings = loadSettings();
    settings.setProperty(LsBenchSettings.INPUT_DATA_REPEAT, String.valueOf(options.valueOf(repeats)));
    Path testCaseConfigPath = null;
    Path testCaseDataPath = null;
    if (options.valueOf(testcase).equals(""custom"")) {
        if (options.has(testcaseconfig)) {
            testCaseConfigPath = options.valueOf(testcaseconfig).toPath();
        } else {
            throw new IllegalArgumentException(""Path to Test Case Config must be provided"");
        }
        if (options.has(testcasedata)) {
            testCaseDataPath = options.valueOf(testcasedata).toPath();
        }
    } else {
        if (options.has(testcaseconfig) || options.has(testcasedata)) {
            throw new IllegalArgumentException(""Path to Test Case Config or Data can only be used with Custom Test Case"");
        }
    }
    final BenchmarkMeta runConfig = new BenchmarkMeta(options.valueOf(testcase), testCaseConfigPath, testCaseDataPath, version, type, options.valueOf(workers), options.valueOf(batchsize));
    execute(new UserOutput(System.out), settings, options.valueOf(pwd).toPath(), runConfig, options.valueOf(esout));
}","/**
 * CLI Entrypoint.
 * @param args Cli Args
 */
", ,/** * CLI Entrypoint. * @param args Cli Args */,65,165,[0],0,[0],0,[0],0,0,0,0,main(String[]),org.logstash.benchmark.cli.Main,main/1[java.lang.String[]],False,65,9,12,8,4,9,17,61,0,22,1,17,2,2,0,0,1,0,3,1,28,0,2,0,0,0,66,9,0,True
751,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\Main.java,org.logstash.benchmark.cli.Main,"void execute(UserOutput, Properties, Path, BenchmarkMeta, String)","/**
 * Programmatic Entrypoint.
 * @param output Output Printer
 * @param settings Properties
 * @param cwd Working Directory to run in and write cache files to
 * @param runConfig Logstash Settings
 * @param esout Elastic Search URL (empty string is interpreted as not using ES output)
 * @throws IOException On Failure
 * @throws NoSuchAlgorithmException On Failure
 */
public static void execute(final UserOutput output, final Properties settings, final Path cwd, final BenchmarkMeta runConfig, final String esout) throws IOException, NoSuchAlgorithmException {
    output.printBanner();
    output.printLine();
    final String version = runConfig.getVersion();
    output.green(String.format(""Benchmarking Version: %s"", version));
    output.green(String.format(""Logstash Parameters: -w %d -b %d"", runConfig.getWorkers(), runConfig.getBatchsize()));
    final String test = runConfig.getTestcase();
    output.green(String.format(""Running Test Case: %s (x%d)"", test, Integer.parseInt(settings.getProperty(LsBenchSettings.INPUT_DATA_REPEAT))));
    if (runConfig.getTestcase().equals(""custom"")) {
        output.green(String.format(""Test Case Config: %s"", runConfig.getConfigPath()));
    }
    output.printLine();
    Files.createDirectories(cwd);
    final LogstashInstallation logstash;
    final LsVersionType type = runConfig.getVtype();
    if (type == LsVersionType.GIT) {
        logstash = LsBenchLsSetup.logstashFromGit(cwd.toAbsolutePath().toString(), version, JRubyInstallation.bootstrapJruby(cwd));
    } else {
        logstash = LsBenchLsSetup.setupLS(cwd.toAbsolutePath().toString(), version, type, output);
    }
    try (final DataStore store = setupDataStore(esout, runConfig)) {
        final Case testcase = setupTestCase(store, logstash, cwd, settings, runConfig, output);
        output.printStartTime();
        final long start = System.currentTimeMillis();
        final AbstractMap<LsMetricStats, ListStatistics> stats = testcase.run();
        output.green(""Statistical Summary:\n"");
        output.green(String.format(""Elapsed Time: %ds"", TimeUnit.SECONDS.convert(System.currentTimeMillis() - start, TimeUnit.MILLISECONDS)));
        output.printStatistics(stats);
    }
}","/**
 * Programmatic Entrypoint.
 * @param output Output Printer
 * @param settings Properties
 * @param cwd Working Directory to run in and write cache files to
 * @param runConfig Logstash Settings
 * @param esout Elastic Search URL (empty string is interpreted as not using ES output)
 * @throws IOException On Failure
 * @throws NoSuchAlgorithmException On Failure
 */
", ,/** * Programmatic Entrypoint. * @param output Output Printer * @param settings Properties * @param cwd Working Directory to run in and write cache files to * @param runConfig Logstash Settings * @param esout Elastic Search URL (empty string is interpreted as not using ES output) * @throws IOException On Failure * @throws NoSuchAlgorithmException On Failure */,177,229,[0],0,[0],0,[0],0,0,0,1,"execute(UserOutput, Properties, Path, BenchmarkMeta, String)",org.logstash.benchmark.cli.Main,"execute/5[org.logstash.benchmark.cli.ui.UserOutput,java.util.Properties,java.nio.file.Path,org.logstash.benchmark.cli.BenchmarkMeta,java.lang.String]",False,179,10,18,1,17,3,26,31,0,8,5,26,2,1,0,1,1,0,7,0,9,1,1,0,0,0,66,9,0,True
752,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\cases\Case.java,org.logstash.benchmark.cli.cases.Case,"AbstractMap<LsMetricStats, ListStatistics> run()","/**
 * Runs the Actual Test Case.
 * @return Map Containing Test Results
 */
AbstractMap<LsMetricStats, ListStatistics> run();","/**
 * Runs the Actual Test Case.
 * @return Map Containing Test Results
 */
", ,/** * Runs the Actual Test Case. * @return Map Containing Test Results */,36,36,[0],0,[0],0,[0],0,0,0,0,run(),org.logstash.benchmark.cli.cases.Case,run/0,False,32,2,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,0,0,True
753,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\util\LsBenchJsonUtil.java,org.logstash.benchmark.cli.util.LsBenchJsonUtil,"Map<String, Object> deserializeMetrics(byte[])","/**
 * Deserializes metrics read from LS HTTP Api.
 * @param data raw bytes read from HTTP API
 * @return Deserialized JSON Map of Metrics
 * @throws IOException On Deserialization Failure
 */
public static Map<String, Object> deserializeMetrics(final byte[] data) throws IOException {
    return LsBenchJsonUtil.OBJECT_MAPPER.readValue(data, LsBenchJsonUtil.LS_METRIC_TYPE);
}","/**
 * Deserializes metrics read from LS HTTP Api.
 * @param data raw bytes read from HTTP API
 * @return Deserialized JSON Map of Metrics
 * @throws IOException On Deserialization Failure
 */
", ,/** * Deserializes metrics read from LS HTTP Api. * @param data raw bytes read from HTTP API * @return Deserialized JSON Map of Metrics * @throws IOException On Deserialization Failure */,52,54,[0],0,[0],0,[0],0,0,0,0,deserializeMetrics(byte[]),org.logstash.benchmark.cli.util.LsBenchJsonUtil,deserializeMetrics/1[byte[]],False,52,1,1,1,0,1,1,3,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,9,0,True
754,..\projects\logstash-8.5.2\tools\benchmark-cli\src\main\java\org\logstash\benchmark\cli\util\LsBenchJsonUtil.java,org.logstash.benchmark.cli.util.LsBenchJsonUtil,"String serializeEsResult(Map<LsMetricStats, ListStatistics>, Map<String, Object>)","/**
 * Serializes result for storage in Elasticsearch.
 * @param data Measurement Data
 * @param meta Metadata
 * @return JSON String
 * @throws JsonProcessingException On Failure to Serialize
 */
public static String serializeEsResult(final Map<LsMetricStats, ListStatistics> data, final Map<String, Object> meta) throws JsonProcessingException {
    final Map<String, Object> measurement = new HashMap<>(4);
    measurement.put(""@timestamp"", System.currentTimeMillis());
    final ListStatistics throughput = data.get(LsMetricStats.THROUGHPUT);
    measurement.put(""throughput_min"", throughput.getMin());
    measurement.put(""throughput_max"", throughput.getMax());
    measurement.put(""throughput_mean"", Math.round(throughput.getMean()));
    measurement.put(""cpu_usage_mean_percent"", Math.round(data.get(LsMetricStats.CPU_USAGE).getMean()));
    measurement.put(""meta"", meta);
    return OBJECT_MAPPER.writeValueAsString(measurement);
}","/**
 * Serializes result for storage in Elasticsearch.
 * @param data Measurement Data
 * @param meta Metadata
 * @return JSON String
 * @throws JsonProcessingException On Failure to Serialize
 */
", ,/** * Serializes result for storage in Elasticsearch. * @param data Measurement Data * @param meta Metadata * @return JSON String * @throws JsonProcessingException On Failure to Serialize */,63,76,[0],0,[0],0,[0],0,0,0,0,"serializeEsResult(Map<LsMetricStats, ListStatistics>, Map<String, Object>)",org.logstash.benchmark.cli.util.LsBenchJsonUtil,"serializeEsResult/2[java.util.Map<org.logstash.benchmark.cli.ui.LsMetricStats,org.logstash.benchmark.cli.util.ListStatistics>,java.util.Map<java.lang.String,java.lang.Object>]",False,64,2,1,1,0,1,8,11,1,2,2,8,0,0,0,0,0,0,6,1,2,0,0,0,0,0,27,9,0,True
755,..\projects\logstash-8.5.2\tools\benchmark-cli\src\test\java\org\logstash\benchmark\cli\MainEsStorageTest.java,org.logstash.benchmark.cli.MainEsStorageTest,void runsAgainstRelease(),"/**
 * @throws Exception On Failure
 */
@Test
public void runsAgainstRelease() throws Exception {
    final File pwd = temp.newFolder();
    Main.main(String.format(""--%s=5.5.0"", UserInput.DISTRIBUTION_VERSION_PARAM), String.format(""--workdir=%s"", pwd.getAbsolutePath()), String.format(""--%s=%s"", UserInput.ES_OUTPUT_PARAM, ""http://127.0.0.1:9200/""));
}","/**
 * @throws Exception On Failure
 */
", ,"/** * @throws Exception On Failure */[[SEP]]//127.0.0.1:9200/""));",40,48,[0],0,[0],0,"[0, 0]",0,0,0,0,runsAgainstRelease(),org.logstash.benchmark.cli.MainEsStorageTest,runsAgainstRelease/0,False,41,2,1,0,1,1,4,4,0,1,0,4,0,0,0,0,0,0,4,0,1,0,0,0,0,0,9,1,0,True
756,..\projects\logstash-8.5.2\tools\benchmark-cli\src\test\java\org\logstash\benchmark\cli\MainTest.java,org.logstash.benchmark.cli.MainTest,void runsAgainstLocal(),"/**
 * @throws Exception On Failure
 * @todo cleanup path here, works though if you plug in a correct path
 */
@Test
public void runsAgainstLocal() throws Exception {
    final File pwd = temp.newFolder();
    Main.main(String.format(""--version=local:%s"", System.getProperty(""logstash.benchmark.test.local.path"")), String.format(""--workdir=%s"", pwd.getAbsolutePath()));
}","/**
 * @throws Exception On Failure
 * @todo cleanup path here, works though if you plug in a correct path
 */
", ,"/** * @throws Exception On Failure * @todo cleanup path here, works though if you plug in a correct path */",60,67,[1],1,[0],0,[1],1,1,1,1,runsAgainstLocal(),org.logstash.benchmark.cli.MainTest,runsAgainstLocal/0,False,61,2,1,0,1,1,5,4,0,1,0,5,0,0,0,0,0,0,3,0,1,0,0,0,0,0,19,1,0,True
757,..\projects\logstash-8.5.2\tools\benchmark-cli\src\test\java\org\logstash\benchmark\cli\MainTest.java,org.logstash.benchmark.cli.MainTest,void runsAgainstRelease(),"/**
 * @throws Exception On Failure
 */
@Test
public void runsAgainstRelease() throws Exception {
    final File pwd = temp.newFolder();
    Main.main(String.format(""--%s=5.5.0"", UserInput.DISTRIBUTION_VERSION_PARAM), String.format(""--workdir=%s"", pwd.getAbsolutePath()));
}","/**
 * @throws Exception On Failure
 */
", ,/** * @throws Exception On Failure */,72,79,[0],0,[0],0,[0],0,0,0,0,runsAgainstRelease(),org.logstash.benchmark.cli.MainTest,runsAgainstRelease/0,False,73,2,1,0,1,1,4,4,0,1,0,4,0,0,0,0,0,0,2,0,1,0,0,0,0,0,9,1,0,True
758,..\projects\logstash-8.5.2\tools\benchmark-cli\src\test\java\org\logstash\benchmark\cli\MainTest.java,org.logstash.benchmark.cli.MainTest,void runsRepeatedDatasetAgainstRelease(),"/**
 * @throws Exception On Failure
 */
@Test
public void runsRepeatedDatasetAgainstRelease() throws Exception {
    final File pwd = temp.newFolder();
    Main.main(String.format(""--%s=5.5.0"", UserInput.DISTRIBUTION_VERSION_PARAM), String.format(""--workdir=%s"", pwd.getAbsolutePath()), String.format(""--%s=%d"", UserInput.REPEAT_PARAM, 2));
}","/**
 * @throws Exception On Failure
 */
", ,/** * @throws Exception On Failure */,84,92,[0],0,[0],0,[0],0,0,0,0,runsRepeatedDatasetAgainstRelease(),org.logstash.benchmark.cli.MainTest,runsRepeatedDatasetAgainstRelease/0,False,85,2,1,0,1,1,4,4,0,1,0,4,0,0,0,0,0,0,3,1,1,0,0,0,0,0,11,1,0,True
759,..\projects\logstash-8.5.2\tools\benchmark-cli\src\test\java\org\logstash\benchmark\cli\MainTest.java,org.logstash.benchmark.cli.MainTest,void runsApacheAgainstRelease(),"/**
 * @throws Exception On Failure
 */
@Test
public void runsApacheAgainstRelease() throws Exception {
    final File pwd = temp.newFolder();
    Main.main(String.format(""--%s=5.5.0"", UserInput.DISTRIBUTION_VERSION_PARAM), String.format(""--%s=apache"", UserInput.TEST_CASE_PARAM), String.format(""--workdir=%s"", pwd.getAbsolutePath()));
}","/**
 * @throws Exception On Failure
 */
", ,/** * @throws Exception On Failure */,97,105,[0],0,[0],0,[0],0,0,0,0,runsApacheAgainstRelease(),org.logstash.benchmark.cli.MainTest,runsApacheAgainstRelease/0,False,98,2,1,0,1,1,4,4,0,1,0,4,0,0,0,0,0,0,3,0,1,0,0,0,0,0,10,1,0,True
760,..\projects\logstash-8.5.2\tools\benchmark-cli\src\test\java\org\logstash\benchmark\cli\MainTest.java,org.logstash.benchmark.cli.MainTest,void runsRepeatApacheAgainstRelease(),"/**
 * @throws Exception On Failure
 */
@Test
public void runsRepeatApacheAgainstRelease() throws Exception {
    final File pwd = temp.newFolder();
    Main.main(String.format(""--%s=5.5.0"", UserInput.DISTRIBUTION_VERSION_PARAM), String.format(""--%s=apache"", UserInput.TEST_CASE_PARAM), String.format(""--workdir=%s"", pwd.getAbsolutePath()), String.format(""--%s=%d"", UserInput.REPEAT_PARAM, 2));
}","/**
 * @throws Exception On Failure
 */
", ,/** * @throws Exception On Failure */,110,119,[0],0,[0],0,[0],0,0,0,0,runsRepeatApacheAgainstRelease(),org.logstash.benchmark.cli.MainTest,runsRepeatApacheAgainstRelease/0,False,111,2,1,0,1,1,4,4,0,1,0,4,0,0,0,0,0,0,4,1,1,0,0,0,0,0,11,1,0,True
761,..\projects\logstash-8.5.2\tools\benchmark-cli\src\test\java\org\logstash\benchmark\cli\MainTest.java,org.logstash.benchmark.cli.MainTest,void runsCustomAgainstLocal(),"/**
 * @throws Exception On Failure
 */
@Test
public void runsCustomAgainstLocal() throws Exception {
    Main.main(String.format(""--%s=custom"", UserInput.TEST_CASE_PARAM), String.format(""--%s=%s"", UserInput.TEST_CASE_CONFIG_PARAM, System.getProperty(""logstash.benchmark.test.config.path"")), String.format(""--%s=%s"", UserInput.LOCAL_VERSION_PARAM, System.getProperty(""logstash.benchmark.test.local.path"")));
}","/**
 * @throws Exception On Failure
 */
", ,/** * @throws Exception On Failure */,124,131,[0],0,[0],0,[0],0,0,0,0,runsCustomAgainstLocal(),org.logstash.benchmark.cli.MainTest,runsCustomAgainstLocal/0,False,125,2,1,0,1,1,3,3,0,0,0,3,0,0,0,0,0,0,5,0,0,0,0,0,0,0,7,1,0,True
762,..\projects\logstash-8.5.2\tools\dependencies-report\src\main\java\org\logstash\dependencies\Dependency.java,org.logstash.dependencies.Dependency,Object[] toCsvReportRecord(),"/**
 * Returns an object array representing this dependency as a CSV record according
 * to the format requested here: https://github.com/elastic/logstash/issues/8725
 */
Object[] toCsvReportRecord() {
    return new String[] { name, version, """", url, spdxLicense, copyright, sourceURL };
}","/**
 * Returns an object array representing this dependency as a CSV record according
 * to the format requested here: https://github.com/elastic/logstash/issues/8725
 */
", ,/** * Returns an object array representing this dependency as a CSV record according * to the format requested here: https://github.com/elastic/logstash/issues/8725 */,48,50,[0],0,[0],0,[0],0,0,0,0,toCsvReportRecord(),org.logstash.dependencies.Dependency,toCsvReportRecord/0,False,48,0,1,1,0,1,0,3,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,23,0,0,True
763,..\projects\logstash-8.5.2\tools\dependencies-report\src\main\java\org\logstash\dependencies\Dependency.java,org.logstash.dependencies.Dependency,"void addDependenciesFromRubyReport(InputStream, SortedSet<Dependency>)","/**
 * Reads dependencies from the specified stream using the Ruby dependency report format
 * and adds them to the supplied set.
 */
static void addDependenciesFromRubyReport(InputStream stream, SortedSet<Dependency> dependencies) throws IOException {
    Reader in = new InputStreamReader(stream);
    for (CSVRecord record : CSVFormat.DEFAULT.withFirstRecordAsHeader().parse(in)) {
        dependencies.add(Dependency.fromRubyCsvRecord(record));
    }
}","/**
 * Reads dependencies from the specified stream using the Ruby dependency report format
 * and adds them to the supplied set.
 */
", ,/** * Reads dependencies from the specified stream using the Ruby dependency report format * and adds them to the supplied set. */,56,62,[0],0,[0],0,[0],0,0,0,0,"addDependenciesFromRubyReport(InputStream, SortedSet<Dependency>)",org.logstash.dependencies.Dependency,"addDependenciesFromRubyReport/2[java.io.InputStream,java.util.SortedSet<org.logstash.dependencies.Dependency>]",False,57,1,2,1,1,2,4,6,0,1,2,4,1,1,1,0,0,0,0,0,1,0,1,0,0,0,32,8,0,True
764,..\projects\logstash-8.5.2\tools\dependencies-report\src\main\java\org\logstash\dependencies\Dependency.java,org.logstash.dependencies.Dependency,"void addDependenciesFromJavaReport(InputStream, SortedSet<Dependency>)","/**
 * Reads dependencies from the specified stream using the Java dependency report format
 * and adds them to the supplied set.
 */
static void addDependenciesFromJavaReport(InputStream stream, SortedSet<Dependency> dependencies) throws IOException {
    Reader in = new InputStreamReader(stream);
    for (CSVRecord record : CSVFormat.DEFAULT.withFirstRecordAsHeader().parse(in)) {
        dependencies.add(Dependency.fromJavaCsvRecord(record));
    }
}","/**
 * Reads dependencies from the specified stream using the Java dependency report format
 * and adds them to the supplied set.
 */
", ,/** * Reads dependencies from the specified stream using the Java dependency report format * and adds them to the supplied set. */,68,74,[0],0,[0],0,[0],0,0,0,0,"addDependenciesFromJavaReport(InputStream, SortedSet<Dependency>)",org.logstash.dependencies.Dependency,"addDependenciesFromJavaReport/2[java.io.InputStream,java.util.SortedSet<org.logstash.dependencies.Dependency>]",False,69,1,2,1,1,2,4,6,0,1,2,4,1,1,1,0,0,0,0,0,1,0,1,0,0,0,32,8,0,True
765,..\projects\logstash-8.5.2\tools\dependencies-report\src\main\java\org\logstash\dependencies\Dependency.java,org.logstash.dependencies.Dependency,Dependency fromRubyCsvRecord(CSVRecord),"private static Dependency fromRubyCsvRecord(CSVRecord record) {
    Dependency d = new Dependency();
    // name, version, url, license, copyright, sourceURL
    d.name = record.get(0);
    d.version = record.get(1);
    if (record.size() > 4) {
        d.copyright = record.get(4);
    }
    if (record.size() > 5) {
        d.sourceURL = record.get(5);
    }
    return d;
}", ,"// name, version, url, license, copyright, sourceURL
","// name, version, url, license, copyright, sourceURL",76,89,[0],0,[0],0,[0],0,0,0,0,fromRubyCsvRecord(CSVRecord),org.logstash.dependencies.Dependency,fromRubyCsvRecord/1[org.logstash.dependencies.CSVRecord],False,76,2,2,1,1,3,2,12,1,1,1,2,0,0,0,0,0,0,0,6,5,0,1,0,0,0,10,10,0,False
766,..\projects\logstash-8.5.2\tools\dependencies-report\src\main\java\org\logstash\dependencies\Dependency.java,org.logstash.dependencies.Dependency,Dependency fromJavaCsvRecord(CSVRecord),"private static Dependency fromJavaCsvRecord(CSVRecord record) {
    Dependency d = new Dependency();
    // artifact,moduleUrl,moduleLicense,moduleLicenseUrl
    String nameAndVersion = record.get(0);
    int colonIndex = nameAndVersion.indexOf(':');
    if (colonIndex == -1) {
        String err = String.format(""Could not parse java artifact name and version from '%s'"", nameAndVersion);
        throw new IllegalStateException(err);
    }
    colonIndex = nameAndVersion.indexOf(':', colonIndex + 1);
    String[] split = nameAndVersion.split("":"");
    if (split.length != 3) {
        String err = String.format(""Could not parse java artifact name and version from '%s', must be of the form group:name:version"", nameAndVersion);
        throw new IllegalStateException(err);
    }
    d.name = nameAndVersion.substring(0, colonIndex);
    d.version = nameAndVersion.substring(colonIndex + 1);
    // We DON'T read the license info out of this CSV because it is not reliable, we want humans
    // to use the overrides to ensure our license info is accurate
    return d;
}", ,"// artifact,moduleUrl,moduleLicense,moduleLicenseUrl
[[SEP]]// We DON'T read the license info out of this CSV because it is not reliable, we want humans
[[SEP]]// to use the overrides to ensure our license info is accurate
","// artifact,moduleUrl,moduleLicense,moduleLicenseUrl[[SEP]]// We DON'T read the license info out of this CSV because it is not reliable, we want humans// to use the overrides to ensure our license info is accurate",91,117,[0],0,"[0, 0, 0]",0,"[0, 0]",0,0,0,0,fromJavaCsvRecord(CSVRecord),org.logstash.dependencies.Dependency,fromJavaCsvRecord/1[org.logstash.dependencies.CSVRecord],False,91,2,2,1,1,3,7,18,1,6,1,7,0,0,0,2,0,0,3,6,9,2,1,0,0,0,31,10,0,False
767,..\projects\logstash-8.5.2\tools\dependencies-report\src\main\java\org\logstash\dependencies\Dependency.java,org.logstash.dependencies.Dependency,String fsCompatibleName(),"/**
 * The name contains colons, which don't work on windows. The compatible name uses `!` which works on multiple platforms
 * @return
 */
public String fsCompatibleName() {
    return name.replace("":"", ""!"");
}","/**
 * The name contains colons, which don't work on windows. The compatible name uses `!` which works on multiple platforms
 * @return
 */
", ,"/** * The name contains colons, which don't work on windows. The compatible name uses `!` which works on multiple platforms * @return */",149,151,[0],0,[0],0,[0],0,0,0,0,fsCompatibleName(),org.logstash.dependencies.Dependency,fsCompatibleName/0,False,149,0,1,1,0,1,1,3,1,0,0,1,0,0,0,0,0,0,2,0,0,0,0,0,0,0,14,1,0,True
768,..\projects\logstash-8.5.2\tools\dependencies-report\src\main\java\org\logstash\dependencies\Main.java,org.logstash.dependencies.Main,void main(String[]),"public static void main(String[] args) throws IOException {
    if (args.length < 4) {
        System.out.println(""Usage: org.logstash.dependencies.Main <pathToRubyDependencies.csv> <pathToJavaLicenseReportFolders.txt> <output.csv> <NOTICE.txt>"");
        System.exit(1);
    }
    InputStream rubyDependenciesStream = new FileInputStream(args[0]);
    List<String> javaDependencyReports = Files.readAllLines(Paths.get(args[1]));
    InputStream[] javaDependenciesStreams = new InputStream[javaDependencyReports.size()];
    for (int k = 0; k < javaDependencyReports.size(); k++) {
        javaDependenciesStreams[k] = new FileInputStream(javaDependencyReports.get(k) + ""/licenses.csv"");
    }
    FileWriter licenseCSVWriter = new FileWriter(args[2]);
    FileWriter noticeWriter = new FileWriter(args[3]);
    StringWriter unusedLicenseWriter = new StringWriter();
    boolean reportResult = new ReportGenerator().generateReport(getResourceAsStream(LICENSE_MAPPING_PATH), getResourceAsStream(ACCEPTABLE_LICENSES_PATH), rubyDependenciesStream, javaDependenciesStreams, licenseCSVWriter, noticeWriter, unusedLicenseWriter);
    // If there were unknown results in the report, exit with a non-zero status
    System.exit(reportResult ? 0 : 1);
}", ,"// If there were unknown results in the report, exit with a non-zero status
","// If there were unknown results in the report, exit with a non-zero status",41,70,[0],0,[0],0,[0],0,0,0,0,main(String[]),org.logstash.dependencies.Main,main/1[java.lang.String[]],False,41,2,3,0,3,4,8,17,0,8,1,8,1,1,1,0,0,0,2,9,9,1,1,0,0,0,42,9,0,False
769,..\projects\logstash-8.5.2\tools\dependencies-report\src\main\java\org\logstash\dependencies\ReportGenerator.java,org.logstash.dependencies.ReportGenerator,"void checkDependencyLicense(Map<String, LicenseInfo>, List<String>, Dependency)","private void checkDependencyLicense(Map<String, LicenseInfo> licenseMapping, List<String> acceptableLicenses, Dependency dependency) {
    if (licenseMapping.containsKey(dependency.name)) {
        LicenseInfo licenseInfo = licenseMapping.get(dependency.name);
        String[] dependencyLicenses = licenseInfo.license.split(""\\|"");
        boolean hasAcceptableLicense = false;
        if (licenseInfo.url != null && !licenseInfo.url.equals("""")) {
            for (int k = 0; k < dependencyLicenses.length && !hasAcceptableLicense; k++) {
                if (acceptableLicenses.stream().anyMatch(dependencyLicenses[k]::equalsIgnoreCase)) {
                    hasAcceptableLicense = true;
                }
            }
        }
        if (hasAcceptableLicense) {
            dependency.spdxLicense = licenseInfo.license;
            dependency.url = licenseInfo.url;
            dependency.sourceURL = licenseInfo.sourceURL;
            dependency.copyright = licenseInfo.copyright;
            licenseInfo.isUnused = false;
        } else {
            // unacceptable license or missing URL
            UNKNOWN_LICENSES.add(dependency);
        }
    } else {
        dependency.spdxLicense = UNKNOWN_LICENSE;
        UNKNOWN_LICENSES.add(dependency);
    }
}", ,"// unacceptable license or missing URL
",// unacceptable license or missing URL,180,208,[0],0,[0],0,[0],0,0,0,0,"checkDependencyLicense(Map<String, LicenseInfo>, List<String>, Dependency)",org.logstash.dependencies.ReportGenerator,"checkDependencyLicense/3[java.util.Map<java.lang.String,org.logstash.dependencies.LicenseInfo>,java.util.List<java.lang.String>,org.logstash.dependencies.Dependency]",False,180,2,1,1,0,8,7,28,0,4,3,7,0,0,1,1,0,0,2,1,11,0,4,0,0,0,24,2,0,False
770,..\projects\logstash-8.5.2\tools\dependencies-report\src\main\java\org\logstash\dependencies\ReportGenerator.java,org.logstash.dependencies.ReportGenerator,"void validateAndAdd(Map<String, LicenseInfo>, String, LicenseInfo)","private static void validateAndAdd(Map<String, LicenseInfo> licenses, String depName, LicenseInfo lup) {
    if (licenses.containsKey(depName)) {
        LicenseInfo existingLicense = licenses.get(depName);
        // Because dependency versions are not treated independently, if dependencies with different versions
        // have different licenses, we cannot distinguish between them
        if (!existingLicense.license.equals(lup.license)) {
            String err = String.format(""License mapping contains duplicate dependencies '%s' with conflicting "" + ""licenses '%s' and '%s'"", depName, existingLicense.license, lup.license);
            throw new IllegalStateException(err);
        }
    } else {
        licenses.put(depName, lup);
    }
}", ,"// Because dependency versions are not treated independently, if dependencies with different versions
[[SEP]]// have different licenses, we cannot distinguish between them
","// Because dependency versions are not treated independently, if dependencies with different versions// have different licenses, we cannot distinguish between them",237,251,[0],0,"[0, 0]",0,[0],0,0,0,0,"validateAndAdd(Map<String, LicenseInfo>, String, LicenseInfo)",org.logstash.dependencies.ReportGenerator,"validateAndAdd/3[java.util.Map<java.lang.String,org.logstash.dependencies.LicenseInfo>,java.lang.String,org.logstash.dependencies.LicenseInfo]",False,237,1,1,1,0,3,5,12,0,2,3,5,0,0,0,0,0,0,2,0,2,1,2,0,0,0,20,10,0,False
771,..\projects\logstash-8.5.2\tools\dependencies-report\src\test\java\org\logstash\dependencies\ReportGeneratorTest.java,org.logstash.dependencies.ReportGeneratorTest,void testSuccessfulReport(),"@Test
public // Tests both licenses and notices
void testSuccessfulReport() throws IOException {
    String expectedOutput = getStringFromStream(Main.getResourceAsStream(""/expectedOutput.txt""));
    String expectedNoticeOutput = getStringFromStream(Main.getResourceAsStream(""/expectedNoticeOutput.txt""));
    boolean result = runReportGenerator(""/licenseMapping-good.csv"", csvOutput, noticeOutput, unusedLicenseWriter);
    assertTrue(result);
    assertEquals(normalizeEol(expectedOutput), normalizeEol(csvOutput.toString()));
    assertEquals(normalizeEol(expectedNoticeOutput), normalizeEol(noticeOutput.toString()));
    String unusedLicenses = unusedLicenseWriter.toString();
    assertThat(unusedLicenses, containsString(""41 license mappings were specified but unused""));
}", ,"// Tests both licenses and notices
",// Tests both licenses and notices,56,70,[0],0,[0],0,[0],0,0,0,0,testSuccessfulReport(),org.logstash.dependencies.ReportGeneratorTest,testSuccessfulReport/0,False,58,3,4,0,4,1,9,10,0,4,0,9,3,2,0,0,0,0,4,0,4,0,0,0,0,0,30,1,0,False
772,..\projects\logstash-8.5.2\tools\dependencies-report\src\test\java\org\logstash\dependencies\ReportGeneratorTest.java,org.logstash.dependencies.ReportGeneratorTest,void testReportWithMissingLicenses(),"@Test
public void testReportWithMissingLicenses() throws IOException {
    boolean result = runReportGenerator(""/licenseMapping-missing.csv"", csvOutput, noticeOutput, unusedLicenseWriter);
    assertFalse(result);
    // verify that the two components in the test input with missing licenses are
    // listed in the output with no license, i.e., an empty license field followed by CR/LF
    assertTrue(csvOutput.toString().contains(""commons-io:commons-io,2.5,,,,,\r\n""));
    assertTrue(csvOutput.toString().contains(""filesize,0.0.4,,,,,\r\n""));
    String unusedLicenses = unusedLicenseWriter.toString();
    assertThat(unusedLicenses, containsString(""43 license mappings were specified but unused""));
}", ,"// verify that the two components in the test input with missing licenses are
[[SEP]]// listed in the output with no license, i.e., an empty license field followed by CR/LF
","// verify that the two components in the test input with missing licenses are// listed in the output with no license, i.e., an empty license field followed by CR/LF",72,84,[0],0,"[0, 0]",0,[0],0,0,0,0,testReportWithMissingLicenses(),org.logstash.dependencies.ReportGeneratorTest,testReportWithMissingLicenses/0,False,73,2,1,0,1,1,7,8,0,2,0,7,1,2,0,0,0,0,4,0,2,0,0,0,0,0,22,1,0,False
773,..\projects\logstash-8.5.2\tools\dependencies-report\src\test\java\org\logstash\dependencies\ReportGeneratorTest.java,org.logstash.dependencies.ReportGeneratorTest,void testReportWithUnacceptableLicenses(),"@Test
public void testReportWithUnacceptableLicenses() throws IOException {
    boolean result = runReportGenerator(""/licenseMapping-unacceptable.csv"", csvOutput, noticeOutput, unusedLicenseWriter);
    assertFalse(result);
    // verify that the two components in the test input with unacceptable licenses are
    // listed in the output with no license, i.e., an empty license field followed by CR/LF
    String csvString = csvOutput.toString();
    assertThat(csvString, containsString(""com.fasterxml.jackson.core:jackson-core,2.7.3,,,,,\r\n""));
    Pattern bundlerPattern = Pattern.compile("".*bundler,1\\.16\\.[0-1],,,,.*"");
    assertThat(bundlerPattern.matcher(csvString).find(), is(true));
    String unusedLicenses = unusedLicenseWriter.toString();
    assertThat(unusedLicenses, containsString(""43 license mappings were specified but unused""));
}", ,"// verify that the two components in the test input with unacceptable licenses are
[[SEP]]// listed in the output with no license, i.e., an empty license field followed by CR/LF
","// verify that the two components in the test input with unacceptable licenses are// listed in the output with no license, i.e., an empty license field followed by CR/LF",97,112,[0],0,"[0, 0]",0,[0],0,0,0,0,testReportWithUnacceptableLicenses(),org.logstash.dependencies.ReportGeneratorTest,testReportWithUnacceptableLicenses/0,False,98,2,1,0,1,1,9,10,0,4,0,9,1,2,0,0,0,0,4,0,4,0,0,0,0,0,26,1,0,False
774,..\projects\logstash-8.5.2\tools\dependencies-report\src\test\java\org\logstash\dependencies\ReportGeneratorTest.java,org.logstash.dependencies.ReportGeneratorTest,void testReportWithMissingUrls(),"@Test
public void testReportWithMissingUrls() throws IOException {
    boolean result = runReportGenerator(""/licenseMapping-missingUrls.csv"", csvOutput, noticeOutput, unusedLicenseWriter);
    assertFalse(result);
    // verify that the two components in the test input with missing URLs are
    // listed in the output with no license, i.e., an empty license field followed by CR/LF
    assertTrue(csvOutput.toString().contains(""org.codehaus.janino:commons-compiler,3.0.8,,,,,\r\n""));
    assertTrue(csvOutput.toString().contains(""json-parser,,,,,,\r\n""));
    String unusedLicenses = unusedLicenseWriter.toString();
    assertThat(unusedLicenses, containsString(""43 license mappings were specified but unused""));
}", ,"// verify that the two components in the test input with missing URLs are
[[SEP]]// listed in the output with no license, i.e., an empty license field followed by CR/LF
","// verify that the two components in the test input with missing URLs are// listed in the output with no license, i.e., an empty license field followed by CR/LF",114,126,[0],0,"[0, 0]",0,[0],0,0,0,0,testReportWithMissingUrls(),org.logstash.dependencies.ReportGeneratorTest,testReportWithMissingUrls/0,False,115,2,1,0,1,1,7,8,0,2,0,7,1,2,0,0,0,0,4,0,2,0,0,0,0,0,23,1,0,False
775,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestAppend.java,org.logstash.ingest.IngestAppend,"String toLogstash(String, boolean)","/**
 * Converts Ingest Append JSON to LS mutate filter.
 */
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
    ObjectMapper mapper = new ObjectMapper();
    TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {
    };
    final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
    final List<Map> processors = (List<Map>) jsonDefinition.get(""processors"");
    List<String> filters_pipeline = processors.stream().map(IngestAppend::mapProcessor).collect(Collectors.toList());
    return IngestConverter.filtersToFile(IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
}","/**
 * Converts Ingest Append JSON to LS mutate filter.
 */
", ,/** * Converts Ingest Append JSON to LS mutate filter. */,36,46,[0],0,[0],0,[0],0,0,0,0,"toLogstash(String, boolean)",org.logstash.ingest.IngestAppend,"toLogstash/2[java.lang.String,boolean]",False,37,3,3,1,2,1,8,10,1,5,2,8,0,0,0,0,0,0,3,0,5,0,0,1,0,0,26,9,0,True
776,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestConvert.java,org.logstash.ingest.IngestConvert,"String toLogstash(String, boolean)","/**
 * Converts Ingest Convert JSON to LS Date filter.
 */
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
    ObjectMapper mapper = new ObjectMapper();
    TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {
    };
    final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
    final List<Map> processors = (List<Map>) jsonDefinition.get(""processors"");
    List<String> filters_pipeline = processors.stream().map(IngestConvert::mapProcessor).collect(Collectors.toList());
    return IngestConverter.filtersToFile(IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
}","/**
 * Converts Ingest Convert JSON to LS Date filter.
 */
", ,/** * Converts Ingest Convert JSON to LS Date filter. */,36,46,[0],0,[0],0,[0],0,0,0,0,"toLogstash(String, boolean)",org.logstash.ingest.IngestConvert,"toLogstash/2[java.lang.String,boolean]",False,37,3,3,1,2,1,8,10,1,5,2,8,0,0,0,0,0,0,3,0,5,0,0,1,0,0,26,9,0,True
777,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestConverter.java,org.logstash.ingest.IngestConverter,String dotsToSquareBrackets(String),"/**
 * Translates the JSON naming pattern (`name.qualifier.sub`) into the LS pattern
 * [name][qualifier][sub] for all applicable tokens in the given string.
 * This function correctly identifies and omits renaming of string literals.
 * @param content to replace naming pattern in
 * @returns {string} with Json naming translated into grok naming
 */
public static String dotsToSquareBrackets(String content) {
    final Pattern pattern = Pattern.compile(""\\(\\?:%\\{.*\\|-\\)"");
    final Matcher matcher = pattern.matcher(content);
    List<String> tokens = new ArrayList<>();
    String right = content;
    while (matcher.find()) {
        final int start = matcher.start();
        final int end = matcher.end();
        final String matchContent = content.substring(start, end);
        right = content.substring(end);
        tokens.add(tokenDotsToSquareBrackets(content.substring(0, start)));
        tokens.add(matchContent);
    }
    tokens.add(tokenDotsToSquareBrackets(right));
    return String.join("""", tokens);
}","/**
 * Translates the JSON naming pattern (`name.qualifier.sub`) into the LS pattern
 * [name][qualifier][sub] for all applicable tokens in the given string.
 * This function correctly identifies and omits renaming of string literals.
 * @param content to replace naming pattern in
 * @returns {string} with Json naming translated into grok naming
 */
", ,/** * Translates the JSON naming pattern (`name.qualifier.sub`) into the LS pattern * [name][qualifier][sub] for all applicable tokens in the given string. * This function correctly identifies and omits renaming of string literals. * @param content to replace naming pattern in * @returns {string} with Json naming translated into grok naming */,38,53,[0],0,[0],0,[0],0,0,0,0,dotsToSquareBrackets(String),org.logstash.ingest.IngestConverter,dotsToSquareBrackets/1[java.lang.String],False,38,1,11,10,1,2,10,16,1,7,1,10,1,1,1,0,0,0,2,1,8,0,1,0,0,0,45,9,0,True
778,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestConverter.java,org.logstash.ingest.IngestConverter,String tokenDotsToSquareBrackets(String),"private static String tokenDotsToSquareBrackets(String content) {
    // Break out if this is not a naming pattern we convert
    final String adjusted;
    if (Pattern.compile(""([\\w_]+\\.)+[\\w_]+"").matcher(content).find()) {
        adjusted = content.replaceAll(""(\\w*)\\.(\\w*)"", ""$1][$2"").replaceAll(""\\[(\\w+)(}|$)"", ""[$1]$2"").replaceAll(""\\{(\\w+):(\\w+)]"", ""{$1:[$2]"").replaceAll(""^(\\w+)]\\["", ""[$1]["");
    } else {
        adjusted = content;
    }
    return adjusted;
}", ,"// Break out if this is not a naming pattern we convert
",// Break out if this is not a naming pattern we convert,55,67,[0],0,[0],0,[0],0,0,0,0,tokenDotsToSquareBrackets(String),org.logstash.ingest.IngestConverter,tokenDotsToSquareBrackets/1[java.lang.String],False,55,0,1,1,0,2,4,10,1,1,1,4,0,0,0,0,0,0,9,0,2,0,1,0,0,0,7,10,0,False
779,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestConverter.java,org.logstash.ingest.IngestConverter,String joinHashFields(String...),"/**
 * All hash fields in LS start on a new line.
 * @param fields Array of Strings of Serialized Hash Fields
 * @returns {string} Joined Serialization of Hash Fields
 */
public static String joinHashFields(String... fields) {
    return String.join(""\n"", fields);
}","/**
 * All hash fields in LS start on a new line.
 * @param fields Array of Strings of Serialized Hash Fields
 * @returns {string} Joined Serialization of Hash Fields
 */
", ,/** * All hash fields in LS start on a new line. * @param fields Array of Strings of Serialized Hash Fields * @returns {string} Joined Serialization of Hash Fields */,90,92,[0],0,[0],0,[0],0,0,0,0,joinHashFields(String[]),org.logstash.ingest.IngestConverter,joinHashFields/1[java.lang.String[]],False,90,0,7,7,0,1,1,3,1,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,19,9,0,True
780,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestConverter.java,org.logstash.ingest.IngestConverter,String fixIndent(String),"/**
 * Fixes indentation in LS string.
 * @param content LS string to fix indentation in, that has no indentation intentionally with
 * all lines starting on a token without preceding spaces.
 * @return LS string indented by 3 spaces per level
 */
public static String fixIndent(String content) {
    final String[] lines = content.split(""\n"");
    int count = 0;
    for (int i = 0; i < lines.length; i++) {
        if (Pattern.compile(""(\\{|\\[)$"").matcher(lines[i]).find()) {
            lines[i] = indent(lines[i], count);
            ++count;
        } else if (Pattern.compile(""(\\}|\\])$"").matcher(lines[i]).find()) {
            --count;
            lines[i] = indent(lines[i], count);
            // Only indent line if previous line ended on relevant control char.
        } else if (i > 0 && Pattern.compile(""(=>\\s+\"".+\""|,|\\{|\\}|\\[|\\])$"").matcher(lines[i - 1]).find()) {
            lines[i] = indent(lines[i], count);
        }
    }
    return String.join(""\n"", lines);
}","/**
 * Fixes indentation in LS string.
 * @param content LS string to fix indentation in, that has no indentation intentionally with
 * all lines starting on a token without preceding spaces.
 * @return LS string indented by 3 spaces per level
 */
","// Only indent line if previous line ended on relevant control char.
","/** * Fixes indentation in LS string. * @param content LS string to fix indentation in, that has no indentation intentionally with * all lines starting on a token without preceding spaces. * @return LS string indented by 3 spaces per level */[[SEP]]// Only indent line if previous line ended on relevant control char.",100,117,[0],0,[0],0,"[0, 0]",0,0,0,0,fixIndent(String),org.logstash.ingest.IngestConverter,fixIndent/1[java.lang.String],False,100,1,5,4,1,13,6,18,1,3,1,6,1,1,1,0,0,0,5,4,6,1,2,0,0,0,30,9,0,True
781,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestConverter.java,org.logstash.ingest.IngestConverter,String createPatternArray(String...),"/**
 * Converts Ingest/JSON style pattern array to LS pattern array, performing necessary variable
 * name and quote escaping adjustments.
 * @param patterns Pattern Array in JSON formatting
 * @return Pattern array in LS formatting
 */
public static String createPatternArray(String... patterns) {
    final String body = Arrays.stream(patterns).map(IngestConverter::dotsToSquareBrackets).map(IngestConverter::quoteString).collect(Collectors.joining("",\n""));
    return ""[\n"" + body + ""\n]"";
}","/**
 * Converts Ingest/JSON style pattern array to LS pattern array, performing necessary variable
 * name and quote escaping adjustments.
 * @param patterns Pattern Array in JSON formatting
 * @return Pattern array in LS formatting
 */
", ,"/** * Converts Ingest/JSON style pattern array to LS pattern array, performing necessary variable * name and quote escaping adjustments. * @param patterns Pattern Array in JSON formatting * @return Pattern array in LS formatting */",133,139,[0],0,[0],0,[0],0,0,0,0,createPatternArray(String[]),org.logstash.ingest.IngestConverter,createPatternArray/1[java.lang.String[]],False,133,0,3,3,0,1,4,4,1,1,1,4,0,0,0,0,0,0,3,0,1,1,0,0,0,0,25,9,0,True
782,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestConverter.java,org.logstash.ingest.IngestConverter,String createPatternArrayOrField(String...),"/**
 * Converts Ingest/JSON style pattern array to LS pattern array or string if the given array
 * contains a single element only, performing necessary variable name and quote escaping
 * adjustments.
 * @param patterns Pattern Array in JSON formatting
 * @return Pattern array or string in LS formatting
 */
public static String createPatternArrayOrField(String... patterns) {
    return patterns.length == 1 ? quoteString(dotsToSquareBrackets(patterns[0])) : createPatternArray(patterns);
}","/**
 * Converts Ingest/JSON style pattern array to LS pattern array or string if the given array
 * contains a single element only, performing necessary variable name and quote escaping
 * adjustments.
 * @param patterns Pattern Array in JSON formatting
 * @return Pattern array or string in LS formatting
 */
", ,"/** * Converts Ingest/JSON style pattern array to LS pattern array or string if the given array * contains a single element only, performing necessary variable name and quote escaping * adjustments. * @param patterns Pattern Array in JSON formatting * @return Pattern array or string in LS formatting */",156,160,[0],0,[0],0,[0],0,0,0,0,createPatternArrayOrField(String[]),org.logstash.ingest.IngestConverter,createPatternArrayOrField/1[java.lang.String[]],False,156,1,4,1,3,2,3,3,1,0,1,3,3,2,0,1,0,0,0,2,0,0,0,0,0,0,39,9,0,True
783,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestConverter.java,org.logstash.ingest.IngestConverter,"boolean hasOnFailure(Map<String, Map>, String)","/**
 * Does it have an on_failure field?
 * @param processor Json
 * @param name Name of the processor
 * @return true if has on failure
 */
@SuppressWarnings(""rawtypes"")
public static boolean hasOnFailure(Map<String, Map> processor, String name) {
    final List onFailure = (List) processor.get(name).get(""on_failure"");
    return onFailure != null && !onFailure.isEmpty();
}","/**
 * Does it have an on_failure field?
 * @param processor Json
 * @param name Name of the processor
 * @return true if has on failure
 */
", ,/** * Does it have an on_failure field? * @param processor Json * @param name Name of the processor * @return true if has on failure */,176,180,[0],0,[0],0,[0],0,0,0,0,"hasOnFailure(Map<String, Map>, String)",org.logstash.ingest.IngestConverter,"hasOnFailure/2[java.util.Map<java.lang.String,java.util.Map>,java.lang.String]",False,177,0,1,1,0,2,3,4,1,1,2,3,0,0,0,1,0,0,2,0,1,0,0,0,0,0,18,9,0,True
784,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestConverter.java,org.logstash.ingest.IngestConverter,"String createTagConditional(String, String)","/**
 * Creates an if clause with the tag name
 * @param tag String tag name to find in [tags] field
 * @param onFailurePipeline The on failure pipeline converted to LS to tack on in the conditional
 * @return a string representing a conditional logic
 */
public static String createTagConditional(String tag, String onFailurePipeline) {
    return ""if "" + quoteString(tag) + "" in [tags] {\n"" + onFailurePipeline + ""\n"" + ""}"";
}","/**
 * Creates an if clause with the tag name
 * @param tag String tag name to find in [tags] field
 * @param onFailurePipeline The on failure pipeline converted to LS to tack on in the conditional
 * @return a string representing a conditional logic
 */
", ,/** * Creates an if clause with the tag name * @param tag String tag name to find in [tags] field * @param onFailurePipeline The on failure pipeline converted to LS to tack on in the conditional * @return a string representing a conditional logic */,193,197,[0],0,[0],0,[0],0,0,0,0,"createTagConditional(String, String)",org.logstash.ingest.IngestConverter,"createTagConditional/2[java.lang.String,java.lang.String]",False,193,1,2,1,1,1,1,3,1,0,2,1,1,1,0,0,0,0,4,0,0,1,0,0,0,0,31,9,0,True
785,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestConverter.java,org.logstash.ingest.IngestConverter,"String appendIoPlugins(List<String>, boolean)","public static String appendIoPlugins(List<String> filtersPipeline, boolean appendStdio) {
    // TODO create unique list to join all
    String filtersPipelineStr = String.join(""\n"", filtersPipeline);
    if (appendStdio) {
        return String.join(""\n"", IngestConverter.getStdinInput(), filtersPipelineStr, IngestConverter.getStdoutOutput());
    } else {
        return String.join(""\n"", filtersPipelineStr, IngestConverter.getElasticsearchOutput());
    }
}", ,"// TODO create unique list to join all
",// TODO create unique list to join all,222,230,[0],0,[1],1,[1],1,1,1,1,"appendIoPlugins(List<String>, boolean)",org.logstash.ingest.IngestConverter,"appendIoPlugins/2[java.util.List<java.lang.String>,boolean]",False,222,1,14,11,3,2,5,9,2,1,2,5,3,3,0,0,0,0,3,0,1,0,1,0,0,0,7,9,0,False
786,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestDate.java,org.logstash.ingest.IngestDate,"String toLogstash(String, boolean)","/**
 * Converts Ingest Date JSON to LS Date filter.
 */
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
    ObjectMapper mapper = new ObjectMapper();
    TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {
    };
    final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
    final List<Map> processors = (List<Map>) jsonDefinition.get(""processors"");
    List<String> filters_pipeline = processors.stream().map(IngestDate::mapProcessor).collect(Collectors.toList());
    return IngestConverter.filtersToFile(IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
}","/**
 * Converts Ingest Date JSON to LS Date filter.
 */
", ,/** * Converts Ingest Date JSON to LS Date filter. */,37,47,[0],0,[0],0,[0],0,0,0,0,"toLogstash(String, boolean)",org.logstash.ingest.IngestDate,"toLogstash/2[java.lang.String,boolean]",False,38,3,3,1,2,1,8,10,1,5,2,8,0,0,0,0,0,0,3,0,5,0,0,1,0,0,25,9,0,True
787,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestGeoIp.java,org.logstash.ingest.IngestGeoIp,"String toLogstash(String, boolean)","/**
 * Converts Ingest Date JSON to LS Date filter.
 */
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
    ObjectMapper mapper = new ObjectMapper();
    TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {
    };
    final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
    final List<Map> processors = (List<Map>) jsonDefinition.get(""processors"");
    List<String> filters_pipeline = processors.stream().map(IngestGeoIp::mapProcessor).collect(Collectors.toList());
    return IngestConverter.filtersToFile(IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
}","/**
 * Converts Ingest Date JSON to LS Date filter.
 */
", ,/** * Converts Ingest Date JSON to LS Date filter. */,37,47,[0],0,[0],0,[0],0,0,0,0,"toLogstash(String, boolean)",org.logstash.ingest.IngestGeoIp,"toLogstash/2[java.lang.String,boolean]",False,38,3,3,1,2,1,8,10,1,5,2,8,0,0,0,0,0,0,3,0,5,0,0,1,0,0,25,9,0,True
788,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestGrok.java,org.logstash.ingest.IngestGrok,"String toLogstash(String, boolean)","/**
 * Converts Ingest JSON to LS Grok.
 */
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
    ObjectMapper mapper = new ObjectMapper();
    TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {
    };
    final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
    final List<Map> processors = (List<Map>) jsonDefinition.get(""processors"");
    List<String> filters_pipeline = processors.stream().map(IngestGrok::mapProcessor).collect(Collectors.toList());
    return IngestConverter.filtersToFile(IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
}","/**
 * Converts Ingest JSON to LS Grok.
 */
", ,/** * Converts Ingest JSON to LS Grok. */,37,47,[0],0,[0],0,[0],0,0,0,0,"toLogstash(String, boolean)",org.logstash.ingest.IngestGrok,"toLogstash/2[java.lang.String,boolean]",False,38,3,3,1,2,1,8,10,1,5,2,8,0,0,0,0,0,0,3,0,5,0,0,1,0,0,24,9,0,True
789,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestGsub.java,org.logstash.ingest.IngestGsub,"String toLogstash(String, boolean)","/**
 * Converts Ingest JSON to LS Grok.
 */
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
    ObjectMapper mapper = new ObjectMapper();
    TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {
    };
    final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
    final List<Map> processors = (List<Map>) jsonDefinition.get(""processors"");
    List<String> filters_pipeline = processors.stream().map(IngestGsub::mapProcessor).collect(Collectors.toList());
    return IngestConverter.filtersToFile(IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
}","/**
 * Converts Ingest JSON to LS Grok.
 */
", ,/** * Converts Ingest JSON to LS Grok. */,36,46,[0],0,[0],0,[0],0,0,0,0,"toLogstash(String, boolean)",org.logstash.ingest.IngestGsub,"toLogstash/2[java.lang.String,boolean]",False,37,3,3,1,2,1,8,10,1,5,2,8,0,0,0,0,0,0,3,0,5,0,0,1,0,0,24,9,0,True
790,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestJson.java,org.logstash.ingest.IngestJson,"String toLogstash(String, boolean)","/**
 * Converts Ingest json processor to LS json filter.
 */
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
    ObjectMapper mapper = new ObjectMapper();
    TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {
    };
    final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
    final List<Map> processors = (List<Map>) jsonDefinition.get(""processors"");
    List<String> filters_pipeline = processors.stream().map(IngestJson::mapProcessor).collect(Collectors.toList());
    return IngestConverter.filtersToFile(IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
}","/**
 * Converts Ingest json processor to LS json filter.
 */
", ,/** * Converts Ingest json processor to LS json filter. */,37,47,[0],0,[0],0,[0],0,0,0,0,"toLogstash(String, boolean)",org.logstash.ingest.IngestJson,"toLogstash/2[java.lang.String,boolean]",False,38,3,3,1,2,1,8,10,1,5,2,8,0,0,0,0,0,0,3,0,5,0,0,1,0,0,22,9,0,True
791,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestLowercase.java,org.logstash.ingest.IngestLowercase,"String toLogstash(String, boolean)","/**
 * Converts Ingest Lowercase JSON to LS mutate filter.
 */
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
    ObjectMapper mapper = new ObjectMapper();
    TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {
    };
    final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
    final List<Map> processors = (List<Map>) jsonDefinition.get(""processors"");
    List<String> filters_pipeline = processors.stream().map(IngestLowercase::mapProcessor).collect(Collectors.toList());
    return IngestConverter.filtersToFile(IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
}","/**
 * Converts Ingest Lowercase JSON to LS mutate filter.
 */
", ,/** * Converts Ingest Lowercase JSON to LS mutate filter. */,36,46,[0],0,[0],0,[0],0,0,0,0,"toLogstash(String, boolean)",org.logstash.ingest.IngestLowercase,"toLogstash/2[java.lang.String,boolean]",False,37,3,3,1,2,1,8,10,1,5,2,8,0,0,0,0,0,0,3,0,5,0,0,1,0,0,26,9,0,True
792,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestPipeline.java,org.logstash.ingest.IngestPipeline,"String toLogstash(String, boolean)","/**
 * Converts Ingest JSON to LS.
 */
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
    ObjectMapper mapper = new ObjectMapper();
    TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {
    };
    final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
    final List<Map> processors = (List<Map>) jsonDefinition.get(""processors"");
    List<String> filters_pipeline = processors.stream().map(IngestPipeline::mapProcessor).collect(Collectors.toList());
    String logstash_pipeline = IngestConverter.filterHash(IngestConverter.joinHashFields(filters_pipeline.toArray(new String[0])));
    return IngestConverter.filtersToFile(IngestConverter.appendIoPlugins(Collections.singletonList(logstash_pipeline), appendStdio));
}","/**
 * Converts Ingest JSON to LS.
 */
", ,/** * Converts Ingest JSON to LS. */,38,51,[0],0,[0],0,[0],0,0,0,0,"toLogstash(String, boolean)",org.logstash.ingest.IngestPipeline,"toLogstash/2[java.lang.String,boolean]",False,39,3,5,1,4,1,12,11,1,6,2,12,0,0,0,0,0,0,3,1,6,0,0,1,0,0,24,9,0,True
793,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestRename.java,org.logstash.ingest.IngestRename,"String toLogstash(String, boolean)","/**
 * Converts Ingest Rename JSON to LS mutate filter.
 */
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
    ObjectMapper mapper = new ObjectMapper();
    TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {
    };
    final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
    final List<Map> processors = (List<Map>) jsonDefinition.get(""processors"");
    List<String> filters_pipeline = processors.stream().map(IngestRename::mapProcessor).collect(Collectors.toList());
    return IngestConverter.filtersToFile(IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
}","/**
 * Converts Ingest Rename JSON to LS mutate filter.
 */
", ,/** * Converts Ingest Rename JSON to LS mutate filter. */,36,46,[0],0,[0],0,[0],0,0,0,0,"toLogstash(String, boolean)",org.logstash.ingest.IngestRename,"toLogstash/2[java.lang.String,boolean]",False,37,3,3,1,2,1,8,10,1,5,2,8,0,0,0,0,0,0,3,0,5,0,0,1,0,0,26,9,0,True
794,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\IngestSet.java,org.logstash.ingest.IngestSet,"String toLogstash(String, boolean)","/**
 * Converts Ingest Set JSON to LS mutate filter.
 */
@SuppressWarnings({ ""rawtypes"", ""unchecked"" })
public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
    ObjectMapper mapper = new ObjectMapper();
    TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {
    };
    final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
    final List<Map> processors = (List<Map>) jsonDefinition.get(""processors"");
    List<String> filters_pipeline = processors.stream().map(IngestSet::mapProcessor).collect(Collectors.toList());
    return IngestConverter.filtersToFile(IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
}","/**
 * Converts Ingest Set JSON to LS mutate filter.
 */
", ,/** * Converts Ingest Set JSON to LS mutate filter. */,36,46,[0],0,[0],0,[0],0,0,0,0,"toLogstash(String, boolean)",org.logstash.ingest.IngestSet,"toLogstash/2[java.lang.String,boolean]",False,37,3,3,1,2,1,8,10,1,5,2,8,0,0,0,0,0,0,3,0,5,0,0,1,0,0,26,9,0,True
795,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\JsUtil.java,org.logstash.ingest.JsUtil,ScriptEngine engine(),"/**
 * Sets up a {@link ScriptEngine} with all Ingest to LS DSL Converter JS scripts loaded.
 * @return {@link ScriptEngine} for Ingest to LS DSL Converter
 */
public static ScriptEngine engine() {
    final ScriptEngine engine = new ScriptEngineManager().getEngineByName(""nashorn"");
    try {
        for (final String file : SCRIPTS) {
            add(engine, String.format(""/ingest-%s.js"", file));
        }
    } catch (final IOException | ScriptException ex) {
        throw new IllegalStateException(ex);
    }
    return engine;
}","/**
 * Sets up a {@link ScriptEngine} with all Ingest to LS DSL Converter JS scripts loaded.
 * @return {@link ScriptEngine} for Ingest to LS DSL Converter
 */
", ,/** * Sets up a {@link ScriptEngine} with all Ingest to LS DSL Converter JS scripts loaded. * @return {@link ScriptEngine} for Ingest to LS DSL Converter */,58,69,[0],0,[0],0,[0],0,0,0,0,engine(),org.logstash.ingest.JsUtil,engine/0,False,58,1,1,0,1,3,3,12,1,1,0,3,1,1,1,0,1,0,2,0,1,0,2,0,0,0,30,9,0,True
796,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\JsUtil.java,org.logstash.ingest.JsUtil,"void convert(String[], String)","/**
 * Converts the given files from ingest to LS conf using the javascript function
 * @param args CLI Arguments
 * @param jsFunc JS function to call
 * @throws ScriptException
 * @throws NoSuchMethodException
 */
public static void convert(final String[] args, final String jsFunc) throws ScriptException, NoSuchMethodException {
    final OptionParser parser = new OptionParser();
    final OptionSpec<URI> input = parser.accepts(""input"", ""Input JSON file location URI. Only supports 'file://' as URI schema."").withRequiredArg().ofType(URI.class).required().forHelp();
    final OptionSpec<URI> output = parser.accepts(""output"", ""Output Logstash DSL file location URI. Only supports 'file://' as URI schema."").withRequiredArg().ofType(URI.class).required().forHelp();
    final OptionSpec<Void> appendStdio = parser.accepts(""append-stdio"", ""Flag to append stdin and stdout as outputs instead of the default ES output."").forHelp();
    try {
        final OptionSet options;
        try {
            options = parser.parse(args);
        } catch (final OptionException ex) {
            parser.printHelpOn(System.out);
            throw ex;
        }
        switch(jsFunc) {
            case ""ingest_append_to_logstash"":
                Files.write(Paths.get(options.valueOf(output)), IngestAppend.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8));
                break;
            case ""ingest_convert_to_logstash"":
                Files.write(Paths.get(options.valueOf(output)), IngestConvert.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8));
                break;
            case ""ingest_to_logstash_date"":
                Files.write(Paths.get(options.valueOf(output)), IngestDate.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8));
                break;
            case ""ingest_to_logstash_geoip"":
                Files.write(Paths.get(options.valueOf(output)), IngestGeoIp.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8));
                break;
            case ""ingest_to_logstash_grok"":
                Files.write(Paths.get(options.valueOf(output)), IngestGrok.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8));
                break;
            case ""ingest_to_logstash_gsub"":
                Files.write(Paths.get(options.valueOf(output)), IngestGsub.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8));
                break;
            case ""ingest_json_to_logstash"":
                Files.write(Paths.get(options.valueOf(output)), IngestJson.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8));
                break;
            case ""ingest_lowercase_to_logstash"":
                Files.write(Paths.get(options.valueOf(output)), IngestLowercase.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8));
                break;
            case ""ingest_rename_to_logstash"":
                Files.write(Paths.get(options.valueOf(output)), IngestRename.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8));
                break;
            case ""ingest_set_to_logstash"":
                Files.write(Paths.get(options.valueOf(output)), IngestSet.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8));
                break;
            case ""ingest_pipeline_to_logstash"":
                Files.write(Paths.get(options.valueOf(output)), IngestPipeline.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8));
                break;
            default:
                {
                    throw new IllegalArgumentException(""Can't recognize "" + jsFunc + "" processor"");
                }
        }
    } catch (final IOException ex) {
        throw new IllegalStateException(ex);
    }
}","/**
 * Converts the given files from ingest to LS conf using the javascript function
 * @param args CLI Arguments
 * @param jsFunc JS function to call
 * @throws ScriptException
 * @throws NoSuchMethodException
 */
", ,"/** * Converts the given files from ingest to LS conf using the javascript function * @param args CLI Arguments * @param jsFunc JS function to call * @throws ScriptException * @throws NoSuchMethodException */[[SEP]]//' as URI schema."").withRequiredArg().ofType(URI.class).required().forHelp();[[SEP]]//' as URI schema."").withRequiredArg().ofType(URI.class).required().forHelp();",78,177,[0],0,[0],0,"[0, 0, 0]",0,0,0,0,"convert(String[], String)",org.logstash.ingest.JsUtil,"convert/2[java.lang.String[],java.lang.String]",False,79,16,23,11,12,14,24,58,0,5,2,24,1,1,0,0,2,0,19,0,5,1,2,0,0,0,59,9,0,True
797,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\JsUtil.java,org.logstash.ingest.JsUtil,String input(URI),"/**
 * Retrieves the input Ingest JSON from a given {@link URI}.
 * @param uri {@link URI} of Ingest JSON
 * @return Json String
 * @throws IOException On failure to load Ingest JSON
 */
private static String input(final URI uri) throws IOException {
    if (""file"".equals(uri.getScheme())) {
        return new String(Files.readAllBytes(Paths.get(uri)), StandardCharsets.UTF_8);
    }
    throw new IllegalArgumentException(""--input must be of schema file://"");
}","/**
 * Retrieves the input Ingest JSON from a given {@link URI}.
 * @param uri {@link URI} of Ingest JSON
 * @return Json String
 * @throws IOException On failure to load Ingest JSON
 */
", ,"/** * Retrieves the input Ingest JSON from a given {@link URI}. * @param uri {@link URI} of Ingest JSON * @return Json String * @throws IOException On failure to load Ingest JSON */[[SEP]]//"");",185,192,[0],0,[0],0,"[0, 0]",0,0,0,0,input(URI),org.logstash.ingest.JsUtil,input/1[java.net.URI],False,185,0,1,1,0,2,4,6,1,0,1,4,0,0,0,0,0,0,2,0,0,0,1,0,0,0,27,10,0,True
798,..\projects\logstash-8.5.2\tools\ingest-converter\src\main\java\org\logstash\ingest\JsUtil.java,org.logstash.ingest.JsUtil,boolean isNotEmpty(String),"/**
 * Not empty check with nullability
 * @param s string to check
 * @return true iff s in not null and not empty
 */
static boolean isNotEmpty(String s) {
    return s != null && !s.isEmpty();
}","/**
 * Not empty check with nullability
 * @param s string to check
 * @return true iff s in not null and not empty
 */
", ,/** * Not empty check with nullability * @param s string to check * @return true iff s in not null and not empty */,207,209,[0],0,[0],0,[0],0,0,0,0,isNotEmpty(String),org.logstash.ingest.JsUtil,isNotEmpty/1[java.lang.String],False,207,0,1,1,0,2,1,3,1,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,16,8,0,True
799,..\projects\logstash-8.5.2\tools\ingest-converter\src\test\java\org\logstash\ingest\IngestTest.java,org.logstash.ingest.IngestTest,String utf8File(URL),"/**
 * Reads a file, normalizes line endings to Unix line endings and returns the whole content
 * as a String.
 * @param path Url to read
 * @return String content of the URL
 * @throws IOException On failure to read from given URL
 */
private static String utf8File(final URL path) throws IOException {
    final ByteArrayOutputStream baos = new ByteArrayOutputStream();
    try (final InputStream input = path.openStream()) {
        IOUtils.copy(input, baos);
    }
    return CARRIAGE_RETURN.matcher(CR_LF.matcher(baos.toString(StandardCharsets.UTF_8.name())).replaceAll(""\n"")).replaceAll(""\n"");
}","/**
 * Reads a file, normalizes line endings to Unix line endings and returns the whole content
 * as a String.
 * @param path Url to read
 * @return String content of the URL
 * @throws IOException On failure to read from given URL
 */
", ,"/** * Reads a file, normalizes line endings to Unix line endings and returns the whole content * as a String. * @param path Url to read * @return String content of the URL * @throws IOException On failure to read from given URL */",83,93,[0],0,[0],0,[0],0,0,0,0,utf8File(URL),org.logstash.ingest.IngestTest,utf8File/1[java.net.URL],False,83,0,1,1,0,1,6,7,1,2,1,6,0,0,0,0,1,0,2,0,2,0,1,0,0,0,36,10,0,True
800,..\projects\logstash-8.5.2\tools\jvm-options-parser\src\main\java\org\logstash\launchers\JavaVersion.java,org.logstash.launchers.JavaVersion,"int compare(JavaVersion, JavaVersion)","private static int compare(final JavaVersion leftVersion, final JavaVersion rightVersion) {
    List<Integer> left = leftVersion.version;
    List<Integer> right = rightVersion.version;
    // lexicographically compare two lists, treating missing entries as zeros
    final int len = Math.max(left.size(), right.size());
    for (int i = 0; i < len; i++) {
        final int l = (i < left.size()) ? left.get(i) : 0;
        final int r = (i < right.size()) ? right.get(i) : 0;
        if (l < r) {
            return -1;
        }
        if (r < l) {
            return 1;
        }
    }
    return 0;
}", ,"// lexicographically compare two lists, treating missing entries as zeros
","// lexicographically compare two lists, treating missing entries as zeros",62,78,[0],0,[0],0,[0],0,0,0,0,"compare(JavaVersion, JavaVersion)",org.logstash.launchers.JavaVersion,"compare/2[org.logstash.launchers.JavaVersion,org.logstash.launchers.JavaVersion]",False,62,1,1,1,0,6,3,16,3,6,2,3,0,0,1,0,0,2,0,6,6,0,2,0,0,0,9,10,0,False
801,..\projects\logstash-8.5.2\tools\jvm-options-parser\src\main\java\org\logstash\launchers\JvmOptionsParser.java,org.logstash.launchers.JvmOptionsParser,void main(String[]),"/**
 * The main entry point. The exit code is 0 if the JVM options were successfully parsed, otherwise the exit code is 1. If an improperly
 * formatted line is discovered, the line is output to standard error.
 *
 * @param args the args to the program which should consist of a single option, the path to LOGSTASH_HOME
 */
public static void main(final String[] args) {
    if (args.length < 1 || args.length > 2) {
        throw new IllegalArgumentException(""Expected two arguments specifying path to LOGSTASH_HOME and an optional LS_JVM_OPTS, but was "" + Arrays.toString(args));
    }
    bailOnOldJava();
    handleJvmOptions(args, System.getenv(""LS_JAVA_OPTS""));
}","/**
 * The main entry point. The exit code is 0 if the JVM options were successfully parsed, otherwise the exit code is 1. If an improperly
 * formatted line is discovered, the line is output to standard error.
 *
 * @param args the args to the program which should consist of a single option, the path to LOGSTASH_HOME
 */
", ,"/** * The main entry point. The exit code is 0 if the JVM options were successfully parsed, otherwise the exit code is 1. If an improperly * formatted line is discovered, the line is output to standard error. * * @param args the args to the program which should consist of a single option, the path to LOGSTASH_HOME */",106,114,[0],0,[0],0,[0],0,0,0,0,main(String[]),org.logstash.launchers.JvmOptionsParser,main/1[java.lang.String[]],False,106,1,2,0,2,3,4,7,0,0,1,4,2,6,0,0,0,0,2,2,0,1,1,0,0,0,57,9,0,True
802,..\projects\logstash-8.5.2\tools\jvm-options-parser\src\main\java\org\logstash\launchers\JvmOptionsParser.java,org.logstash.launchers.JvmOptionsParser,Optional<Path> lookupJvmOptionsFile(String),"private Optional<Path> lookupJvmOptionsFile(String jvmOpts) {
    if (jvmOpts != null && !jvmOpts.isEmpty()) {
        return Optional.of(Paths.get(jvmOpts));
    }
    // Set the initial JVM options from the jvm.options file. Look in
    // /etc/logstash first, and break if that file is found readable there.
    return Arrays.stream(new Path[] { Paths.get(""/etc/logstash/jvm.options""), Paths.get(logstashHome, ""config/jvm.options"") }).filter(p -> p.toFile().canRead()).findFirst();
}", ,"// Set the initial JVM options from the jvm.options file. Look in
[[SEP]]// /etc/logstash first, and break if that file is found readable there.
","// Set the initial JVM options from the jvm.options file. Look in// /etc/logstash first, and break if that file is found readable there.",155,165,[0],0,"[0, 0]",0,[0],0,0,0,0,lookupJvmOptionsFile(String),org.logstash.launchers.JvmOptionsParser,lookupJvmOptionsFile/1[java.lang.String],False,155,0,1,1,0,3,8,6,2,1,1,8,0,0,0,1,0,0,2,0,0,0,1,0,0,1,10,2,0,False
803,..\projects\logstash-8.5.2\tools\jvm-options-parser\src\main\java\org\logstash\launchers\JvmOptionsParser.java,org.logstash.launchers.JvmOptionsParser,"void handleJvmOptions(Optional<Path>, String)","private void handleJvmOptions(Optional<Path> jvmOptionsFile, String lsJavaOpts) throws IOException, JvmOptionsFileParserException {
    int javaMajorVersion = javaMajorVersion();
    // Add JVM Options from config/jvm.options
    final Set<String> jvmOptionsContent = new LinkedHashSet<>(getJvmOptionsFromFile(jvmOptionsFile, javaMajorVersion));
    // Add JVM Options from LS_JAVA_OPTS
    if (lsJavaOpts != null && !lsJavaOpts.isEmpty()) {
        if (isDebugEnabled()) {
            System.err.println(""Appending jvm options from environment LS_JAVA_OPTS"");
        }
        jvmOptionsContent.add(lsJavaOpts);
    }
    // Set mandatory JVM options
    jvmOptionsContent.addAll(getMandatoryJvmOptions(javaMajorVersion));
    System.out.println(String.join("" "", jvmOptionsContent));
}", ,"// Add JVM Options from config/jvm.options
[[SEP]]// Add JVM Options from LS_JAVA_OPTS
[[SEP]]// Set mandatory JVM options
",// Add JVM Options from config/jvm.options[[SEP]]// Add JVM Options from LS_JAVA_OPTS[[SEP]]// Set mandatory JVM options,167,184,[0],0,"[0, 0, 0]",0,"[0, 0, 0]",0,0,0,0,"handleJvmOptions(Optional<Path>, String)",org.logstash.launchers.JvmOptionsParser,"handleJvmOptions/2[java.util.Optional<java.nio.file.Path>,java.lang.String]",False,167,1,5,1,4,4,9,12,0,2,2,9,4,4,0,1,0,0,2,0,2,0,2,0,0,0,24,2,0,False
804,..\projects\logstash-8.5.2\tools\jvm-options-parser\src\main\java\org\logstash\launchers\JvmOptionsParser.java,org.logstash.launchers.JvmOptionsParser,Collection<String> getMandatoryJvmOptions(int),"/**
 * Returns the list of mandatory JVM options for the given version of Java.
 * @param javaMajorVersion
 * @return Collection of mandatory options
 */
static Collection<String> getMandatoryJvmOptions(int javaMajorVersion) {
    return Arrays.stream(MANDATORY_JVM_OPTIONS).map(option -> jvmOptionFromLine(javaMajorVersion, option)).flatMap(Optional::stream).collect(Collectors.toUnmodifiableList());
}","/**
 * Returns the list of mandatory JVM options for the given version of Java.
 * @param javaMajorVersion
 * @return Collection of mandatory options
 */
", ,/** * Returns the list of mandatory JVM options for the given version of Java. * @param javaMajorVersion * @return Collection of mandatory options */,191,196,[0],0,[0],0,[0],0,0,0,0,getMandatoryJvmOptions(int),org.logstash.launchers.JvmOptionsParser,getMandatoryJvmOptions/1[int],False,191,1,5,4,1,1,6,3,1,1,1,6,1,1,0,0,0,0,0,0,0,0,0,0,0,1,35,8,0,True
805,..\projects\logstash-8.5.2\tools\jvm-options-parser\src\main\java\org\logstash\launchers\JvmOptionsParser.java,org.logstash.launchers.JvmOptionsParser,"ParseResult parse(int, BufferedReader)","/**
 * If the version syntax specified on a line matches the specified JVM options, the JVM option callback will be invoked with the JVM
 * option. If the line does not match the specified syntax for the JVM options, the invalid line callback will be invoked with the
 * contents of the entire line.
 *
 * @param javaMajorVersion the Java major version to match JVM options against
 * @param br the buffered reader to read line-delimited JVM options from
 * @return the admitted options lines respecting the javaMajorVersion and the error lines
 * @throws IOException if an I/O exception occurs reading from the buffered reader
 */
static ParseResult parse(final int javaMajorVersion, final BufferedReader br) throws IOException {
    final ParseResult result = new ParseResult();
    int lineNumber = 0;
    while (true) {
        final String line = br.readLine();
        lineNumber++;
        if (line == null) {
            break;
        }
        try {
            jvmOptionFromLine(javaMajorVersion, line).ifPresent(result::appendOption);
        } catch (IllegalArgumentException e) {
            result.appendError(lineNumber, line);
        }
        ;
    }
    return result;
}","/**
 * If the version syntax specified on a line matches the specified JVM options, the JVM option callback will be invoked with the JVM
 * option. If the line does not match the specified syntax for the JVM options, the invalid line callback will be invoked with the
 * contents of the entire line.
 *
 * @param javaMajorVersion the Java major version to match JVM options against
 * @param br the buffered reader to read line-delimited JVM options from
 * @return the admitted options lines respecting the javaMajorVersion and the error lines
 * @throws IOException if an I/O exception occurs reading from the buffered reader
 */
", ,"/** * If the version syntax specified on a line matches the specified JVM options, the JVM option callback will be invoked with the JVM * option. If the line does not match the specified syntax for the JVM options, the invalid line callback will be invoked with the * contents of the entire line. * * @param javaMajorVersion the Java major version to match JVM options against * @param br the buffered reader to read line-delimited JVM options from * @return the admitted options lines respecting the javaMajorVersion and the error lines * @throws IOException if an I/O exception occurs reading from the buffered reader */",274,290,[0],0,[0],0,[0],0,0,0,0,"parse(int, BufferedReader)",org.logstash.launchers.JvmOptionsParser,"parse/2[int,java.io.BufferedReader]",False,274,2,9,6,3,4,4,19,1,3,2,4,1,1,1,1,1,0,0,1,3,0,2,0,0,0,67,8,0,True
806,..\projects\logstash-8.5.2\tools\jvm-options-parser\src\main\java\org\logstash\launchers\JvmOptionsParser.java,org.logstash.launchers.JvmOptionsParser,"Optional<String> jvmOptionFromLine(int, String)","/**
 * Parse the line-delimited JVM options from the specified string for the specified Java major version.
 * Valid JVM options are:
 * <ul>
 *     <li>
 *         a line starting with a dash is treated as a JVM option that applies to all versions
 *     </li>
 *     <li>
 *         a line starting with a number followed by a colon is treated as a JVM option that applies to the matching Java major version
 *         only
 *     </li>
 *     <li>
 *         a line starting with a number followed by a dash followed by a colon is treated as a JVM option that applies to the matching
 *         Java specified major version and all larger Java major versions
 *     </li>
 *     <li>
 *         a line starting with a number followed by a dash followed by a number followed by a colon is treated as a JVM option that
 *         applies to the specified range of matching Java major versions
 *     </li>
 * </ul>
 *
 * For example, if the specified Java major version is 8, the following JVM options will be accepted:
 * <ul>
 *     <li>
 *         {@code -XX:+PrintGCDateStamps}
 *     </li>
 *     <li>
 *         {@code 8:-XX:+PrintGCDateStamps}
 *     </li>
 *     <li>
 *         {@code 8-:-XX:+PrintGCDateStamps}
 *     </li>
 *     <li>
 *         {@code 7-8:-XX:+PrintGCDateStamps}
 *     </li>
 * </ul>
 * and the following JVM options will be skipped:
 * <ul>
 *     <li>
 *         {@code 9:-Xlog:age*=trace,gc*,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m}
 *     </li>
 *     <li>
 *         {@code 9-:-Xlog:age*=trace,gc*,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m}
 *     </li>
 *     <li>
 *         {@code 9-10:-Xlog:age*=trace,gc*,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m}
 *     </li>
 * </ul>
 *
 * @param javaMajorVersion
 * @param line
 * @return Returns an Optional containing a string if the line contains a valid option for the specified Java
 *         version and empty otherwise
 */
private static Optional<String> jvmOptionFromLine(final int javaMajorVersion, final String line) {
    if (line.startsWith(""#"") || line.matches(""\\s*"")) {
        // Skip comments and blank lines
        return Optional.empty();
    }
    final Matcher matcher = OPTION_DEFINITION.matcher(line);
    if (matcher.matches()) {
        final String start = matcher.group(""start"");
        final String end = matcher.group(""end"");
        if (start == null) {
            // no range present, unconditionally apply the JVM option
            return Optional.of(line);
        } else {
            final int lower = Integer.parseInt(start);
            final int upper;
            if (matcher.group(""range"") == null) {
                // no range is present, apply the JVM option to the specified major version only
                upper = lower;
            } else if (end == null) {
                // a range of the form \\d+- is present, apply the JVM option to all major versions larger than the specified one
                upper = Integer.MAX_VALUE;
            } else {
                upper = Integer.parseInt(end);
                if (upper < lower) {
                    throw new IllegalArgumentException(""Upper bound must be greater than lower bound"");
                }
            }
            if (lower <= javaMajorVersion && javaMajorVersion <= upper) {
                return Optional.of(matcher.group(""option""));
            }
        }
    } else {
        throw new IllegalArgumentException(""Illegal JVM Option"");
    }
    return Optional.empty();
}","/**
 * Parse the line-delimited JVM options from the specified string for the specified Java major version.
 * Valid JVM options are:
 * <ul>
 *     <li>
 *         a line starting with a dash is treated as a JVM option that applies to all versions
 *     </li>
 *     <li>
 *         a line starting with a number followed by a colon is treated as a JVM option that applies to the matching Java major version
 *         only
 *     </li>
 *     <li>
 *         a line starting with a number followed by a dash followed by a colon is treated as a JVM option that applies to the matching
 *         Java specified major version and all larger Java major versions
 *     </li>
 *     <li>
 *         a line starting with a number followed by a dash followed by a number followed by a colon is treated as a JVM option that
 *         applies to the specified range of matching Java major versions
 *     </li>
 * </ul>
 *
 * For example, if the specified Java major version is 8, the following JVM options will be accepted:
 * <ul>
 *     <li>
 *         {@code -XX:+PrintGCDateStamps}
 *     </li>
 *     <li>
 *         {@code 8:-XX:+PrintGCDateStamps}
 *     </li>
 *     <li>
 *         {@code 8-:-XX:+PrintGCDateStamps}
 *     </li>
 *     <li>
 *         {@code 7-8:-XX:+PrintGCDateStamps}
 *     </li>
 * </ul>
 * and the following JVM options will be skipped:
 * <ul>
 *     <li>
 *         {@code 9:-Xlog:age*=trace,gc*,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m}
 *     </li>
 *     <li>
 *         {@code 9-:-Xlog:age*=trace,gc*,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m}
 *     </li>
 *     <li>
 *         {@code 9-10:-Xlog:age*=trace,gc*,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m}
 *     </li>
 * </ul>
 *
 * @param javaMajorVersion
 * @param line
 * @return Returns an Optional containing a string if the line contains a valid option for the specified Java
 *         version and empty otherwise
 */
","// Skip comments and blank lines
[[SEP]]// no range present, unconditionally apply the JVM option
[[SEP]]// no range is present, apply the JVM option to the specified major version only
[[SEP]]// a range of the form \\d+- is present, apply the JVM option to all major versions larger than the specified one
","/** * Parse the line-delimited JVM options from the specified string for the specified Java major version. * Valid JVM options are: * <ul> *     <li> *         a line starting with a dash is treated as a JVM option that applies to all versions *     </li> *     <li> *         a line starting with a number followed by a colon is treated as a JVM option that applies to the matching Java major version *         only *     </li> *     <li> *         a line starting with a number followed by a dash followed by a colon is treated as a JVM option that applies to the matching *         Java specified major version and all larger Java major versions *     </li> *     <li> *         a line starting with a number followed by a dash followed by a number followed by a colon is treated as a JVM option that *         applies to the specified range of matching Java major versions *     </li> * </ul> * * For example, if the specified Java major version is 8, the following JVM options will be accepted: * <ul> *     <li> *         {@code -XX:+PrintGCDateStamps} *     </li> *     <li> *         {@code 8:-XX:+PrintGCDateStamps} *     </li> *     <li> *         {@code 8-:-XX:+PrintGCDateStamps} *     </li> *     <li> *         {@code 7-8:-XX:+PrintGCDateStamps} *     </li> * </ul> * and the following JVM options will be skipped: * <ul> *     <li> *         {@code 9:-Xlog:age*=trace,gc*,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m} *     </li> *     <li> *         {@code 9-:-Xlog:age*=trace,gc*,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m} *     </li> *     <li> *         {@code 9-10:-Xlog:age*=trace,gc*,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m} *     </li> * </ul> * * @param javaMajorVersion * @param line * @return Returns an Optional containing a string if the line contains a valid option for the specified Java *         version and empty otherwise */[[SEP]]// Skip comments and blank lines[[SEP]]// no range present, unconditionally apply the JVM option[[SEP]]// no range is present, apply the JVM option to the specified major version only[[SEP]]// a range of the form \\d+- is present, apply the JVM option to all major versions larger than the specified one",346,381,[0],0,"[0, 0, 0, 0]",0,"[0, 0, 0, 0, 0]",0,0,0,0,"jvmOptionFromLine(int, String)",org.logstash.launchers.JvmOptionsParser,"jvmOptionFromLine/2[int,java.lang.String]",False,346,0,2,2,0,10,8,36,4,5,2,8,0,0,0,3,0,0,8,0,7,0,4,0,0,0,81,10,0,True
807,..\projects\logstash-8.5.2\tools\jvm-options-parser\src\test\java\org\logstash\launchers\JvmOptionsParserTest.java,org.logstash.launchers.JvmOptionsParserTest,void test_LS_JAVA_OPTS_isUsedWhenNoJvmOptionsIsAvailable(),"@Test
public void test_LS_JAVA_OPTS_isUsedWhenNoJvmOptionsIsAvailable() throws IOException, InterruptedException, ReflectiveOperationException {
    JvmOptionsParser.handleJvmOptions(new String[] { temp.toString() }, ""-Xblabla"");
    // Verify
    final String output = outputStreamCaptor.toString();
    assertTrue(""Output MUST contains the options present in LS_JAVA_OPTS"", output.contains(""-Xblabla""));
}", ,"// Verify
",// Verify,37,44,[0],0,[0],0,[0],0,0,0,0,test_LS_JAVA_OPTS_isUsedWhenNoJvmOptionsIsAvailable(),org.logstash.launchers.JvmOptionsParserTest,test_LS_JAVA_OPTS_isUsedWhenNoJvmOptionsIsAvailable/0,False,38,2,1,0,1,1,5,5,0,1,0,5,0,0,0,0,0,0,3,0,1,0,0,0,0,0,31,1,0,False
808,..\projects\logstash-8.5.2\x-pack\src\test\java\org\logstash\xpack\test\RSpecTests.java,org.logstash.xpack.test.RSpecTests,Ruby initializeGlobalRuntime(Path),"private static Ruby initializeGlobalRuntime(final Path root) {
    String[] args = new String[] { ""--disable-did_you_mean"" };
    Ruby runtime = Ruby.newInstance(Logstash.initRubyConfig(root, null, /* qa/integration */
    args));
    if (runtime != RubyUtil.RUBY)
        throw new AssertionError(""runtime already initialized"");
    return runtime;
}", ,"/* qa/integration */
",/* qa/integration */,48,53,[0],0,[0],0,[0],0,0,0,0,initializeGlobalRuntime(Path),org.logstash.xpack.test.RSpecTests,initializeGlobalRuntime/1[java.nio.file.Path],False,48,1,1,1,0,2,2,6,1,2,1,2,0,0,0,1,0,0,2,0,2,0,1,0,0,0,12,10,0,False
